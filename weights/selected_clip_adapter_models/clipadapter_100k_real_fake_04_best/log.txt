***************
** Arguments **
***************
backbone: 
config_file: C:/Users/sohail/Desktop/Research/PhD/Year3/DeepfakeDetection/CoOp/configs/trainers/coop/vit_l14_ep2.yaml
dataset_config_file: C:/Users/sohail/Desktop/Research/PhD/Year3/DeepfakeDetection/CoOp/configs/datasets/imagenet.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '100000']
output_dir: C:/Users/sohail/Desktop/Research/PhD/Year3/DeepfakeDetection/CoOp/outputs/
resume: 
root: C:/Users/sohail/Desktop/Research/PhD/Year3/DeepfakeDetection/CoOp/data/
seed: 17
source_domains: None
target_domains: None
trainer: CLIP_Adapter
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: ImageNet
  NUM_LABELED: -1
  NUM_SHOTS: 100000
  ROOT: C:/Users/sohail/Desktop/Research/PhD/Year3/DeepfakeDetection/CoOp/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-L/14
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 2
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: C:/Users/sohail/Desktop/Research/PhD/Year3/DeepfakeDetection/CoOp/outputs/
RESUME: 
SEED: 17
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CLIP_Adapter
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.1.2+cu118
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 11 Home
GCC version: Could not collect
Clang version: Could not collect
CMake version: version 3.26.0-rc1
Libc version: N/A

Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.22631-SP0
Is CUDA available: True
CUDA runtime version: 11.7.64
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3060
Nvidia driver version: 546.33
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture=9
CurrentClockSpeed=2500
DeviceID=CPU0
Family=205
L2CacheSize=7680
L2CacheSpeed=
Manufacturer=GenuineIntel
MaxClockSpeed=2500
Name=12th Gen Intel(R) Core(TM) i5-12400F
ProcessorType=3
Revision=

Versions of relevant libraries:
[pip3] flake8==3.7.9
[pip3] numpy==1.24.3
[pip3] torch==2.1.2+cu118
[pip3] torchaudio==2.1.2+cu118
[pip3] torchvision==0.16.2
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              h74a9793_1  
[conda] mkl                       2023.1.0         h6b88ed4_46358  
[conda] mkl-service               2.4.0            py38h2bbff1b_1  
[conda] mkl_fft                   1.3.8            py38h2bbff1b_0  
[conda] mkl_random                1.2.4            py38h59b6b97_0  
[conda] numpy                     1.24.3           py38h79a8e48_1  
[conda] numpy-base                1.24.3           py38h8a87ada_1  
[conda] pytorch-mutex             1.0                         cpu    pytorch
[conda] torch                     2.1.2+cu118              pypi_0    pypi
[conda] torchaudio                2.1.2+cu118              pypi_0    pypi
[conda] torchvision               0.16.2                 py38_cpu    pytorch
        Pillow (10.0.1)

Loading trainer: CLIP_Adapter
Loading dataset: ImageNet
Creating a 100000-shot dataset
Saving preprocessed few-shot data to C:\Users\sohail\Desktop\Research\PhD\Year3\DeepfakeDetection\CoOp\data\imagenet\split_fewshot\shot_100000-seed_17.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  --------
Dataset    ImageNet
# classes  2
# train_x  200,000
# val      5,396
# test     5,396
---------  --------
Loading CLIP (backbone: ViT-L/14)
Building custom CLIP
['real', 'fake']
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=C:/Users/sohail/Desktop/Research/PhD/Year3/DeepfakeDetection/CoOp/outputs/tensorboard)
epoch [1/2] batch [5/6250] time 1.099 (8.805) data 0.001 (7.585) loss 0.7076 (0.6443) acc 59.3750 (68.7500) lr 1.0000e-05 eta 1 day, 6:33:39
epoch [1/2] batch [10/6250] time 1.100 (4.952) data 0.000 (3.793) loss 0.8598 (0.6827) acc 59.3750 (66.2500) lr 1.0000e-05 eta 17:10:55
epoch [1/2] batch [15/6250] time 1.105 (3.670) data 0.001 (2.528) loss 0.8119 (0.7073) acc 59.3750 (66.2500) lr 1.0000e-05 eta 12:43:34
epoch [1/2] batch [20/6250] time 1.105 (3.028) data 0.001 (1.896) loss 0.7989 (0.7052) acc 62.5000 (66.0938) lr 1.0000e-05 eta 10:29:54
epoch [1/2] batch [25/6250] time 1.103 (2.644) data 0.000 (1.517) loss 0.4719 (0.6919) acc 75.0000 (66.6250) lr 1.0000e-05 eta 9:09:40
epoch [1/2] batch [30/6250] time 1.107 (2.388) data 0.000 (1.264) loss 0.9414 (0.7053) acc 53.1250 (65.9375) lr 1.0000e-05 eta 8:16:12
epoch [1/2] batch [35/6250] time 1.105 (2.204) data 0.000 (1.084) loss 0.6478 (0.7046) acc 68.7500 (65.8929) lr 1.0000e-05 eta 7:37:57
epoch [1/2] batch [40/6250] time 1.105 (2.067) data 0.000 (0.948) loss 0.7580 (0.7130) acc 65.6250 (66.0938) lr 1.0000e-05 eta 7:09:15
epoch [1/2] batch [45/6250] time 1.103 (1.960) data 0.000 (0.843) loss 0.9786 (0.7151) acc 53.1250 (65.6944) lr 1.0000e-05 eta 6:46:54
epoch [1/2] batch [50/6250] time 1.107 (1.875) data 0.000 (0.759) loss 0.8646 (0.7125) acc 62.5000 (65.8750) lr 1.0000e-05 eta 6:29:00
epoch [1/2] batch [55/6250] time 1.110 (1.805) data 0.002 (0.690) loss 0.5890 (0.7076) acc 78.1250 (65.9659) lr 1.0000e-05 eta 6:14:22
epoch [1/2] batch [60/6250] time 1.107 (1.747) data 0.000 (0.632) loss 0.8826 (0.7080) acc 62.5000 (66.2500) lr 1.0000e-05 eta 6:02:12
epoch [1/2] batch [65/6250] time 1.104 (1.698) data 0.000 (0.584) loss 1.0203 (0.7240) acc 43.7500 (65.0000) lr 1.0000e-05 eta 5:51:50
epoch [1/2] batch [70/6250] time 1.106 (1.655) data 0.000 (0.542) loss 0.5854 (0.7302) acc 71.8750 (64.9107) lr 1.0000e-05 eta 5:42:56
epoch [1/2] batch [75/6250] time 1.105 (1.619) data 0.000 (0.506) loss 0.8441 (0.7367) acc 56.2500 (64.2917) lr 1.0000e-05 eta 5:35:14
epoch [1/2] batch [80/6250] time 1.109 (1.587) data 0.001 (0.474) loss 0.5723 (0.7376) acc 59.3750 (64.3750) lr 1.0000e-05 eta 5:28:29
epoch [1/2] batch [85/6250] time 1.107 (1.559) data 0.001 (0.446) loss 0.7594 (0.7357) acc 53.1250 (64.3750) lr 1.0000e-05 eta 5:22:30
epoch [1/2] batch [90/6250] time 1.106 (1.534) data 0.000 (0.422) loss 0.6472 (0.7312) acc 78.1250 (64.5833) lr 1.0000e-05 eta 5:17:12
epoch [1/2] batch [95/6250] time 1.108 (1.511) data 0.000 (0.399) loss 0.8860 (0.7328) acc 56.2500 (64.3750) lr 1.0000e-05 eta 5:12:26
epoch [1/2] batch [100/6250] time 1.106 (1.491) data 0.000 (0.380) loss 0.6015 (0.7375) acc 75.0000 (64.2500) lr 1.0000e-05 eta 5:08:08
epoch [1/2] batch [105/6250] time 1.109 (1.473) data 0.000 (0.361) loss 0.8492 (0.7388) acc 65.6250 (64.1369) lr 1.0000e-05 eta 5:04:15
epoch [1/2] batch [110/6250] time 1.106 (1.456) data 0.000 (0.345) loss 0.9267 (0.7395) acc 50.0000 (64.0909) lr 1.0000e-05 eta 5:00:42
epoch [1/2] batch [115/6250] time 1.108 (1.441) data 0.000 (0.330) loss 0.9423 (0.7447) acc 56.2500 (63.9402) lr 1.0000e-05 eta 4:57:27
epoch [1/2] batch [120/6250] time 1.108 (1.427) data 0.000 (0.316) loss 0.8647 (0.7423) acc 62.5000 (64.2188) lr 1.0000e-05 eta 4:54:28
epoch [1/2] batch [125/6250] time 1.107 (1.414) data 0.000 (0.304) loss 0.6707 (0.7439) acc 62.5000 (64.0250) lr 1.0000e-05 eta 4:51:43
epoch [1/2] batch [130/6250] time 1.109 (1.403) data 0.000 (0.292) loss 0.6212 (0.7400) acc 65.6250 (64.0865) lr 1.0000e-05 eta 4:49:10
epoch [1/2] batch [135/6250] time 1.111 (1.392) data 0.000 (0.281) loss 0.6412 (0.7387) acc 65.6250 (64.1204) lr 1.0000e-05 eta 4:46:48
epoch [1/2] batch [140/6250] time 1.107 (1.382) data 0.000 (0.271) loss 0.7907 (0.7400) acc 59.3750 (64.0848) lr 1.0000e-05 eta 4:44:36
epoch [1/2] batch [145/6250] time 1.107 (1.372) data 0.000 (0.262) loss 0.6010 (0.7383) acc 75.0000 (64.0948) lr 1.0000e-05 eta 4:42:33
epoch [1/2] batch [150/6250] time 1.107 (1.363) data 0.000 (0.253) loss 1.2573 (0.7399) acc 53.1250 (64.1667) lr 1.0000e-05 eta 4:40:37
epoch [1/2] batch [155/6250] time 1.107 (1.355) data 0.000 (0.245) loss 0.8516 (0.7397) acc 53.1250 (64.0927) lr 1.0000e-05 eta 4:38:49
epoch [1/2] batch [160/6250] time 1.107 (1.347) data 0.000 (0.237) loss 1.0145 (0.7399) acc 50.0000 (64.1211) lr 1.0000e-05 eta 4:37:07
epoch [1/2] batch [165/6250] time 1.108 (1.340) data 0.000 (0.230) loss 0.6336 (0.7391) acc 65.6250 (64.1667) lr 1.0000e-05 eta 4:35:31
epoch [1/2] batch [170/6250] time 1.108 (1.333) data 0.001 (0.223) loss 0.5919 (0.7387) acc 71.8750 (64.1728) lr 1.0000e-05 eta 4:34:00
epoch [1/2] batch [175/6250] time 1.109 (1.327) data 0.000 (0.217) loss 0.6210 (0.7375) acc 75.0000 (64.3750) lr 1.0000e-05 eta 4:32:34
epoch [1/2] batch [180/6250] time 1.109 (1.321) data 0.000 (0.211) loss 0.9125 (0.7395) acc 50.0000 (64.2014) lr 1.0000e-05 eta 4:31:13
epoch [1/2] batch [185/6250] time 1.110 (1.315) data 0.000 (0.205) loss 0.5619 (0.7364) acc 71.8750 (64.3243) lr 1.0000e-05 eta 4:29:56
epoch [1/2] batch [190/6250] time 1.146 (1.310) data 0.000 (0.200) loss 0.6822 (0.7341) acc 71.8750 (64.4901) lr 1.0000e-05 eta 4:28:51
epoch [1/2] batch [195/6250] time 1.110 (1.305) data 0.000 (0.195) loss 0.6189 (0.7343) acc 71.8750 (64.4551) lr 1.0000e-05 eta 4:27:43
epoch [1/2] batch [200/6250] time 1.106 (1.300) data 0.000 (0.190) loss 0.9509 (0.7338) acc 53.1250 (64.5625) lr 1.0000e-05 eta 4:26:36
epoch [1/2] batch [205/6250] time 1.107 (1.296) data 0.000 (0.185) loss 1.0169 (0.7336) acc 53.1250 (64.6494) lr 1.0000e-05 eta 4:25:31
epoch [1/2] batch [210/6250] time 1.107 (1.291) data 0.000 (0.181) loss 0.7592 (0.7333) acc 53.1250 (64.5536) lr 1.0000e-05 eta 4:24:30
epoch [1/2] batch [215/6250] time 1.111 (1.287) data 0.000 (0.177) loss 0.4956 (0.7341) acc 75.0000 (64.4767) lr 1.0000e-05 eta 4:23:31
epoch [1/2] batch [220/6250] time 1.111 (1.283) data 0.000 (0.173) loss 0.8716 (0.7330) acc 53.1250 (64.4744) lr 1.0000e-05 eta 4:22:35
epoch [1/2] batch [225/6250] time 1.108 (1.279) data 0.000 (0.169) loss 0.7529 (0.7308) acc 68.7500 (64.6667) lr 1.0000e-05 eta 4:21:41
epoch [1/2] batch [230/6250] time 1.107 (1.275) data 0.000 (0.165) loss 0.7267 (0.7341) acc 62.5000 (64.5245) lr 1.0000e-05 eta 4:20:49
epoch [1/2] batch [235/6250] time 1.110 (1.272) data 0.000 (0.162) loss 0.6139 (0.7314) acc 65.6250 (64.6011) lr 1.0000e-05 eta 4:19:59
epoch [1/2] batch [240/6250] time 1.108 (1.268) data 0.000 (0.158) loss 0.8963 (0.7309) acc 56.2500 (64.6354) lr 1.0000e-05 eta 4:19:11
epoch [1/2] batch [245/6250] time 1.107 (1.265) data 0.000 (0.155) loss 0.9055 (0.7317) acc 59.3750 (64.6556) lr 1.0000e-05 eta 4:18:25
epoch [1/2] batch [250/6250] time 1.109 (1.262) data 0.000 (0.152) loss 0.6109 (0.7321) acc 81.2500 (64.6750) lr 1.0000e-05 eta 4:17:40
epoch [1/2] batch [255/6250] time 1.109 (1.259) data 0.000 (0.149) loss 0.7996 (0.7307) acc 65.6250 (64.7917) lr 1.0000e-05 eta 4:16:57
epoch [1/2] batch [260/6250] time 1.110 (1.256) data 0.000 (0.146) loss 0.7674 (0.7309) acc 65.6250 (64.7596) lr 1.0000e-05 eta 4:16:16
epoch [1/2] batch [265/6250] time 1.108 (1.253) data 0.000 (0.143) loss 0.6323 (0.7308) acc 68.7500 (64.7759) lr 1.0000e-05 eta 4:15:36
epoch [1/2] batch [270/6250] time 1.108 (1.251) data 0.000 (0.141) loss 0.5909 (0.7318) acc 62.5000 (64.7454) lr 1.0000e-05 eta 4:14:57
epoch [1/2] batch [275/6250] time 1.109 (1.248) data 0.000 (0.138) loss 0.6585 (0.7316) acc 68.7500 (64.6932) lr 1.0000e-05 eta 4:14:19
epoch [1/2] batch [280/6250] time 1.111 (1.246) data 0.000 (0.136) loss 0.5582 (0.7304) acc 71.8750 (64.7545) lr 1.0000e-05 eta 4:13:43
epoch [1/2] batch [285/6250] time 1.107 (1.243) data 0.000 (0.133) loss 0.8632 (0.7295) acc 59.3750 (64.7917) lr 1.0000e-05 eta 4:13:07
epoch [1/2] batch [290/6250] time 1.108 (1.241) data 0.000 (0.131) loss 0.9319 (0.7295) acc 59.3750 (64.7414) lr 1.0000e-05 eta 4:12:32
epoch [1/2] batch [295/6250] time 1.109 (1.239) data 0.001 (0.129) loss 0.8018 (0.7297) acc 56.2500 (64.7140) lr 1.0000e-05 eta 4:11:59
epoch [1/2] batch [300/6250] time 1.106 (1.237) data 0.000 (0.127) loss 0.5695 (0.7282) acc 71.8750 (64.7396) lr 1.0000e-05 eta 4:11:26
epoch [1/2] batch [305/6250] time 1.108 (1.235) data 0.001 (0.125) loss 0.8944 (0.7261) acc 62.5000 (64.8668) lr 1.0000e-05 eta 4:10:55
epoch [1/2] batch [310/6250] time 1.110 (1.232) data 0.000 (0.123) loss 0.6506 (0.7269) acc 68.7500 (64.8185) lr 1.0000e-05 eta 4:10:24
epoch [1/2] batch [315/6250] time 1.111 (1.231) data 0.000 (0.121) loss 0.9592 (0.7282) acc 53.1250 (64.7421) lr 1.0000e-05 eta 4:09:54
epoch [1/2] batch [320/6250] time 1.108 (1.229) data 0.000 (0.119) loss 0.4228 (0.7258) acc 78.1250 (64.8730) lr 1.0000e-05 eta 4:09:24
epoch [1/2] batch [325/6250] time 1.108 (1.227) data 0.000 (0.117) loss 0.4983 (0.7252) acc 78.1250 (64.8750) lr 1.0000e-05 eta 4:08:56
epoch [1/2] batch [330/6250] time 1.109 (1.225) data 0.000 (0.115) loss 0.4755 (0.7232) acc 71.8750 (64.9905) lr 1.0000e-05 eta 4:08:28
epoch [1/2] batch [335/6250] time 1.113 (1.223) data 0.000 (0.113) loss 0.5068 (0.7224) acc 84.3750 (65.1306) lr 1.0000e-05 eta 4:08:01
epoch [1/2] batch [340/6250] time 1.109 (1.222) data 0.000 (0.112) loss 0.6418 (0.7224) acc 62.5000 (65.1011) lr 1.0000e-05 eta 4:07:34
epoch [1/2] batch [345/6250] time 1.109 (1.220) data 0.001 (0.110) loss 0.5718 (0.7215) acc 68.7500 (65.0906) lr 1.0000e-05 eta 4:07:08
epoch [1/2] batch [350/6250] time 1.107 (1.218) data 0.000 (0.109) loss 0.5991 (0.7201) acc 68.7500 (65.1339) lr 1.0000e-05 eta 4:06:43
epoch [1/2] batch [355/6250] time 1.108 (1.217) data 0.000 (0.107) loss 0.4957 (0.7190) acc 75.0000 (65.1849) lr 1.0000e-05 eta 4:06:18
epoch [1/2] batch [360/6250] time 1.106 (1.215) data 0.000 (0.106) loss 0.5513 (0.7183) acc 75.0000 (65.2344) lr 1.0000e-05 eta 4:05:53
epoch [1/2] batch [365/6250] time 1.107 (1.214) data 0.000 (0.104) loss 0.4935 (0.7181) acc 75.0000 (65.2740) lr 1.0000e-05 eta 4:05:30
epoch [1/2] batch [370/6250] time 1.107 (1.212) data 0.000 (0.103) loss 0.5828 (0.7166) acc 68.7500 (65.3801) lr 1.0000e-05 eta 4:05:06
epoch [1/2] batch [375/6250] time 1.108 (1.211) data 0.001 (0.101) loss 0.8325 (0.7169) acc 56.2500 (65.3167) lr 1.0000e-05 eta 4:04:43
epoch [1/2] batch [380/6250] time 1.109 (1.210) data 0.000 (0.100) loss 0.6863 (0.7162) acc 59.3750 (65.3207) lr 1.0000e-05 eta 4:04:21
epoch [1/2] batch [385/6250] time 1.108 (1.208) data 0.000 (0.099) loss 0.8919 (0.7165) acc 68.7500 (65.3003) lr 1.0000e-05 eta 4:03:59
epoch [1/2] batch [390/6250] time 1.110 (1.207) data 0.000 (0.097) loss 0.4977 (0.7168) acc 78.1250 (65.3125) lr 1.0000e-05 eta 4:03:38
epoch [1/2] batch [395/6250] time 1.110 (1.206) data 0.000 (0.096) loss 0.7940 (0.7162) acc 56.2500 (65.3323) lr 1.0000e-05 eta 4:03:17
epoch [1/2] batch [400/6250] time 1.109 (1.205) data 0.001 (0.095) loss 0.5717 (0.7146) acc 71.8750 (65.4453) lr 1.0000e-05 eta 4:02:56
epoch [1/2] batch [405/6250] time 1.108 (1.204) data 0.000 (0.094) loss 0.7576 (0.7130) acc 59.3750 (65.4938) lr 1.0000e-05 eta 4:02:36
epoch [1/2] batch [410/6250] time 1.109 (1.202) data 0.000 (0.093) loss 0.6918 (0.7131) acc 62.5000 (65.4573) lr 1.0000e-05 eta 4:02:16
epoch [1/2] batch [415/6250] time 1.107 (1.201) data 0.000 (0.092) loss 0.5691 (0.7121) acc 68.7500 (65.4895) lr 1.0000e-05 eta 4:01:57
epoch [1/2] batch [420/6250] time 1.110 (1.200) data 0.000 (0.091) loss 0.8139 (0.7126) acc 56.2500 (65.4464) lr 1.0000e-05 eta 4:01:38
epoch [1/2] batch [425/6250] time 1.108 (1.199) data 0.000 (0.089) loss 0.3142 (0.7096) acc 90.6250 (65.6324) lr 1.0000e-05 eta 4:01:19
epoch [1/2] batch [430/6250] time 1.110 (1.198) data 0.000 (0.088) loss 0.4319 (0.7081) acc 81.2500 (65.6904) lr 1.0000e-05 eta 4:01:00
epoch [1/2] batch [435/6250] time 1.110 (1.197) data 0.001 (0.087) loss 0.5778 (0.7067) acc 68.7500 (65.7256) lr 1.0000e-05 eta 4:00:42
epoch [1/2] batch [440/6250] time 1.110 (1.196) data 0.000 (0.086) loss 0.7205 (0.7065) acc 65.6250 (65.7102) lr 1.0000e-05 eta 4:00:24
epoch [1/2] batch [445/6250] time 1.108 (1.195) data 0.000 (0.085) loss 0.5180 (0.7060) acc 71.8750 (65.6952) lr 1.0000e-05 eta 4:00:07
epoch [1/2] batch [450/6250] time 1.108 (1.194) data 0.000 (0.085) loss 0.8005 (0.7059) acc 62.5000 (65.7153) lr 1.0000e-05 eta 3:59:49
epoch [1/2] batch [455/6250] time 1.108 (1.193) data 0.001 (0.084) loss 0.7677 (0.7053) acc 65.6250 (65.7898) lr 1.0000e-05 eta 3:59:32
epoch [1/2] batch [460/6250] time 1.111 (1.192) data 0.000 (0.083) loss 0.8887 (0.7065) acc 53.1250 (65.7133) lr 1.0000e-05 eta 3:59:15
epoch [1/2] batch [465/6250] time 1.108 (1.191) data 0.000 (0.082) loss 0.8332 (0.7059) acc 65.6250 (65.7460) lr 1.0000e-05 eta 3:58:58
epoch [1/2] batch [470/6250] time 1.110 (1.191) data 0.001 (0.081) loss 0.5610 (0.7049) acc 78.1250 (65.7979) lr 1.0000e-05 eta 3:58:42
epoch [1/2] batch [475/6250] time 1.107 (1.190) data 0.000 (0.080) loss 0.6577 (0.7047) acc 68.7500 (65.8355) lr 1.0000e-05 eta 3:58:26
epoch [1/2] batch [480/6250] time 1.107 (1.189) data 0.000 (0.079) loss 0.6425 (0.7045) acc 65.6250 (65.8268) lr 1.0000e-05 eta 3:58:10
epoch [1/2] batch [485/6250] time 1.108 (1.188) data 0.001 (0.078) loss 0.6304 (0.7039) acc 71.8750 (65.8827) lr 1.0000e-05 eta 3:57:54
epoch [1/2] batch [490/6250] time 1.111 (1.187) data 0.001 (0.078) loss 0.6400 (0.7034) acc 78.1250 (65.9439) lr 1.0000e-05 eta 3:57:38
epoch [1/2] batch [495/6250] time 1.112 (1.186) data 0.000 (0.077) loss 0.5460 (0.7037) acc 71.8750 (65.9407) lr 1.0000e-05 eta 3:57:23
epoch [1/2] batch [500/6250] time 1.112 (1.186) data 0.000 (0.076) loss 0.8287 (0.7036) acc 59.3750 (65.9375) lr 1.0000e-05 eta 3:57:08
epoch [1/2] batch [505/6250] time 1.106 (1.185) data 0.001 (0.075) loss 0.7732 (0.7032) acc 59.3750 (65.9468) lr 1.0000e-05 eta 3:56:53
epoch [1/2] batch [510/6250] time 1.110 (1.184) data 0.000 (0.075) loss 0.5907 (0.7027) acc 81.2500 (65.9988) lr 1.0000e-05 eta 3:56:38
epoch [1/2] batch [515/6250] time 1.114 (1.184) data 0.001 (0.074) loss 0.8258 (0.7028) acc 59.3750 (65.9587) lr 1.0000e-05 eta 3:56:24
epoch [1/2] batch [520/6250] time 1.111 (1.183) data 0.000 (0.073) loss 0.4079 (0.7014) acc 78.1250 (66.0156) lr 1.0000e-05 eta 3:56:10
epoch [1/2] batch [525/6250] time 1.111 (1.182) data 0.001 (0.072) loss 0.5410 (0.7005) acc 75.0000 (66.0893) lr 1.0000e-05 eta 3:55:56
epoch [1/2] batch [530/6250] time 1.110 (1.181) data 0.000 (0.072) loss 0.6423 (0.7002) acc 68.7500 (66.0672) lr 1.0000e-05 eta 3:55:42
epoch [1/2] batch [535/6250] time 1.100 (1.181) data 0.000 (0.071) loss 0.6482 (0.6995) acc 65.6250 (66.0923) lr 1.0000e-05 eta 3:55:27
epoch [1/2] batch [540/6250] time 1.109 (1.180) data 0.001 (0.070) loss 0.3625 (0.6991) acc 81.2500 (66.0764) lr 1.0000e-05 eta 3:55:13
epoch [1/2] batch [545/6250] time 1.110 (1.179) data 0.001 (0.070) loss 0.6592 (0.6987) acc 62.5000 (66.1067) lr 1.0000e-05 eta 3:55:00
epoch [1/2] batch [550/6250] time 1.111 (1.179) data 0.000 (0.069) loss 0.5138 (0.6979) acc 78.1250 (66.1136) lr 1.0000e-05 eta 3:54:46
epoch [1/2] batch [555/6250] time 1.111 (1.178) data 0.001 (0.069) loss 0.5248 (0.6974) acc 84.3750 (66.1655) lr 1.0000e-05 eta 3:54:33
epoch [1/2] batch [560/6250] time 1.110 (1.178) data 0.000 (0.068) loss 0.5791 (0.6969) acc 65.6250 (66.1719) lr 1.0000e-05 eta 3:54:20
epoch [1/2] batch [565/6250] time 1.095 (1.177) data 0.000 (0.067) loss 0.5966 (0.6961) acc 71.8750 (66.1947) lr 1.0000e-05 eta 3:54:06
epoch [1/2] batch [570/6250] time 1.130 (1.176) data 0.001 (0.067) loss 0.8498 (0.6969) acc 62.5000 (66.1513) lr 1.0000e-05 eta 3:53:54
