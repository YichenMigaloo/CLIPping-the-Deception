{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a81e766-cbf8-4837-a449-45a8d8b64f25",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c798a272-e435-4507-b24f-33d50e22f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import glob\n",
    "from sklearn.metrics import average_precision_score, accuracy_score, roc_auc_score, precision_recall_curve\n",
    "\n",
    "from dassl.utils import setup_logger, set_random_seed, collect_env_info\n",
    "from dassl.config import get_cfg_default\n",
    "from dassl.engine import build_trainer\n",
    "\n",
    "# custom\n",
    "\n",
    "import datasets.imagenet\n",
    "import datasets.guided\n",
    "import datasets.biggan\n",
    "import datasets.cyclegan\n",
    "import datasets.dalle2\n",
    "import datasets.deepfake\n",
    "import datasets.gaugan\n",
    "import datasets.glide_50_27\n",
    "import datasets.glide_100_10\n",
    "import datasets.glide_100_27\n",
    "import datasets.ldm_100\n",
    "import datasets.ldm_200\n",
    "import datasets.ldm_200_cfg\n",
    "import datasets.stargan\n",
    "import datasets.stylegan\n",
    "import datasets.stylegan2\n",
    "import datasets.stylegan3\n",
    "import datasets.sd_512x512\n",
    "import datasets.sdxl\n",
    "import datasets.dalle3\n",
    "import datasets.taming\n",
    "import datasets.eg3d\n",
    "import datasets.firefly\n",
    "import datasets.midjourney_v5\n",
    "import datasets.progan\n",
    "import datasets.faceswap\n",
    "\n",
    "\n",
    "import trainers.coop\n",
    "import trainers.clip_adapter\n",
    "import trainers.clip_zero_shot\n",
    "import trainers.cocoop\n",
    "import trainers.zsclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9b449fe-d023-4a73-8031-0511b628a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from eval_utils import print_args, reset_cfg, extend_cfg, setup_cfg, get_parsed_args\n",
    "\n",
    "from eval_utils_fine_tuned import print_args, reset_cfg, extend_cfg, setup_cfg, get_parsed_args\n",
    "# from eval_utils_zero_shot import print_args, reset_cfg, extend_cfg, setup_cfg, get_parsed_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "761ec7aa-eb30-4bfa-92ad-872ef707a545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/',\n",
       " '../finetuned_clip/finetuned_clips/finetuned_2_epoch_80k/',\n",
       " '../finetuned_clip/finetuned_clips/finetuned_2_epoch_60k/',\n",
       " '../finetuned_clip/finetuned_clips/finetuned_2_epoch_40k/',\n",
       " '../finetuned_clip/finetuned_clips/finetuned_2_epoch_20k/']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_dirs = sorted(glob.glob('../CoOp_Trained/selected_models/*/'))\n",
    "# model_dirs = [path.replace('\\\\','/') for path in model_dirs]\n",
    "# model_dirs\n",
    "\n",
    "model_dirs = sorted(glob.glob('../finetuned_clip/finetuned_clips/*/'))\n",
    "model_dirs = [path.replace('\\\\','/') for path in model_dirs]\n",
    "model_dirs = model_dirs[::-1]\n",
    "model_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4d75469-71dd-44d6-8fbe-24eedded166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dirs[1].split('/')[-2].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c0305d0-5583-4fb4-8dbf-660015eff8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['progan', 'biggan', 'cyclegan', 'eg3d', 'gaugan',  'stargan', 'stylegan', 'stylegan2', 'stylegan3', \n",
    "                 'dalle2', 'glide_50_27', 'glide_100_10', 'glide_100_27', 'guided', 'ldm_100', 'ldm_200', 'ldm_200_cfg',\n",
    "                 'sd_512x512', 'sdxl', 'taming', 'deepfake', 'firefly', 'midjourney_v5', 'dalle3', 'faceswap']\n",
    "# dataset_names = ['deepfake']\n",
    "                #  'eg3d', 'stylegan3'] 'dalle2', 'deepfake', 'glide_50_27', 'glide_100_10', 'glide_100_27',\n",
    "                # 'guided', 'ldm_100', 'ldm_200', 'ldm_200_cfg',\n",
    "# dataset_names = ['firefly', 'midjourney_v5', 'dalle3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb68c87b-a54c-4c20-9488-b6df9cc19e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e16509f2-38d3-4a00-8fbd-0f53c75709f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def update_and_save_evaluation(model_name, dataset_name, accuracy, f1_score, average_precision):\n",
    "    global model_evaluations\n",
    "    \n",
    "    # Check if the model key exists in the dictionary\n",
    "    if model_name not in model_evaluations:\n",
    "        model_evaluations[model_name] = {}\n",
    "    \n",
    "    # Check if the dataset key exists for the given model\n",
    "    if dataset_name not in model_evaluations[model_name]:\n",
    "        model_evaluations[model_name][dataset_name] = {}\n",
    "    \n",
    "    # Update evaluation results\n",
    "    model_evaluations[model_name][dataset_name][\"accuracy\"] = accuracy\n",
    "    model_evaluations[model_name][dataset_name][\"f1_score\"] = f1_score\n",
    "    model_evaluations[model_name][dataset_name][\"average_precision\"] = average_precision\n",
    "    \n",
    "    # Save the updated dictionary to a JSON file\n",
    "    with open(\"finetuned_model_evaluations_2_sigma.json\", \"w\") as json_file:\n",
    "        json.dump(model_evaluations, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b223ab9-988b-445d-b4aa-72a579281aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "0\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/progan.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: progan\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: progan\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  ------\n",
      "Dataset    progan\n",
      "# classes  2\n",
      "# train_x  8,000\n",
      "# val      8,000\n",
      "# test     8,000\n",
      "---------  ------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [02:08<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 8,000\n",
      "* correct: 5,607\n",
      "* average_precision: 95.4%\n",
      "* accuracy: 70.1%\n",
      "* error: 29.9%\n",
      "* macro_f1: 67.2%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/biggan.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: biggan\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: biggan\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  ------\n",
      "Dataset    biggan\n",
      "# classes  2\n",
      "# train_x  4,000\n",
      "# val      4,000\n",
      "# test     4,000\n",
      "---------  ------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:16<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 4,000\n",
      "* correct: 2,305\n",
      "* average_precision: 81.8%\n",
      "* accuracy: 57.6%\n",
      "* error: 42.4%\n",
      "* macro_f1: 48.4%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/cyclegan.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: cyclegan\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: cyclegan\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  --------\n",
      "Dataset    cyclegan\n",
      "# classes  2\n",
      "# train_x  2,000\n",
      "# val      2,000\n",
      "# test     2,000\n",
      "---------  --------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:50<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 2,000\n",
      "* correct: 1,155\n",
      "* average_precision: 84.3%\n",
      "* accuracy: 57.8%\n",
      "* error: 42.2%\n",
      "* macro_f1: 48.6%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/eg3d.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: eg3d\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: eg3d\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -----\n",
      "Dataset    eg3d\n",
      "# classes  2\n",
      "# train_x  2,000\n",
      "# val      2,000\n",
      "# test     2,000\n",
      "---------  -----\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:51<00:00,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 2,000\n",
      "* correct: 1,267\n",
      "* average_precision: 96.2%\n",
      "* accuracy: 63.4%\n",
      "* error: 36.6%\n",
      "* macro_f1: 57.7%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/gaugan.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: gaugan\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: gaugan\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  ------\n",
      "Dataset    gaugan\n",
      "# classes  2\n",
      "# train_x  10,000\n",
      "# val      10,000\n",
      "# test     10,000\n",
      "---------  ------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:33<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 10,000\n",
      "* correct: 5,188\n",
      "* average_precision: 87.2%\n",
      "* accuracy: 51.9%\n",
      "* error: 48.1%\n",
      "* macro_f1: 37.4%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/stargan.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: stargan\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: stargan\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------\n",
      "Dataset    stargan\n",
      "# classes  2\n",
      "# train_x  3,998\n",
      "# val      3,998\n",
      "# test     3,998\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:18<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 3,998\n",
      "* correct: 3,873\n",
      "* average_precision: 99.9%\n",
      "* accuracy: 96.9%\n",
      "* error: 3.1%\n",
      "* macro_f1: 96.9%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/stylegan.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: stylegan\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: stylegan\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  --------\n",
      "Dataset    stylegan\n",
      "# classes  2\n",
      "# train_x  2,000\n",
      "# val      2,000\n",
      "# test     2,000\n",
      "---------  --------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:51<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 2,000\n",
      "* correct: 1,722\n",
      "* average_precision: 93.1%\n",
      "* accuracy: 86.1%\n",
      "* error: 13.9%\n",
      "* macro_f1: 85.9%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/stylegan2.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: stylegan2\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: stylegan2\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  ---------\n",
      "Dataset    stylegan2\n",
      "# classes  2\n",
      "# train_x  2,000\n",
      "# val      2,000\n",
      "# test     2,000\n",
      "---------  ---------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:51<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 2,000\n",
      "* correct: 1,653\n",
      "* average_precision: 93.4%\n",
      "* accuracy: 82.7%\n",
      "* error: 17.3%\n",
      "* macro_f1: 82.1%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/stylegan3.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: stylegan3\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: stylegan3\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  ---------\n",
      "Dataset    stylegan3\n",
      "# classes  2\n",
      "# train_x  1,900\n",
      "# val      1,900\n",
      "# test     1,900\n",
      "---------  ---------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:50<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 1,900\n",
      "* correct: 1,569\n",
      "* average_precision: 95.6%\n",
      "* accuracy: 82.6%\n",
      "* error: 17.4%\n",
      "* macro_f1: 82.0%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/dalle2.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: dalle2\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: dalle2\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  ------\n",
      "Dataset    dalle2\n",
      "# classes  2\n",
      "# train_x  2,000\n",
      "# val      2,000\n",
      "# test     2,000\n",
      "---------  ------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:50<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 2,000\n",
      "* correct: 1,254\n",
      "* average_precision: 85.1%\n",
      "* accuracy: 62.7%\n",
      "* error: 37.3%\n",
      "* macro_f1: 57.1%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/glide_50_27.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: glide_50_27\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: glide_50_27\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -----------\n",
      "Dataset    glide_50_27\n",
      "# classes  2\n",
      "# train_x  2,000\n",
      "# val      2,000\n",
      "# test     2,000\n",
      "---------  -----------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:50<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 2,000\n",
      "* correct: 1,255\n",
      "* average_precision: 81.1%\n",
      "* accuracy: 62.8%\n",
      "* error: 37.2%\n",
      "* macro_f1: 57.2%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/glide_100_10.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: glide_100_10\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: glide_100_10\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  ------------\n",
      "Dataset    glide_100_10\n",
      "# classes  2\n",
      "# train_x  2,000\n",
      "# val      2,000\n",
      "# test     2,000\n",
      "---------  ------------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:51<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 2,000\n",
      "* correct: 1,252\n",
      "* average_precision: 80.5%\n",
      "* accuracy: 62.6%\n",
      "* error: 37.4%\n",
      "* macro_f1: 57.1%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/glide_100_27.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: glide_100_27\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: glide_100_27\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  ------------\n",
      "Dataset    glide_100_27\n",
      "# classes  2\n",
      "# train_x  2,000\n",
      "# val      2,000\n",
      "# test     2,000\n",
      "---------  ------------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:50<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 2,000\n",
      "* correct: 1,257\n",
      "* average_precision: 80.8%\n",
      "* accuracy: 62.9%\n",
      "* error: 37.1%\n",
      "* macro_f1: 57.3%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/guided.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: guided\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: guided\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  ------\n",
      "Dataset    guided\n",
      "# classes  2\n",
      "# train_x  2,000\n",
      "# val      2,000\n",
      "# test     2,000\n",
      "---------  ------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:50<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 2,000\n",
      "* correct: 1,246\n",
      "* average_precision: 89.9%\n",
      "* accuracy: 62.3%\n",
      "* error: 37.7%\n",
      "* macro_f1: 56.8%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/ldm_100.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: ldm_100\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: ldm_100\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------\n",
      "Dataset    ldm_100\n",
      "# classes  2\n",
      "# train_x  2,000\n",
      "# val      2,000\n",
      "# test     2,000\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:49<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 2,000\n",
      "* correct: 1,250\n",
      "* average_precision: 88.1%\n",
      "* accuracy: 62.5%\n",
      "* error: 37.5%\n",
      "* macro_f1: 57.0%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/ldm_200.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: ldm_200\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: ldm_200\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------\n",
      "Dataset    ldm_200\n",
      "# classes  2\n",
      "# train_x  2,000\n",
      "# val      2,000\n",
      "# test     2,000\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:49<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 2,000\n",
      "* correct: 1,247\n",
      "* average_precision: 88.0%\n",
      "* accuracy: 62.4%\n",
      "* error: 37.6%\n",
      "* macro_f1: 56.9%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/ldm_200_cfg.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: ldm_200_cfg\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: ldm_200_cfg\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -----------\n",
      "Dataset    ldm_200_cfg\n",
      "# classes  2\n",
      "# train_x  2,000\n",
      "# val      2,000\n",
      "# test     2,000\n",
      "---------  -----------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:50<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 2,000\n",
      "* correct: 1,223\n",
      "* average_precision: 75.5%\n",
      "* accuracy: 61.1%\n",
      "* error: 38.9%\n",
      "* macro_f1: 55.9%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/sd_512x512.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: sd_512x512\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: sd_512x512\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  ----------\n",
      "Dataset    sd_512x512\n",
      "# classes  2\n",
      "# train_x  2,000\n",
      "# val      2,000\n",
      "# test     2,000\n",
      "---------  ----------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:52<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 2,000\n",
      "* correct: 1,217\n",
      "* average_precision: 82.0%\n",
      "* accuracy: 60.9%\n",
      "* error: 39.1%\n",
      "* macro_f1: 55.7%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/sdxl.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: sdxl\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: sdxl\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -----\n",
      "Dataset    sdxl\n",
      "# classes  2\n",
      "# train_x  2,000\n",
      "# val      2,000\n",
      "# test     2,000\n",
      "---------  -----\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:52<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 2,000\n",
      "* correct: 1,249\n",
      "* average_precision: 92.5%\n",
      "* accuracy: 62.5%\n",
      "* error: 37.5%\n",
      "* macro_f1: 56.9%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/taming.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: taming\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: taming\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  ------\n",
      "Dataset    taming\n",
      "# classes  2\n",
      "# train_x  2,000\n",
      "# val      2,000\n",
      "# test     2,000\n",
      "---------  ------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:51<00:00,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 2,000\n",
      "* correct: 1,259\n",
      "* average_precision: 85.0%\n",
      "* accuracy: 63.0%\n",
      "* error: 37.0%\n",
      "* macro_f1: 57.3%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/deepfake.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: deepfake\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: deepfake\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  --------\n",
      "Dataset    deepfake\n",
      "# classes  2\n",
      "# train_x  5,396\n",
      "# val      5,396\n",
      "# test     5,396\n",
      "---------  --------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [01:34<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 5,396\n",
      "* correct: 3,428\n",
      "* average_precision: 70.0%\n",
      "* accuracy: 63.5%\n",
      "* error: 36.5%\n",
      "* macro_f1: 62.4%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/firefly.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: firefly\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: firefly\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------\n",
      "Dataset    firefly\n",
      "# classes  2\n",
      "# train_x  2,000\n",
      "# val      2,000\n",
      "# test     2,000\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:20<00:00,  4.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 2,000\n",
      "* correct: 1,244\n",
      "* average_precision: 90.2%\n",
      "* accuracy: 62.2%\n",
      "* error: 37.8%\n",
      "* macro_f1: 56.7%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/midjourney_v5.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: midjourney_v5\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: midjourney_v5\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    midjourney_v5\n",
      "# classes  2\n",
      "# train_x  2,000\n",
      "# val      2,000\n",
      "# test     2,000\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:51<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 2,000\n",
      "* correct: 1,251\n",
      "* average_precision: 83.8%\n",
      "* accuracy: 62.5%\n",
      "* error: 37.5%\n",
      "* macro_f1: 57.0%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/dalle3.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: dalle3\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: dalle3\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  ------\n",
      "Dataset    dalle3\n",
      "# classes  2\n",
      "# train_x  2,000\n",
      "# val      2,000\n",
      "# test     2,000\n",
      "---------  ------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:51<00:00,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 2,000\n",
      "* correct: 1,184\n",
      "* average_precision: 78.6%\n",
      "* accuracy: 59.2%\n",
      "* error: 40.8%\n",
      "* macro_f1: 54.4%\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/faceswap.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 1\n",
      "model_dir: ../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CLIP_Adapter\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: faceswap\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CLIP_Adapter\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CLIP_Adapter\n",
      "Loading dataset: faceswap\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  --------\n",
      "Dataset    faceswap\n",
      "# classes  2\n",
      "# train_x  5,600\n",
      "# val      5,600\n",
      "# test     5,600\n",
      "---------  --------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Trainable Parameters:  427616513.0\n",
      "Loading evaluator: Classification\n",
      "Loading weights to clip_adapter from \"../finetuned_clip/finetuned_clips/finetuned_from_own_pc_1_epoch_100k/clip_adapter\\model.pth.tar-1\" (epoch = 1)\n",
      "torch.float32\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [01:37<00:00,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 5,600\n",
      "* correct: 3,208\n",
      "* average_precision: 58.8%\n",
      "* accuracy: 57.3%\n",
      "* error: 42.7%\n",
      "* macro_f1: 57.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # model_evaluations = {}\n",
    "# # for model in model_dirs:\n",
    "# #     num_ctx_tokens = int(model.split('/')[-2].split('_')[1])\n",
    "# #     print(num_ctx_tokens)\n",
    "# #     for dataset in dataset_names:\n",
    "# #         args = get_parsed_args(model, dataset, num_ctx_tokens)\n",
    "# #         cfg = setup_cfg(args)\n",
    "# #         print(\"Setting fixed seed: {}\".format(cfg.SEED))\n",
    "# #         set_random_seed(cfg.SEED)\n",
    "# #         if torch.cuda.is_available() and cfg.USE_CUDA:\n",
    "# #             print('Using CUDA!!!')\n",
    "# #             torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# #         print_args(args, cfg)\n",
    "# #         print(\"Collecting env info ...\")\n",
    "# #         print(\"** System info **\\n{}\\n\".format(collect_env_info()))\n",
    "\n",
    "# #         trainer = build_trainer(cfg)\n",
    "# #         trainer.load_model(args.model_dir, epoch=args.load_epoch)\n",
    "\n",
    "# #         results, results_dict = trainer.test()\n",
    "# #         update_and_save_evaluation(model, dataset, results_dict['accuracy'], results_dict['macro_f1'], results_dict['average_precision'])\n",
    "\n",
    "\n",
    "model_evaluations = {}\n",
    "for model in model_dirs:\n",
    "    print(model)\n",
    "    num_ctx_tokens = 0\n",
    "    print(num_ctx_tokens)\n",
    "    for dataset in dataset_names:\n",
    "        args = get_parsed_args(model, dataset)\n",
    "        cfg = setup_cfg(args)\n",
    "        print(\"Setting fixed seed: {}\".format(cfg.SEED))\n",
    "        set_random_seed(cfg.SEED)\n",
    "        if torch.cuda.is_available() and cfg.USE_CUDA:\n",
    "            print('Using CUDA!!!')\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "\n",
    "        print_args(args, cfg)\n",
    "        print(\"Collecting env info ...\")\n",
    "        print(\"** System info **\\n{}\\n\".format(collect_env_info()))\n",
    "\n",
    "        trainer = build_trainer(cfg)\n",
    "        trainer.load_model(args.model_dir, epoch=args.load_epoch)\n",
    "        print(trainer.model.dtype)\n",
    "        results, results_dict = trainer.test()\n",
    "        update_and_save_evaluation(model, dataset, results_dict['accuracy'], results_dict['macro_f1'], results_dict['average_precision'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc4bee28-59d2-41ba-8fee-7ef7956a675c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "s = model_dirs[0].split('/')[-2].split('_')[1]\n",
    "int(re.split('(\\d+)',s)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5267fdf1-1d1e-4db2-abb6-4f0ab31cf932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/faceswap.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 2\n",
      "model_dir: ../CoOp_Trained/selected_models/100000_16context_best_until_now/\n",
      "no_train: False\n",
      "num_ctx_tokens: 16\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CoOp\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: faceswap\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CoOp\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CoOp\n",
      "Loading dataset: faceswap\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  --------\n",
      "Dataset    faceswap\n",
      "# classes  2\n",
      "# train_x  5,600\n",
      "# val      5,600\n",
      "# test     5,600\n",
      "---------  --------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Loading evaluator: Classification\n",
      "Loading weights to prompt_learner from \"../CoOp_Trained/selected_models/100000_16context_best_until_now/prompt_learner\\model.pth.tar-2\" (epoch = 2)\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:53<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 5,600\n",
      "* correct: 4,181\n",
      "* average_precision: 88.0%\n",
      "* accuracy: 74.7%\n",
      "* error: 25.3%\n",
      "* macro_f1: 73.6%\n",
      "4\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/faceswap.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 2\n",
      "model_dir: ../CoOp_Trained/selected_models/100000_4context/\n",
      "no_train: False\n",
      "num_ctx_tokens: 4\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CoOp\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: faceswap\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 4\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CoOp\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CoOp\n",
      "Loading dataset: faceswap\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  --------\n",
      "Dataset    faceswap\n",
      "# classes  2\n",
      "# train_x  5,600\n",
      "# val      5,600\n",
      "# test     5,600\n",
      "---------  --------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Loading evaluator: Classification\n",
      "Loading weights to prompt_learner from \"../CoOp_Trained/selected_models/100000_4context/prompt_learner\\model.pth.tar-2\" (epoch = 2)\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:52<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 5,600\n",
      "* correct: 3,878\n",
      "* average_precision: 83.6%\n",
      "* accuracy: 69.2%\n",
      "* error: 30.8%\n",
      "* macro_f1: 66.9%\n",
      "8\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/faceswap.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 2\n",
      "model_dir: ../CoOp_Trained/selected_models/100000_8context/\n",
      "no_train: False\n",
      "num_ctx_tokens: 8\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CoOp\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: faceswap\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 8\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CoOp\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CoOp\n",
      "Loading dataset: faceswap\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  --------\n",
      "Dataset    faceswap\n",
      "# classes  2\n",
      "# train_x  5,600\n",
      "# val      5,600\n",
      "# test     5,600\n",
      "---------  --------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Loading evaluator: Classification\n",
      "Loading weights to prompt_learner from \"../CoOp_Trained/selected_models/100000_8context/prompt_learner\\model.pth.tar-2\" (epoch = 2)\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:54<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 5,600\n",
      "* correct: 3,942\n",
      "* average_precision: 86.1%\n",
      "* accuracy: 70.4%\n",
      "* error: 29.6%\n",
      "* macro_f1: 67.9%\n",
      "16\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/faceswap.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 2\n",
      "model_dir: ../CoOp_Trained/selected_models/10000_16context/\n",
      "no_train: False\n",
      "num_ctx_tokens: 16\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CoOp\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: faceswap\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CoOp\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CoOp\n",
      "Loading dataset: faceswap\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  --------\n",
      "Dataset    faceswap\n",
      "# classes  2\n",
      "# train_x  5,600\n",
      "# val      5,600\n",
      "# test     5,600\n",
      "---------  --------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Loading evaluator: Classification\n",
      "Loading weights to prompt_learner from \"../CoOp_Trained/selected_models/10000_16context/prompt_learner\\model.pth.tar-2\" (epoch = 2)\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:53<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 5,600\n",
      "* correct: 3,874\n",
      "* average_precision: 85.3%\n",
      "* accuracy: 69.2%\n",
      "* error: 30.8%\n",
      "* macro_f1: 66.6%\n",
      "4\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/faceswap.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 2\n",
      "model_dir: ../CoOp_Trained/selected_models/10000_4context/\n",
      "no_train: False\n",
      "num_ctx_tokens: 4\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CoOp\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: faceswap\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 4\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CoOp\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CoOp\n",
      "Loading dataset: faceswap\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  --------\n",
      "Dataset    faceswap\n",
      "# classes  2\n",
      "# train_x  5,600\n",
      "# val      5,600\n",
      "# test     5,600\n",
      "---------  --------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Loading evaluator: Classification\n",
      "Loading weights to prompt_learner from \"../CoOp_Trained/selected_models/10000_4context/prompt_learner\\model.pth.tar-2\" (epoch = 2)\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:53<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 5,600\n",
      "* correct: 3,512\n",
      "* average_precision: 80.6%\n",
      "* accuracy: 62.7%\n",
      "* error: 37.3%\n",
      "* macro_f1: 57.2%\n",
      "8\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/faceswap.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 2\n",
      "model_dir: ../CoOp_Trained/selected_models/10000_8context/\n",
      "no_train: False\n",
      "num_ctx_tokens: 8\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CoOp\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: faceswap\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 8\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CoOp\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CoOp\n",
      "Loading dataset: faceswap\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  --------\n",
      "Dataset    faceswap\n",
      "# classes  2\n",
      "# train_x  5,600\n",
      "# val      5,600\n",
      "# test     5,600\n",
      "---------  --------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Loading evaluator: Classification\n",
      "Loading weights to prompt_learner from \"../CoOp_Trained/selected_models/10000_8context/prompt_learner\\model.pth.tar-2\" (epoch = 2)\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:54<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 5,600\n",
      "* correct: 3,767\n",
      "* average_precision: 77.6%\n",
      "* accuracy: 67.3%\n",
      "* error: 32.7%\n",
      "* macro_f1: 65.8%\n",
      "16\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/faceswap.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 2\n",
      "model_dir: ../CoOp_Trained/selected_models/20000_16context_best_until_now/\n",
      "no_train: False\n",
      "num_ctx_tokens: 16\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CoOp\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: faceswap\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CoOp\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CoOp\n",
      "Loading dataset: faceswap\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  --------\n",
      "Dataset    faceswap\n",
      "# classes  2\n",
      "# train_x  5,600\n",
      "# val      5,600\n",
      "# test     5,600\n",
      "---------  --------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Loading evaluator: Classification\n",
      "Loading weights to prompt_learner from \"../CoOp_Trained/selected_models/20000_16context_best_until_now/prompt_learner\\model.pth.tar-2\" (epoch = 2)\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:54<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 5,600\n",
      "* correct: 4,264\n",
      "* average_precision: 87.8%\n",
      "* accuracy: 76.1%\n",
      "* error: 23.9%\n",
      "* macro_f1: 75.6%\n",
      "4\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/faceswap.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 2\n",
      "model_dir: ../CoOp_Trained/selected_models/20000_4context/\n",
      "no_train: False\n",
      "num_ctx_tokens: 4\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CoOp\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: faceswap\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 4\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CoOp\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CoOp\n",
      "Loading dataset: faceswap\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  --------\n",
      "Dataset    faceswap\n",
      "# classes  2\n",
      "# train_x  5,600\n",
      "# val      5,600\n",
      "# test     5,600\n",
      "---------  --------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Loading evaluator: Classification\n",
      "Loading weights to prompt_learner from \"../CoOp_Trained/selected_models/20000_4context/prompt_learner\\model.pth.tar-2\" (epoch = 2)\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:55<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 5,600\n",
      "* correct: 3,565\n",
      "* average_precision: 85.8%\n",
      "* accuracy: 63.7%\n",
      "* error: 36.3%\n",
      "* macro_f1: 58.2%\n",
      "8\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/faceswap.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 2\n",
      "model_dir: ../CoOp_Trained/selected_models/20000_8context/\n",
      "no_train: False\n",
      "num_ctx_tokens: 8\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CoOp\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: faceswap\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 8\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CoOp\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CoOp\n",
      "Loading dataset: faceswap\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  --------\n",
      "Dataset    faceswap\n",
      "# classes  2\n",
      "# train_x  5,600\n",
      "# val      5,600\n",
      "# test     5,600\n",
      "---------  --------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Loading evaluator: Classification\n",
      "Loading weights to prompt_learner from \"../CoOp_Trained/selected_models/20000_8context/prompt_learner\\model.pth.tar-2\" (epoch = 2)\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:51<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 5,600\n",
      "* correct: 3,722\n",
      "* average_precision: 84.7%\n",
      "* accuracy: 66.5%\n",
      "* error: 33.5%\n",
      "* macro_f1: 62.4%\n",
      "16\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/faceswap.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 2\n",
      "model_dir: ../CoOp_Trained/selected_models/30000_16context/\n",
      "no_train: False\n",
      "num_ctx_tokens: 16\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CoOp\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: faceswap\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CoOp\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CoOp\n",
      "Loading dataset: faceswap\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  --------\n",
      "Dataset    faceswap\n",
      "# classes  2\n",
      "# train_x  5,600\n",
      "# val      5,600\n",
      "# test     5,600\n",
      "---------  --------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Loading evaluator: Classification\n",
      "Loading weights to prompt_learner from \"../CoOp_Trained/selected_models/30000_16context/prompt_learner\\model.pth.tar-2\" (epoch = 2)\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:54<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 5,600\n",
      "* correct: 3,852\n",
      "* average_precision: 84.6%\n",
      "* accuracy: 68.8%\n",
      "* error: 31.2%\n",
      "* macro_f1: 65.8%\n",
      "4\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/faceswap.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 2\n",
      "model_dir: ../CoOp_Trained/selected_models/30000_4context/\n",
      "no_train: False\n",
      "num_ctx_tokens: 4\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CoOp\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: faceswap\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 4\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CoOp\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CoOp\n",
      "Loading dataset: faceswap\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  --------\n",
      "Dataset    faceswap\n",
      "# classes  2\n",
      "# train_x  5,600\n",
      "# val      5,600\n",
      "# test     5,600\n",
      "---------  --------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Loading evaluator: Classification\n",
      "Loading weights to prompt_learner from \"../CoOp_Trained/selected_models/30000_4context/prompt_learner\\model.pth.tar-2\" (epoch = 2)\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:53<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 5,600\n",
      "* correct: 3,371\n",
      "* average_precision: 86.1%\n",
      "* accuracy: 60.2%\n",
      "* error: 39.8%\n",
      "* macro_f1: 52.8%\n",
      "8\n",
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/faceswap.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 2\n",
      "model_dir: ../CoOp_Trained/selected_models/30000_8context/\n",
      "no_train: False\n",
      "num_ctx_tokens: 8\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CoOp\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 16\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: faceswap\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 8\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CoOp\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n",
      "Loading trainer: CoOp\n",
      "Loading dataset: faceswap\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  --------\n",
      "Dataset    faceswap\n",
      "# classes  2\n",
      "# train_x  5,600\n",
      "# val      5,600\n",
      "# test     5,600\n",
      "---------  --------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "CLIP OpenAI state dict loaded!\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Loading evaluator: Classification\n",
      "Loading weights to prompt_learner from \"../CoOp_Trained/selected_models/30000_8context/prompt_learner\\model.pth.tar-2\" (epoch = 2)\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 15/56 [00:33<01:31,  2.22s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m build_trainer(cfg)\n\u001b[0;32m     20\u001b[0m trainer\u001b[38;5;241m.\u001b[39mload_model(args\u001b[38;5;241m.\u001b[39mmodel_dir, epoch\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mload_epoch)\n\u001b[1;32m---> 22\u001b[0m results, results_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m update_and_save_evaluation(model, dataset, results_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], results_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro_f1\u001b[39m\u001b[38;5;124m'\u001b[39m], results_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_precision\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\.conda\\envs\\dassl\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Research\\PhD\\Year3\\DeepfakeDetection\\CoOp\\dassl\\engine\\trainer.py:465\u001b[0m, in \u001b[0;36mSimpleTrainer.test\u001b[1;34m(self, split)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(data_loader)):\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28minput\u001b[39m, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_batch_test(batch)\n\u001b[1;32m--> 465\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluator\u001b[38;5;241m.\u001b[39mprocess(output, label)\n\u001b[0;32m    468\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluator\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32m~\\Desktop\\Research\\PhD\\Year3\\DeepfakeDetection\\CoOp\\dassl\\engine\\trainer.py:477\u001b[0m, in \u001b[0;36mSimpleTrainer.model_inference\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_inference\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\dassl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\dassl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Research\\PhD\\Year3\\DeepfakeDetection\\CoOp\\trainers\\coop.py:200\u001b[0m, in \u001b[0;36mCustomCLIP.forward\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    198\u001b[0m prompts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_learner()\n\u001b[0;32m    199\u001b[0m tokenized_prompts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenized_prompts\n\u001b[1;32m--> 200\u001b[0m text_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_prompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m image_features \u001b[38;5;241m=\u001b[39m image_features \u001b[38;5;241m/\u001b[39m image_features\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    203\u001b[0m text_features \u001b[38;5;241m=\u001b[39m text_features \u001b[38;5;241m/\u001b[39m text_features\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\dassl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\dassl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Research\\PhD\\Year3\\DeepfakeDetection\\CoOp\\trainers\\coop.py:55\u001b[0m, in \u001b[0;36mTextEncoder.forward\u001b[1;34m(self, prompts, tokenized_prompts)\u001b[0m\n\u001b[0;32m     51\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_final(x)\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# x.shape = [batch_size, n_ctx, transformer.width]\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# take features from the eot embedding (eot_token is the highest number in each sequence)\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_prompts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_projection\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_evaluations = {}\n",
    "for model in model_dirs:\n",
    "    splitted_string = model.split('/')[-2].split('_')[1]\n",
    "    num_ctx_tokens = int(re.split('(\\d+)',splitted_string)[1])\n",
    "    print(num_ctx_tokens)\n",
    "    for dataset in dataset_names:\n",
    "        args = get_parsed_args(model, dataset, num_ctx_tokens)\n",
    "        cfg = setup_cfg(args)\n",
    "        print(\"Setting fixed seed: {}\".format(cfg.SEED))\n",
    "        set_random_seed(cfg.SEED)\n",
    "        if torch.cuda.is_available() and cfg.USE_CUDA:\n",
    "            print('Using CUDA!!!')\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "\n",
    "        print_args(args, cfg)\n",
    "        print(\"Collecting env info ...\")\n",
    "        print(\"** System info **\\n{}\\n\".format(collect_env_info()))\n",
    "\n",
    "        trainer = build_trainer(cfg)\n",
    "        trainer.load_model(args.model_dir, epoch=args.load_epoch)\n",
    "\n",
    "        results, results_dict = trainer.test()\n",
    "        update_and_save_evaluation(model, dataset, results_dict['accuracy'], results_dict['macro_f1'], results_dict['average_precision'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d989268-dd01-4a7c-8b6d-ad064641c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==============UNTIL HERE=================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92b532ac-6c6c-4027-ab8d-41b5f99cfe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting fixed seed: 17\n",
      "Using CUDA!!!\n"
     ]
    }
   ],
   "source": [
    "args = get_parsed_args(model_dirs[0], 'guided')\n",
    "cfg = setup_cfg(args)\n",
    "print(\"Setting fixed seed: {}\".format(cfg.SEED))\n",
    "set_random_seed(cfg.SEED)\n",
    "if torch.cuda.is_available() and cfg.USE_CUDA:\n",
    "    print('Using CUDA!!!')\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb168993-87e7-4e28-95d7-620a480894f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: ../CoOp/configs/trainers/coop/vit_l14_ep2.yaml\n",
      "dataset_config_file: ../CoOp/configs/datasets/guided.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 2\n",
      "model_dir: ../CoOp_Trained/selected_models/100000_16context_best_until_now/\n",
      "no_train: False\n",
      "opts: []\n",
      "output_dir: ../CoOp/outputs/\n",
      "resume: \n",
      "root: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "seed: 17\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: CoOp\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 8\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: guided\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: -1\n",
      "  ROOT: ../Datasets/ICMRDataset/test/deepfake_eval/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: all\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize', 'compress_blur')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-L/14\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.002\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 2\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: ../CoOp/outputs/\n",
      "RESUME: \n",
      "SEED: 17\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 5\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: front\n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 16\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: CoOp\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.1.2+cu118\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.8\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22631-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.105\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\n",
      "Nvidia driver version: 546.65\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture=9\n",
      "CurrentClockSpeed=3801\n",
      "DeviceID=CPU0\n",
      "Family=107\n",
      "L2CacheSize=4096\n",
      "L2CacheSpeed=\n",
      "Manufacturer=AuthenticAMD\n",
      "MaxClockSpeed=3801\n",
      "Name=AMD Ryzen 7 5800X 8-Core Processor             \n",
      "ProcessorType=3\n",
      "Revision=8448\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.1\n",
      "[pip3] torch==2.1.2+cu118\n",
      "[pip3] torchaudio==2.1.2+cu118\n",
      "[pip3] torchvision==0.16.2+cu118\n",
      "[conda] numpy                     1.24.1                   pypi_0    pypi\n",
      "[conda] torch                     2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\n",
      "[conda] torchvision               0.16.2+cu118             pypi_0    pypi\n",
      "        Pillow (9.3.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_args(args, cfg)\n",
    "print(\"Collecting env info ...\")\n",
    "print(\"** System info **\\n{}\\n\".format(collect_env_info()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a73d9e7-9574-4910-9638-fec153156f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trainer: CoOp\n",
      "Loading dataset: guided\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  ------\n",
      "Dataset    guided\n",
      "# classes  2\n",
      "# train_x  2,000\n",
      "# val      2,000\n",
      "# test     2,000\n",
      "---------  ------\n",
      "Loading CLIP (backbone: ViT-L/14)\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
      "Number of context words (tokens): 16\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Loading evaluator: Classification\n",
      "Loading weights to prompt_learner from \"../CoOp_Trained/selected_models/100000_16context_best_until_now/prompt_learner\\model.pth.tar-2\" (epoch = 2)\n"
     ]
    }
   ],
   "source": [
    "trainer = build_trainer(cfg)\n",
    "trainer.load_model(args.model_dir, epoch=args.load_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5e927b8-9165-4c02-a5fe-42b307d58baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:27<08:40, 27.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00439453125, 0.0009765625, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.00048828125, 0.00439453125, 0.0048828125, 0.00048828125, 0.009765625, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.455078125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.0146484375, 0.00146484375, 0.00146484375, 0.0, 0.0, 0.0009765625, 0.0078125, 0.0, 0.0107421875, 0.0009765625, 0.60400390625, 0.0, 0.00390625, 0.013671875, 0.0, 0.0, 0.0, 0.310546875, 0.017578125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09716796875, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.03955078125, 0.0, 0.0, 0.01025390625, 0.0, 0.0068359375, 0.00341796875, 0.06201171875, 0.0, 0.01513671875, 0.0, 0.01171875, 0.30419921875, 0.00244140625, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0146484375, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.00048828125]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:27<03:27, 11.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00439453125, 0.0009765625, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.00048828125, 0.00439453125, 0.0048828125, 0.00048828125, 0.009765625, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.455078125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.0146484375, 0.00146484375, 0.00146484375, 0.0, 0.0, 0.0009765625, 0.0078125, 0.0, 0.0107421875, 0.0009765625, 0.60400390625, 0.0, 0.00390625, 0.013671875, 0.0, 0.0, 0.0, 0.310546875, 0.017578125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09716796875, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.03955078125, 0.0, 0.0, 0.01025390625, 0.0, 0.0068359375, 0.00341796875, 0.06201171875, 0.0, 0.01513671875, 0.0, 0.01171875, 0.30419921875, 0.00244140625, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0146484375, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.00048828125, 0.0029296875, 0.0048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.02294921875, 0.0029296875, 0.1982421875, 0.0009765625, 0.06396484375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.06884765625, 0.00048828125, 0.0185546875, 0.0009765625, 0.0, 0.0, 0.001953125, 0.0, 0.00244140625, 0.0, 0.0, 0.55810546875, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.03466796875, 0.001953125, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0078125, 0.9794921875, 0.00439453125, 0.0, 0.00048828125, 0.0341796875, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0283203125, 0.0, 0.00146484375, 0.0009765625, 0.0087890625, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.00634765625, 0.00439453125, 0.0009765625, 0.0, 0.0, 0.00390625, 0.0, 0.0029296875, 0.00048828125, 0.0, 0.02783203125, 0.0, 0.0009765625, 0.0, 0.0, 0.033203125, 0.0, 0.0009765625, 0.0, 0.00732421875, 0.0, 0.0, 0.0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:28<01:50,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00439453125, 0.0009765625, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.00048828125, 0.00439453125, 0.0048828125, 0.00048828125, 0.009765625, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.455078125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.0146484375, 0.00146484375, 0.00146484375, 0.0, 0.0, 0.0009765625, 0.0078125, 0.0, 0.0107421875, 0.0009765625, 0.60400390625, 0.0, 0.00390625, 0.013671875, 0.0, 0.0, 0.0, 0.310546875, 0.017578125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09716796875, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.03955078125, 0.0, 0.0, 0.01025390625, 0.0, 0.0068359375, 0.00341796875, 0.06201171875, 0.0, 0.01513671875, 0.0, 0.01171875, 0.30419921875, 0.00244140625, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0146484375, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.00048828125, 0.0029296875, 0.0048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.02294921875, 0.0029296875, 0.1982421875, 0.0009765625, 0.06396484375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.06884765625, 0.00048828125, 0.0185546875, 0.0009765625, 0.0, 0.0, 0.001953125, 0.0, 0.00244140625, 0.0, 0.0, 0.55810546875, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.03466796875, 0.001953125, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0078125, 0.9794921875, 0.00439453125, 0.0, 0.00048828125, 0.0341796875, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0283203125, 0.0, 0.00146484375, 0.0009765625, 0.0087890625, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.00634765625, 0.00439453125, 0.0009765625, 0.0, 0.0, 0.00390625, 0.0, 0.0029296875, 0.00048828125, 0.0, 0.02783203125, 0.0, 0.0009765625, 0.0, 0.0, 0.033203125, 0.0, 0.0009765625, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.06298828125, 0.08447265625, 0.0, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.0498046875, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0634765625, 0.00244140625, 0.0, 0.0009765625, 0.0029296875, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.01220703125, 0.0, 0.03076171875, 0.01416015625, 0.00732421875, 0.00048828125, 0.01318359375, 0.0, 0.04296875, 0.0, 0.0, 0.0, 0.12109375, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.0009765625, 0.0029296875, 0.041015625, 0.0, 0.0, 0.00341796875, 0.1669921875, 0.00048828125, 0.00146484375, 0.0, 0.00048828125, 0.0, 0.00244140625, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.142578125, 0.0, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.0009765625, 0.06884765625, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.0, 0.01318359375, 0.01513671875, 0.01904296875, 0.0009765625, 0.00048828125, 0.0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:28<01:05,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00439453125, 0.0009765625, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.00048828125, 0.00439453125, 0.0048828125, 0.00048828125, 0.009765625, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.455078125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.0146484375, 0.00146484375, 0.00146484375, 0.0, 0.0, 0.0009765625, 0.0078125, 0.0, 0.0107421875, 0.0009765625, 0.60400390625, 0.0, 0.00390625, 0.013671875, 0.0, 0.0, 0.0, 0.310546875, 0.017578125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09716796875, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.03955078125, 0.0, 0.0, 0.01025390625, 0.0, 0.0068359375, 0.00341796875, 0.06201171875, 0.0, 0.01513671875, 0.0, 0.01171875, 0.30419921875, 0.00244140625, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0146484375, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.00048828125, 0.0029296875, 0.0048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.02294921875, 0.0029296875, 0.1982421875, 0.0009765625, 0.06396484375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.06884765625, 0.00048828125, 0.0185546875, 0.0009765625, 0.0, 0.0, 0.001953125, 0.0, 0.00244140625, 0.0, 0.0, 0.55810546875, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.03466796875, 0.001953125, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0078125, 0.9794921875, 0.00439453125, 0.0, 0.00048828125, 0.0341796875, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0283203125, 0.0, 0.00146484375, 0.0009765625, 0.0087890625, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.00634765625, 0.00439453125, 0.0009765625, 0.0, 0.0, 0.00390625, 0.0, 0.0029296875, 0.00048828125, 0.0, 0.02783203125, 0.0, 0.0009765625, 0.0, 0.0, 0.033203125, 0.0, 0.0009765625, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.06298828125, 0.08447265625, 0.0, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.0498046875, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0634765625, 0.00244140625, 0.0, 0.0009765625, 0.0029296875, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.01220703125, 0.0, 0.03076171875, 0.01416015625, 0.00732421875, 0.00048828125, 0.01318359375, 0.0, 0.04296875, 0.0, 0.0, 0.0, 0.12109375, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.0009765625, 0.0029296875, 0.041015625, 0.0, 0.0, 0.00341796875, 0.1669921875, 0.00048828125, 0.00146484375, 0.0, 0.00048828125, 0.0, 0.00244140625, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.142578125, 0.0, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.0009765625, 0.06884765625, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.0, 0.01318359375, 0.01513671875, 0.01904296875, 0.0009765625, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0009765625, 0.00048828125, 0.00048828125, 0.0009765625, 0.00390625, 0.00244140625, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.02294921875, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.00830078125, 0.00048828125, 0.0, 0.0, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.26416015625, 0.28955078125, 0.0, 0.0, 0.0, 0.0732421875, 0.0009765625, 0.0009765625, 0.0009765625, 0.12939453125, 0.001953125, 0.00341796875, 0.0, 0.00048828125, 0.00048828125, 0.083984375, 0.0, 0.0, 0.03125, 0.0029296875, 0.0, 0.0029296875, 0.00537109375, 0.0, 0.00927734375, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.30224609375, 0.0, 0.0, 0.00927734375, 0.0, 0.00048828125, 0.11767578125, 0.0, 0.0, 0.0029296875, 0.00244140625, 0.00341796875, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0185546875, 0.0, 0.00048828125, 0.0009765625, 0.0, 0.00048828125, 0.06689453125, 0.00048828125, 0.00537109375, 0.0048828125, 0.0009765625, 0.0029296875, 0.00048828125, 0.00244140625, 0.0, 0.0634765625, 0.0, 0.00048828125]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:29<00:42,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00439453125, 0.0009765625, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.00048828125, 0.00439453125, 0.0048828125, 0.00048828125, 0.009765625, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.455078125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.0146484375, 0.00146484375, 0.00146484375, 0.0, 0.0, 0.0009765625, 0.0078125, 0.0, 0.0107421875, 0.0009765625, 0.60400390625, 0.0, 0.00390625, 0.013671875, 0.0, 0.0, 0.0, 0.310546875, 0.017578125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09716796875, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.03955078125, 0.0, 0.0, 0.01025390625, 0.0, 0.0068359375, 0.00341796875, 0.06201171875, 0.0, 0.01513671875, 0.0, 0.01171875, 0.30419921875, 0.00244140625, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0146484375, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.00048828125, 0.0029296875, 0.0048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.02294921875, 0.0029296875, 0.1982421875, 0.0009765625, 0.06396484375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.06884765625, 0.00048828125, 0.0185546875, 0.0009765625, 0.0, 0.0, 0.001953125, 0.0, 0.00244140625, 0.0, 0.0, 0.55810546875, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.03466796875, 0.001953125, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0078125, 0.9794921875, 0.00439453125, 0.0, 0.00048828125, 0.0341796875, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0283203125, 0.0, 0.00146484375, 0.0009765625, 0.0087890625, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.00634765625, 0.00439453125, 0.0009765625, 0.0, 0.0, 0.00390625, 0.0, 0.0029296875, 0.00048828125, 0.0, 0.02783203125, 0.0, 0.0009765625, 0.0, 0.0, 0.033203125, 0.0, 0.0009765625, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.06298828125, 0.08447265625, 0.0, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.0498046875, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0634765625, 0.00244140625, 0.0, 0.0009765625, 0.0029296875, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.01220703125, 0.0, 0.03076171875, 0.01416015625, 0.00732421875, 0.00048828125, 0.01318359375, 0.0, 0.04296875, 0.0, 0.0, 0.0, 0.12109375, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.0009765625, 0.0029296875, 0.041015625, 0.0, 0.0, 0.00341796875, 0.1669921875, 0.00048828125, 0.00146484375, 0.0, 0.00048828125, 0.0, 0.00244140625, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.142578125, 0.0, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.0009765625, 0.06884765625, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.0, 0.01318359375, 0.01513671875, 0.01904296875, 0.0009765625, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0009765625, 0.00048828125, 0.00048828125, 0.0009765625, 0.00390625, 0.00244140625, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.02294921875, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.00830078125, 0.00048828125, 0.0, 0.0, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.26416015625, 0.28955078125, 0.0, 0.0, 0.0, 0.0732421875, 0.0009765625, 0.0009765625, 0.0009765625, 0.12939453125, 0.001953125, 0.00341796875, 0.0, 0.00048828125, 0.00048828125, 0.083984375, 0.0, 0.0, 0.03125, 0.0029296875, 0.0, 0.0029296875, 0.00537109375, 0.0, 0.00927734375, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.30224609375, 0.0, 0.0, 0.00927734375, 0.0, 0.00048828125, 0.11767578125, 0.0, 0.0, 0.0029296875, 0.00244140625, 0.00341796875, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0185546875, 0.0, 0.00048828125, 0.0009765625, 0.0, 0.00048828125, 0.06689453125, 0.00048828125, 0.00537109375, 0.0048828125, 0.0009765625, 0.0029296875, 0.00048828125, 0.00244140625, 0.0, 0.0634765625, 0.0, 0.00048828125, 0.005859375, 0.0, 0.00732421875, 0.0078125, 0.00341796875, 0.0, 0.0, 0.0087890625, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.0, 0.013671875, 0.02392578125, 0.0, 0.00537109375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.00830078125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.0, 0.00341796875, 0.00244140625, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.01806640625, 0.0, 0.0, 0.0029296875, 0.0048828125, 0.01220703125, 0.0, 0.00244140625, 0.0, 0.05712890625, 0.0, 0.0087890625, 0.00048828125, 0.0263671875, 0.03369140625, 0.1337890625, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0009765625, 0.0205078125, 0.0, 0.0087890625, 0.02587890625, 0.00146484375, 0.1943359375, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0087890625, 0.0, 0.0, 0.33447265625, 0.0078125, 0.0, 0.25927734375, 0.0, 0.0, 0.0, 0.00048828125, 0.0048828125, 0.0458984375, 0.0068359375, 0.0, 0.00537109375, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0322265625, 0.0, 0.0, 0.0, 0.357421875]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:29<00:28,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00439453125, 0.0009765625, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.00048828125, 0.00439453125, 0.0048828125, 0.00048828125, 0.009765625, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.455078125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.0146484375, 0.00146484375, 0.00146484375, 0.0, 0.0, 0.0009765625, 0.0078125, 0.0, 0.0107421875, 0.0009765625, 0.60400390625, 0.0, 0.00390625, 0.013671875, 0.0, 0.0, 0.0, 0.310546875, 0.017578125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09716796875, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.03955078125, 0.0, 0.0, 0.01025390625, 0.0, 0.0068359375, 0.00341796875, 0.06201171875, 0.0, 0.01513671875, 0.0, 0.01171875, 0.30419921875, 0.00244140625, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0146484375, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.00048828125, 0.0029296875, 0.0048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.02294921875, 0.0029296875, 0.1982421875, 0.0009765625, 0.06396484375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.06884765625, 0.00048828125, 0.0185546875, 0.0009765625, 0.0, 0.0, 0.001953125, 0.0, 0.00244140625, 0.0, 0.0, 0.55810546875, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.03466796875, 0.001953125, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0078125, 0.9794921875, 0.00439453125, 0.0, 0.00048828125, 0.0341796875, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0283203125, 0.0, 0.00146484375, 0.0009765625, 0.0087890625, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.00634765625, 0.00439453125, 0.0009765625, 0.0, 0.0, 0.00390625, 0.0, 0.0029296875, 0.00048828125, 0.0, 0.02783203125, 0.0, 0.0009765625, 0.0, 0.0, 0.033203125, 0.0, 0.0009765625, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.06298828125, 0.08447265625, 0.0, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.0498046875, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0634765625, 0.00244140625, 0.0, 0.0009765625, 0.0029296875, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.01220703125, 0.0, 0.03076171875, 0.01416015625, 0.00732421875, 0.00048828125, 0.01318359375, 0.0, 0.04296875, 0.0, 0.0, 0.0, 0.12109375, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.0009765625, 0.0029296875, 0.041015625, 0.0, 0.0, 0.00341796875, 0.1669921875, 0.00048828125, 0.00146484375, 0.0, 0.00048828125, 0.0, 0.00244140625, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.142578125, 0.0, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.0009765625, 0.06884765625, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.0, 0.01318359375, 0.01513671875, 0.01904296875, 0.0009765625, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0009765625, 0.00048828125, 0.00048828125, 0.0009765625, 0.00390625, 0.00244140625, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.02294921875, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.00830078125, 0.00048828125, 0.0, 0.0, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.26416015625, 0.28955078125, 0.0, 0.0, 0.0, 0.0732421875, 0.0009765625, 0.0009765625, 0.0009765625, 0.12939453125, 0.001953125, 0.00341796875, 0.0, 0.00048828125, 0.00048828125, 0.083984375, 0.0, 0.0, 0.03125, 0.0029296875, 0.0, 0.0029296875, 0.00537109375, 0.0, 0.00927734375, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.30224609375, 0.0, 0.0, 0.00927734375, 0.0, 0.00048828125, 0.11767578125, 0.0, 0.0, 0.0029296875, 0.00244140625, 0.00341796875, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0185546875, 0.0, 0.00048828125, 0.0009765625, 0.0, 0.00048828125, 0.06689453125, 0.00048828125, 0.00537109375, 0.0048828125, 0.0009765625, 0.0029296875, 0.00048828125, 0.00244140625, 0.0, 0.0634765625, 0.0, 0.00048828125, 0.005859375, 0.0, 0.00732421875, 0.0078125, 0.00341796875, 0.0, 0.0, 0.0087890625, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.0, 0.013671875, 0.02392578125, 0.0, 0.00537109375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.00830078125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.0, 0.00341796875, 0.00244140625, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.01806640625, 0.0, 0.0, 0.0029296875, 0.0048828125, 0.01220703125, 0.0, 0.00244140625, 0.0, 0.05712890625, 0.0, 0.0087890625, 0.00048828125, 0.0263671875, 0.03369140625, 0.1337890625, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0009765625, 0.0205078125, 0.0, 0.0087890625, 0.02587890625, 0.00146484375, 0.1943359375, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0087890625, 0.0, 0.0, 0.33447265625, 0.0078125, 0.0, 0.25927734375, 0.0, 0.0, 0.0, 0.00048828125, 0.0048828125, 0.0458984375, 0.0068359375, 0.0, 0.00537109375, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0322265625, 0.0, 0.0, 0.0, 0.357421875, 0.0, 0.01318359375, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.04541015625, 0.0, 0.0, 0.00146484375, 0.00244140625, 0.00927734375, 0.0009765625, 0.00244140625, 0.0, 0.037109375, 0.09619140625, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.00830078125, 0.00341796875, 0.0, 0.0, 0.03955078125, 0.00439453125, 0.0, 0.00244140625, 0.00439453125, 0.03271484375, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00341796875, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.001953125, 0.0, 0.08349609375, 0.0, 0.0, 0.0, 0.01318359375, 0.07763671875, 0.001953125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.06640625, 0.0, 0.09423828125, 0.0126953125, 0.00048828125, 0.00048828125, 0.0244140625, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0029296875, 0.00146484375, 0.00048828125, 0.0, 0.0, 0.0, 0.0009765625, 0.001953125, 0.01806640625, 0.00390625, 0.01123046875, 0.49609375, 0.0, 0.0, 0.0, 0.05029296875, 0.00439453125, 0.0, 0.0, 0.00390625, 0.0, 0.00048828125]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:30<00:19,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00439453125, 0.0009765625, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.00048828125, 0.00439453125, 0.0048828125, 0.00048828125, 0.009765625, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.455078125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.0146484375, 0.00146484375, 0.00146484375, 0.0, 0.0, 0.0009765625, 0.0078125, 0.0, 0.0107421875, 0.0009765625, 0.60400390625, 0.0, 0.00390625, 0.013671875, 0.0, 0.0, 0.0, 0.310546875, 0.017578125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09716796875, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.03955078125, 0.0, 0.0, 0.01025390625, 0.0, 0.0068359375, 0.00341796875, 0.06201171875, 0.0, 0.01513671875, 0.0, 0.01171875, 0.30419921875, 0.00244140625, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0146484375, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.00048828125, 0.0029296875, 0.0048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.02294921875, 0.0029296875, 0.1982421875, 0.0009765625, 0.06396484375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.06884765625, 0.00048828125, 0.0185546875, 0.0009765625, 0.0, 0.0, 0.001953125, 0.0, 0.00244140625, 0.0, 0.0, 0.55810546875, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.03466796875, 0.001953125, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0078125, 0.9794921875, 0.00439453125, 0.0, 0.00048828125, 0.0341796875, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0283203125, 0.0, 0.00146484375, 0.0009765625, 0.0087890625, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.00634765625, 0.00439453125, 0.0009765625, 0.0, 0.0, 0.00390625, 0.0, 0.0029296875, 0.00048828125, 0.0, 0.02783203125, 0.0, 0.0009765625, 0.0, 0.0, 0.033203125, 0.0, 0.0009765625, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.06298828125, 0.08447265625, 0.0, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.0498046875, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0634765625, 0.00244140625, 0.0, 0.0009765625, 0.0029296875, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.01220703125, 0.0, 0.03076171875, 0.01416015625, 0.00732421875, 0.00048828125, 0.01318359375, 0.0, 0.04296875, 0.0, 0.0, 0.0, 0.12109375, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.0009765625, 0.0029296875, 0.041015625, 0.0, 0.0, 0.00341796875, 0.1669921875, 0.00048828125, 0.00146484375, 0.0, 0.00048828125, 0.0, 0.00244140625, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.142578125, 0.0, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.0009765625, 0.06884765625, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.0, 0.01318359375, 0.01513671875, 0.01904296875, 0.0009765625, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0009765625, 0.00048828125, 0.00048828125, 0.0009765625, 0.00390625, 0.00244140625, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.02294921875, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.00830078125, 0.00048828125, 0.0, 0.0, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.26416015625, 0.28955078125, 0.0, 0.0, 0.0, 0.0732421875, 0.0009765625, 0.0009765625, 0.0009765625, 0.12939453125, 0.001953125, 0.00341796875, 0.0, 0.00048828125, 0.00048828125, 0.083984375, 0.0, 0.0, 0.03125, 0.0029296875, 0.0, 0.0029296875, 0.00537109375, 0.0, 0.00927734375, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.30224609375, 0.0, 0.0, 0.00927734375, 0.0, 0.00048828125, 0.11767578125, 0.0, 0.0, 0.0029296875, 0.00244140625, 0.00341796875, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0185546875, 0.0, 0.00048828125, 0.0009765625, 0.0, 0.00048828125, 0.06689453125, 0.00048828125, 0.00537109375, 0.0048828125, 0.0009765625, 0.0029296875, 0.00048828125, 0.00244140625, 0.0, 0.0634765625, 0.0, 0.00048828125, 0.005859375, 0.0, 0.00732421875, 0.0078125, 0.00341796875, 0.0, 0.0, 0.0087890625, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.0, 0.013671875, 0.02392578125, 0.0, 0.00537109375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.00830078125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.0, 0.00341796875, 0.00244140625, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.01806640625, 0.0, 0.0, 0.0029296875, 0.0048828125, 0.01220703125, 0.0, 0.00244140625, 0.0, 0.05712890625, 0.0, 0.0087890625, 0.00048828125, 0.0263671875, 0.03369140625, 0.1337890625, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0009765625, 0.0205078125, 0.0, 0.0087890625, 0.02587890625, 0.00146484375, 0.1943359375, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0087890625, 0.0, 0.0, 0.33447265625, 0.0078125, 0.0, 0.25927734375, 0.0, 0.0, 0.0, 0.00048828125, 0.0048828125, 0.0458984375, 0.0068359375, 0.0, 0.00537109375, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0322265625, 0.0, 0.0, 0.0, 0.357421875, 0.0, 0.01318359375, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.04541015625, 0.0, 0.0, 0.00146484375, 0.00244140625, 0.00927734375, 0.0009765625, 0.00244140625, 0.0, 0.037109375, 0.09619140625, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.00830078125, 0.00341796875, 0.0, 0.0, 0.03955078125, 0.00439453125, 0.0, 0.00244140625, 0.00439453125, 0.03271484375, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00341796875, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.001953125, 0.0, 0.08349609375, 0.0, 0.0, 0.0, 0.01318359375, 0.07763671875, 0.001953125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.06640625, 0.0, 0.09423828125, 0.0126953125, 0.00048828125, 0.00048828125, 0.0244140625, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0029296875, 0.00146484375, 0.00048828125, 0.0, 0.0, 0.0, 0.0009765625, 0.001953125, 0.01806640625, 0.00390625, 0.01123046875, 0.49609375, 0.0, 0.0, 0.0, 0.05029296875, 0.00439453125, 0.0, 0.0, 0.00390625, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.0, 0.0068359375, 0.00048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.099609375, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.00048828125, 0.0009765625, 0.03173828125, 0.0, 0.0, 0.08203125, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00244140625, 0.00048828125, 0.0, 0.001953125, 0.1767578125, 0.00048828125, 0.0, 0.0, 0.009765625, 0.00146484375, 0.00244140625, 0.0, 0.00341796875, 0.0, 0.064453125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.00048828125, 0.001953125, 0.00244140625, 0.3720703125, 0.13232421875, 0.0, 0.55810546875, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00927734375, 0.00048828125, 0.00048828125, 0.001953125, 0.0, 0.0205078125, 0.0, 0.33642578125, 0.0, 0.005859375, 0.4814453125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.12939453125, 0.00146484375, 0.0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:30<00:14,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00439453125, 0.0009765625, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.00048828125, 0.00439453125, 0.0048828125, 0.00048828125, 0.009765625, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.455078125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.0146484375, 0.00146484375, 0.00146484375, 0.0, 0.0, 0.0009765625, 0.0078125, 0.0, 0.0107421875, 0.0009765625, 0.60400390625, 0.0, 0.00390625, 0.013671875, 0.0, 0.0, 0.0, 0.310546875, 0.017578125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09716796875, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.03955078125, 0.0, 0.0, 0.01025390625, 0.0, 0.0068359375, 0.00341796875, 0.06201171875, 0.0, 0.01513671875, 0.0, 0.01171875, 0.30419921875, 0.00244140625, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0146484375, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.00048828125, 0.0029296875, 0.0048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.02294921875, 0.0029296875, 0.1982421875, 0.0009765625, 0.06396484375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.06884765625, 0.00048828125, 0.0185546875, 0.0009765625, 0.0, 0.0, 0.001953125, 0.0, 0.00244140625, 0.0, 0.0, 0.55810546875, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.03466796875, 0.001953125, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0078125, 0.9794921875, 0.00439453125, 0.0, 0.00048828125, 0.0341796875, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0283203125, 0.0, 0.00146484375, 0.0009765625, 0.0087890625, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.00634765625, 0.00439453125, 0.0009765625, 0.0, 0.0, 0.00390625, 0.0, 0.0029296875, 0.00048828125, 0.0, 0.02783203125, 0.0, 0.0009765625, 0.0, 0.0, 0.033203125, 0.0, 0.0009765625, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.06298828125, 0.08447265625, 0.0, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.0498046875, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0634765625, 0.00244140625, 0.0, 0.0009765625, 0.0029296875, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.01220703125, 0.0, 0.03076171875, 0.01416015625, 0.00732421875, 0.00048828125, 0.01318359375, 0.0, 0.04296875, 0.0, 0.0, 0.0, 0.12109375, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.0009765625, 0.0029296875, 0.041015625, 0.0, 0.0, 0.00341796875, 0.1669921875, 0.00048828125, 0.00146484375, 0.0, 0.00048828125, 0.0, 0.00244140625, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.142578125, 0.0, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.0009765625, 0.06884765625, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.0, 0.01318359375, 0.01513671875, 0.01904296875, 0.0009765625, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0009765625, 0.00048828125, 0.00048828125, 0.0009765625, 0.00390625, 0.00244140625, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.02294921875, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.00830078125, 0.00048828125, 0.0, 0.0, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.26416015625, 0.28955078125, 0.0, 0.0, 0.0, 0.0732421875, 0.0009765625, 0.0009765625, 0.0009765625, 0.12939453125, 0.001953125, 0.00341796875, 0.0, 0.00048828125, 0.00048828125, 0.083984375, 0.0, 0.0, 0.03125, 0.0029296875, 0.0, 0.0029296875, 0.00537109375, 0.0, 0.00927734375, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.30224609375, 0.0, 0.0, 0.00927734375, 0.0, 0.00048828125, 0.11767578125, 0.0, 0.0, 0.0029296875, 0.00244140625, 0.00341796875, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0185546875, 0.0, 0.00048828125, 0.0009765625, 0.0, 0.00048828125, 0.06689453125, 0.00048828125, 0.00537109375, 0.0048828125, 0.0009765625, 0.0029296875, 0.00048828125, 0.00244140625, 0.0, 0.0634765625, 0.0, 0.00048828125, 0.005859375, 0.0, 0.00732421875, 0.0078125, 0.00341796875, 0.0, 0.0, 0.0087890625, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.0, 0.013671875, 0.02392578125, 0.0, 0.00537109375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.00830078125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.0, 0.00341796875, 0.00244140625, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.01806640625, 0.0, 0.0, 0.0029296875, 0.0048828125, 0.01220703125, 0.0, 0.00244140625, 0.0, 0.05712890625, 0.0, 0.0087890625, 0.00048828125, 0.0263671875, 0.03369140625, 0.1337890625, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0009765625, 0.0205078125, 0.0, 0.0087890625, 0.02587890625, 0.00146484375, 0.1943359375, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0087890625, 0.0, 0.0, 0.33447265625, 0.0078125, 0.0, 0.25927734375, 0.0, 0.0, 0.0, 0.00048828125, 0.0048828125, 0.0458984375, 0.0068359375, 0.0, 0.00537109375, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0322265625, 0.0, 0.0, 0.0, 0.357421875, 0.0, 0.01318359375, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.04541015625, 0.0, 0.0, 0.00146484375, 0.00244140625, 0.00927734375, 0.0009765625, 0.00244140625, 0.0, 0.037109375, 0.09619140625, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.00830078125, 0.00341796875, 0.0, 0.0, 0.03955078125, 0.00439453125, 0.0, 0.00244140625, 0.00439453125, 0.03271484375, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00341796875, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.001953125, 0.0, 0.08349609375, 0.0, 0.0, 0.0, 0.01318359375, 0.07763671875, 0.001953125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.06640625, 0.0, 0.09423828125, 0.0126953125, 0.00048828125, 0.00048828125, 0.0244140625, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0029296875, 0.00146484375, 0.00048828125, 0.0, 0.0, 0.0, 0.0009765625, 0.001953125, 0.01806640625, 0.00390625, 0.01123046875, 0.49609375, 0.0, 0.0, 0.0, 0.05029296875, 0.00439453125, 0.0, 0.0, 0.00390625, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.0, 0.0068359375, 0.00048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.099609375, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.00048828125, 0.0009765625, 0.03173828125, 0.0, 0.0, 0.08203125, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00244140625, 0.00048828125, 0.0, 0.001953125, 0.1767578125, 0.00048828125, 0.0, 0.0, 0.009765625, 0.00146484375, 0.00244140625, 0.0, 0.00341796875, 0.0, 0.064453125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.00048828125, 0.001953125, 0.00244140625, 0.3720703125, 0.13232421875, 0.0, 0.55810546875, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00927734375, 0.00048828125, 0.00048828125, 0.001953125, 0.0, 0.0205078125, 0.0, 0.33642578125, 0.0, 0.005859375, 0.4814453125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.12939453125, 0.00146484375, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0009765625, 0.00048828125, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00537109375, 0.00048828125, 0.00244140625, 0.0, 0.00048828125, 0.16015625, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.04296875, 0.00048828125, 0.0, 0.4453125, 0.0, 0.0, 0.0, 0.0, 0.00830078125, 0.0283203125, 0.01318359375, 0.009765625, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.0, 0.01171875, 0.0, 0.0029296875, 0.01513671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.025390625, 0.00146484375, 0.00048828125, 0.00048828125, 0.0, 0.00439453125, 0.0, 0.00146484375, 0.0078125, 0.0029296875, 0.00244140625, 0.00244140625, 0.04296875, 0.533203125, 0.46875, 0.0, 0.01171875, 0.0, 0.0, 0.001953125, 0.0029296875, 0.0, 0.00146484375, 0.0, 0.00146484375, 0.00146484375, 0.0, 0.00048828125, 0.00146484375, 0.00341796875, 0.0, 0.00048828125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:31<00:10,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00439453125, 0.0009765625, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.00048828125, 0.00439453125, 0.0048828125, 0.00048828125, 0.009765625, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.455078125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.0146484375, 0.00146484375, 0.00146484375, 0.0, 0.0, 0.0009765625, 0.0078125, 0.0, 0.0107421875, 0.0009765625, 0.60400390625, 0.0, 0.00390625, 0.013671875, 0.0, 0.0, 0.0, 0.310546875, 0.017578125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09716796875, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.03955078125, 0.0, 0.0, 0.01025390625, 0.0, 0.0068359375, 0.00341796875, 0.06201171875, 0.0, 0.01513671875, 0.0, 0.01171875, 0.30419921875, 0.00244140625, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0146484375, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.00048828125, 0.0029296875, 0.0048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.02294921875, 0.0029296875, 0.1982421875, 0.0009765625, 0.06396484375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.06884765625, 0.00048828125, 0.0185546875, 0.0009765625, 0.0, 0.0, 0.001953125, 0.0, 0.00244140625, 0.0, 0.0, 0.55810546875, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.03466796875, 0.001953125, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0078125, 0.9794921875, 0.00439453125, 0.0, 0.00048828125, 0.0341796875, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0283203125, 0.0, 0.00146484375, 0.0009765625, 0.0087890625, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.00634765625, 0.00439453125, 0.0009765625, 0.0, 0.0, 0.00390625, 0.0, 0.0029296875, 0.00048828125, 0.0, 0.02783203125, 0.0, 0.0009765625, 0.0, 0.0, 0.033203125, 0.0, 0.0009765625, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.06298828125, 0.08447265625, 0.0, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.0498046875, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0634765625, 0.00244140625, 0.0, 0.0009765625, 0.0029296875, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.01220703125, 0.0, 0.03076171875, 0.01416015625, 0.00732421875, 0.00048828125, 0.01318359375, 0.0, 0.04296875, 0.0, 0.0, 0.0, 0.12109375, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.0009765625, 0.0029296875, 0.041015625, 0.0, 0.0, 0.00341796875, 0.1669921875, 0.00048828125, 0.00146484375, 0.0, 0.00048828125, 0.0, 0.00244140625, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.142578125, 0.0, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.0009765625, 0.06884765625, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.0, 0.01318359375, 0.01513671875, 0.01904296875, 0.0009765625, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0009765625, 0.00048828125, 0.00048828125, 0.0009765625, 0.00390625, 0.00244140625, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.02294921875, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.00830078125, 0.00048828125, 0.0, 0.0, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.26416015625, 0.28955078125, 0.0, 0.0, 0.0, 0.0732421875, 0.0009765625, 0.0009765625, 0.0009765625, 0.12939453125, 0.001953125, 0.00341796875, 0.0, 0.00048828125, 0.00048828125, 0.083984375, 0.0, 0.0, 0.03125, 0.0029296875, 0.0, 0.0029296875, 0.00537109375, 0.0, 0.00927734375, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.30224609375, 0.0, 0.0, 0.00927734375, 0.0, 0.00048828125, 0.11767578125, 0.0, 0.0, 0.0029296875, 0.00244140625, 0.00341796875, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0185546875, 0.0, 0.00048828125, 0.0009765625, 0.0, 0.00048828125, 0.06689453125, 0.00048828125, 0.00537109375, 0.0048828125, 0.0009765625, 0.0029296875, 0.00048828125, 0.00244140625, 0.0, 0.0634765625, 0.0, 0.00048828125, 0.005859375, 0.0, 0.00732421875, 0.0078125, 0.00341796875, 0.0, 0.0, 0.0087890625, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.0, 0.013671875, 0.02392578125, 0.0, 0.00537109375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.00830078125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.0, 0.00341796875, 0.00244140625, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.01806640625, 0.0, 0.0, 0.0029296875, 0.0048828125, 0.01220703125, 0.0, 0.00244140625, 0.0, 0.05712890625, 0.0, 0.0087890625, 0.00048828125, 0.0263671875, 0.03369140625, 0.1337890625, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0009765625, 0.0205078125, 0.0, 0.0087890625, 0.02587890625, 0.00146484375, 0.1943359375, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0087890625, 0.0, 0.0, 0.33447265625, 0.0078125, 0.0, 0.25927734375, 0.0, 0.0, 0.0, 0.00048828125, 0.0048828125, 0.0458984375, 0.0068359375, 0.0, 0.00537109375, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0322265625, 0.0, 0.0, 0.0, 0.357421875, 0.0, 0.01318359375, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.04541015625, 0.0, 0.0, 0.00146484375, 0.00244140625, 0.00927734375, 0.0009765625, 0.00244140625, 0.0, 0.037109375, 0.09619140625, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.00830078125, 0.00341796875, 0.0, 0.0, 0.03955078125, 0.00439453125, 0.0, 0.00244140625, 0.00439453125, 0.03271484375, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00341796875, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.001953125, 0.0, 0.08349609375, 0.0, 0.0, 0.0, 0.01318359375, 0.07763671875, 0.001953125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.06640625, 0.0, 0.09423828125, 0.0126953125, 0.00048828125, 0.00048828125, 0.0244140625, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0029296875, 0.00146484375, 0.00048828125, 0.0, 0.0, 0.0, 0.0009765625, 0.001953125, 0.01806640625, 0.00390625, 0.01123046875, 0.49609375, 0.0, 0.0, 0.0, 0.05029296875, 0.00439453125, 0.0, 0.0, 0.00390625, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.0, 0.0068359375, 0.00048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.099609375, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.00048828125, 0.0009765625, 0.03173828125, 0.0, 0.0, 0.08203125, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00244140625, 0.00048828125, 0.0, 0.001953125, 0.1767578125, 0.00048828125, 0.0, 0.0, 0.009765625, 0.00146484375, 0.00244140625, 0.0, 0.00341796875, 0.0, 0.064453125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.00048828125, 0.001953125, 0.00244140625, 0.3720703125, 0.13232421875, 0.0, 0.55810546875, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00927734375, 0.00048828125, 0.00048828125, 0.001953125, 0.0, 0.0205078125, 0.0, 0.33642578125, 0.0, 0.005859375, 0.4814453125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.12939453125, 0.00146484375, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0009765625, 0.00048828125, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00537109375, 0.00048828125, 0.00244140625, 0.0, 0.00048828125, 0.16015625, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.04296875, 0.00048828125, 0.0, 0.4453125, 0.0, 0.0, 0.0, 0.0, 0.00830078125, 0.0283203125, 0.01318359375, 0.009765625, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.0, 0.01171875, 0.0, 0.0029296875, 0.01513671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.025390625, 0.00146484375, 0.00048828125, 0.00048828125, 0.0, 0.00439453125, 0.0, 0.00146484375, 0.0078125, 0.0029296875, 0.00244140625, 0.00244140625, 0.04296875, 0.533203125, 0.46875, 0.0, 0.01171875, 0.0, 0.0, 0.001953125, 0.0029296875, 0.0, 0.00146484375, 0.0, 0.00146484375, 0.00146484375, 0.0, 0.00048828125, 0.00146484375, 0.00341796875, 0.0, 0.00048828125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.0, 0.0, 0.0390625, 0.0, 0.00634765625, 0.0, 0.0, 0.00244140625, 0.0, 0.0, 0.03466796875, 0.0, 0.00537109375, 0.0, 0.0029296875, 0.2392578125, 0.0, 0.0546875, 0.0, 0.0, 0.0, 0.36279296875, 0.0, 0.00048828125, 0.0029296875, 0.00244140625, 0.005859375, 0.0, 0.0, 0.00146484375, 0.00390625, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00048828125, 0.0, 0.01513671875, 0.001953125, 0.00048828125, 0.00048828125, 0.00341796875, 0.05712890625, 0.0, 0.0, 0.0, 0.06640625, 0.0, 0.00537109375, 0.0, 0.0, 0.0400390625, 0.095703125, 0.00048828125, 0.0, 0.0, 0.0, 0.06494140625, 0.0, 0.0009765625, 0.01611328125, 0.0009765625, 0.01318359375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00146484375, 0.0, 0.001953125, 0.00146484375, 0.00146484375, 0.0029296875, 0.01318359375, 0.00341796875, 0.0, 0.0, 0.0, 0.0009765625, 0.00146484375, 0.0185546875, 0.00146484375, 0.0029296875, 0.0, 0.0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:31<00:08,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00439453125, 0.0009765625, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.00048828125, 0.00439453125, 0.0048828125, 0.00048828125, 0.009765625, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.455078125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.0146484375, 0.00146484375, 0.00146484375, 0.0, 0.0, 0.0009765625, 0.0078125, 0.0, 0.0107421875, 0.0009765625, 0.60400390625, 0.0, 0.00390625, 0.013671875, 0.0, 0.0, 0.0, 0.310546875, 0.017578125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09716796875, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.03955078125, 0.0, 0.0, 0.01025390625, 0.0, 0.0068359375, 0.00341796875, 0.06201171875, 0.0, 0.01513671875, 0.0, 0.01171875, 0.30419921875, 0.00244140625, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0146484375, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.00048828125, 0.0029296875, 0.0048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.02294921875, 0.0029296875, 0.1982421875, 0.0009765625, 0.06396484375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.06884765625, 0.00048828125, 0.0185546875, 0.0009765625, 0.0, 0.0, 0.001953125, 0.0, 0.00244140625, 0.0, 0.0, 0.55810546875, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.03466796875, 0.001953125, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0078125, 0.9794921875, 0.00439453125, 0.0, 0.00048828125, 0.0341796875, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0283203125, 0.0, 0.00146484375, 0.0009765625, 0.0087890625, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.00634765625, 0.00439453125, 0.0009765625, 0.0, 0.0, 0.00390625, 0.0, 0.0029296875, 0.00048828125, 0.0, 0.02783203125, 0.0, 0.0009765625, 0.0, 0.0, 0.033203125, 0.0, 0.0009765625, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.06298828125, 0.08447265625, 0.0, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.0498046875, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0634765625, 0.00244140625, 0.0, 0.0009765625, 0.0029296875, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.01220703125, 0.0, 0.03076171875, 0.01416015625, 0.00732421875, 0.00048828125, 0.01318359375, 0.0, 0.04296875, 0.0, 0.0, 0.0, 0.12109375, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.0009765625, 0.0029296875, 0.041015625, 0.0, 0.0, 0.00341796875, 0.1669921875, 0.00048828125, 0.00146484375, 0.0, 0.00048828125, 0.0, 0.00244140625, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.142578125, 0.0, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.0009765625, 0.06884765625, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.0, 0.01318359375, 0.01513671875, 0.01904296875, 0.0009765625, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0009765625, 0.00048828125, 0.00048828125, 0.0009765625, 0.00390625, 0.00244140625, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.02294921875, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.00830078125, 0.00048828125, 0.0, 0.0, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.26416015625, 0.28955078125, 0.0, 0.0, 0.0, 0.0732421875, 0.0009765625, 0.0009765625, 0.0009765625, 0.12939453125, 0.001953125, 0.00341796875, 0.0, 0.00048828125, 0.00048828125, 0.083984375, 0.0, 0.0, 0.03125, 0.0029296875, 0.0, 0.0029296875, 0.00537109375, 0.0, 0.00927734375, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.30224609375, 0.0, 0.0, 0.00927734375, 0.0, 0.00048828125, 0.11767578125, 0.0, 0.0, 0.0029296875, 0.00244140625, 0.00341796875, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0185546875, 0.0, 0.00048828125, 0.0009765625, 0.0, 0.00048828125, 0.06689453125, 0.00048828125, 0.00537109375, 0.0048828125, 0.0009765625, 0.0029296875, 0.00048828125, 0.00244140625, 0.0, 0.0634765625, 0.0, 0.00048828125, 0.005859375, 0.0, 0.00732421875, 0.0078125, 0.00341796875, 0.0, 0.0, 0.0087890625, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.0, 0.013671875, 0.02392578125, 0.0, 0.00537109375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.00830078125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.0, 0.00341796875, 0.00244140625, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.01806640625, 0.0, 0.0, 0.0029296875, 0.0048828125, 0.01220703125, 0.0, 0.00244140625, 0.0, 0.05712890625, 0.0, 0.0087890625, 0.00048828125, 0.0263671875, 0.03369140625, 0.1337890625, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0009765625, 0.0205078125, 0.0, 0.0087890625, 0.02587890625, 0.00146484375, 0.1943359375, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0087890625, 0.0, 0.0, 0.33447265625, 0.0078125, 0.0, 0.25927734375, 0.0, 0.0, 0.0, 0.00048828125, 0.0048828125, 0.0458984375, 0.0068359375, 0.0, 0.00537109375, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0322265625, 0.0, 0.0, 0.0, 0.357421875, 0.0, 0.01318359375, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.04541015625, 0.0, 0.0, 0.00146484375, 0.00244140625, 0.00927734375, 0.0009765625, 0.00244140625, 0.0, 0.037109375, 0.09619140625, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.00830078125, 0.00341796875, 0.0, 0.0, 0.03955078125, 0.00439453125, 0.0, 0.00244140625, 0.00439453125, 0.03271484375, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00341796875, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.001953125, 0.0, 0.08349609375, 0.0, 0.0, 0.0, 0.01318359375, 0.07763671875, 0.001953125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.06640625, 0.0, 0.09423828125, 0.0126953125, 0.00048828125, 0.00048828125, 0.0244140625, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0029296875, 0.00146484375, 0.00048828125, 0.0, 0.0, 0.0, 0.0009765625, 0.001953125, 0.01806640625, 0.00390625, 0.01123046875, 0.49609375, 0.0, 0.0, 0.0, 0.05029296875, 0.00439453125, 0.0, 0.0, 0.00390625, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.0, 0.0068359375, 0.00048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.099609375, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.00048828125, 0.0009765625, 0.03173828125, 0.0, 0.0, 0.08203125, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00244140625, 0.00048828125, 0.0, 0.001953125, 0.1767578125, 0.00048828125, 0.0, 0.0, 0.009765625, 0.00146484375, 0.00244140625, 0.0, 0.00341796875, 0.0, 0.064453125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.00048828125, 0.001953125, 0.00244140625, 0.3720703125, 0.13232421875, 0.0, 0.55810546875, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00927734375, 0.00048828125, 0.00048828125, 0.001953125, 0.0, 0.0205078125, 0.0, 0.33642578125, 0.0, 0.005859375, 0.4814453125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.12939453125, 0.00146484375, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0009765625, 0.00048828125, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00537109375, 0.00048828125, 0.00244140625, 0.0, 0.00048828125, 0.16015625, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.04296875, 0.00048828125, 0.0, 0.4453125, 0.0, 0.0, 0.0, 0.0, 0.00830078125, 0.0283203125, 0.01318359375, 0.009765625, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.0, 0.01171875, 0.0, 0.0029296875, 0.01513671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.025390625, 0.00146484375, 0.00048828125, 0.00048828125, 0.0, 0.00439453125, 0.0, 0.00146484375, 0.0078125, 0.0029296875, 0.00244140625, 0.00244140625, 0.04296875, 0.533203125, 0.46875, 0.0, 0.01171875, 0.0, 0.0, 0.001953125, 0.0029296875, 0.0, 0.00146484375, 0.0, 0.00146484375, 0.00146484375, 0.0, 0.00048828125, 0.00146484375, 0.00341796875, 0.0, 0.00048828125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.0, 0.0, 0.0390625, 0.0, 0.00634765625, 0.0, 0.0, 0.00244140625, 0.0, 0.0, 0.03466796875, 0.0, 0.00537109375, 0.0, 0.0029296875, 0.2392578125, 0.0, 0.0546875, 0.0, 0.0, 0.0, 0.36279296875, 0.0, 0.00048828125, 0.0029296875, 0.00244140625, 0.005859375, 0.0, 0.0, 0.00146484375, 0.00390625, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00048828125, 0.0, 0.01513671875, 0.001953125, 0.00048828125, 0.00048828125, 0.00341796875, 0.05712890625, 0.0, 0.0, 0.0, 0.06640625, 0.0, 0.00537109375, 0.0, 0.0, 0.0400390625, 0.095703125, 0.00048828125, 0.0, 0.0, 0.0, 0.06494140625, 0.0, 0.0009765625, 0.01611328125, 0.0009765625, 0.01318359375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00146484375, 0.0, 0.001953125, 0.00146484375, 0.00146484375, 0.0029296875, 0.01318359375, 0.00341796875, 0.0, 0.0, 0.0, 0.0009765625, 0.00146484375, 0.0185546875, 0.00146484375, 0.0029296875, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.01025390625, 0.0, 0.27490234375, 0.00537109375, 0.08251953125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00634765625, 0.0029296875, 0.00048828125, 0.0, 0.0, 0.0302734375, 0.00048828125, 0.0, 0.00048828125, 0.21923828125, 0.0, 0.001953125, 0.0068359375, 0.0, 0.001953125, 0.0341796875, 0.00390625, 0.0, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.00244140625, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0009765625, 0.87255859375, 0.05419921875, 0.0, 0.0, 0.0654296875, 0.00048828125, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00146484375, 0.0771484375, 0.00927734375, 0.00146484375, 0.0, 0.0, 0.0, 0.68603515625, 0.00048828125, 0.00244140625, 0.00048828125, 0.0048828125, 0.0009765625, 0.0087890625, 0.0, 0.00048828125, 0.27490234375, 0.0, 0.0, 0.1357421875, 0.00830078125, 0.0009765625, 0.00390625, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.02001953125, 0.0, 0.00048828125, 0.00537109375, 0.00048828125, 0.0, 0.0, 0.00439453125, 0.00244140625, 0.0, 0.0, 0.01318359375, 0.0, 0.01318359375, 0.00341796875]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:32<00:06,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00439453125, 0.0009765625, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.00048828125, 0.00439453125, 0.0048828125, 0.00048828125, 0.009765625, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.455078125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.0146484375, 0.00146484375, 0.00146484375, 0.0, 0.0, 0.0009765625, 0.0078125, 0.0, 0.0107421875, 0.0009765625, 0.60400390625, 0.0, 0.00390625, 0.013671875, 0.0, 0.0, 0.0, 0.310546875, 0.017578125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09716796875, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.03955078125, 0.0, 0.0, 0.01025390625, 0.0, 0.0068359375, 0.00341796875, 0.06201171875, 0.0, 0.01513671875, 0.0, 0.01171875, 0.30419921875, 0.00244140625, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0146484375, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.00048828125, 0.0029296875, 0.0048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.02294921875, 0.0029296875, 0.1982421875, 0.0009765625, 0.06396484375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.06884765625, 0.00048828125, 0.0185546875, 0.0009765625, 0.0, 0.0, 0.001953125, 0.0, 0.00244140625, 0.0, 0.0, 0.55810546875, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.03466796875, 0.001953125, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0078125, 0.9794921875, 0.00439453125, 0.0, 0.00048828125, 0.0341796875, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0283203125, 0.0, 0.00146484375, 0.0009765625, 0.0087890625, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.00634765625, 0.00439453125, 0.0009765625, 0.0, 0.0, 0.00390625, 0.0, 0.0029296875, 0.00048828125, 0.0, 0.02783203125, 0.0, 0.0009765625, 0.0, 0.0, 0.033203125, 0.0, 0.0009765625, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.06298828125, 0.08447265625, 0.0, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.0498046875, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0634765625, 0.00244140625, 0.0, 0.0009765625, 0.0029296875, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.01220703125, 0.0, 0.03076171875, 0.01416015625, 0.00732421875, 0.00048828125, 0.01318359375, 0.0, 0.04296875, 0.0, 0.0, 0.0, 0.12109375, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.0009765625, 0.0029296875, 0.041015625, 0.0, 0.0, 0.00341796875, 0.1669921875, 0.00048828125, 0.00146484375, 0.0, 0.00048828125, 0.0, 0.00244140625, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.142578125, 0.0, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.0009765625, 0.06884765625, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.0, 0.01318359375, 0.01513671875, 0.01904296875, 0.0009765625, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0009765625, 0.00048828125, 0.00048828125, 0.0009765625, 0.00390625, 0.00244140625, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.02294921875, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.00830078125, 0.00048828125, 0.0, 0.0, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.26416015625, 0.28955078125, 0.0, 0.0, 0.0, 0.0732421875, 0.0009765625, 0.0009765625, 0.0009765625, 0.12939453125, 0.001953125, 0.00341796875, 0.0, 0.00048828125, 0.00048828125, 0.083984375, 0.0, 0.0, 0.03125, 0.0029296875, 0.0, 0.0029296875, 0.00537109375, 0.0, 0.00927734375, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.30224609375, 0.0, 0.0, 0.00927734375, 0.0, 0.00048828125, 0.11767578125, 0.0, 0.0, 0.0029296875, 0.00244140625, 0.00341796875, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0185546875, 0.0, 0.00048828125, 0.0009765625, 0.0, 0.00048828125, 0.06689453125, 0.00048828125, 0.00537109375, 0.0048828125, 0.0009765625, 0.0029296875, 0.00048828125, 0.00244140625, 0.0, 0.0634765625, 0.0, 0.00048828125, 0.005859375, 0.0, 0.00732421875, 0.0078125, 0.00341796875, 0.0, 0.0, 0.0087890625, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.0, 0.013671875, 0.02392578125, 0.0, 0.00537109375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.00830078125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.0, 0.00341796875, 0.00244140625, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.01806640625, 0.0, 0.0, 0.0029296875, 0.0048828125, 0.01220703125, 0.0, 0.00244140625, 0.0, 0.05712890625, 0.0, 0.0087890625, 0.00048828125, 0.0263671875, 0.03369140625, 0.1337890625, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0009765625, 0.0205078125, 0.0, 0.0087890625, 0.02587890625, 0.00146484375, 0.1943359375, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0087890625, 0.0, 0.0, 0.33447265625, 0.0078125, 0.0, 0.25927734375, 0.0, 0.0, 0.0, 0.00048828125, 0.0048828125, 0.0458984375, 0.0068359375, 0.0, 0.00537109375, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0322265625, 0.0, 0.0, 0.0, 0.357421875, 0.0, 0.01318359375, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.04541015625, 0.0, 0.0, 0.00146484375, 0.00244140625, 0.00927734375, 0.0009765625, 0.00244140625, 0.0, 0.037109375, 0.09619140625, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.00830078125, 0.00341796875, 0.0, 0.0, 0.03955078125, 0.00439453125, 0.0, 0.00244140625, 0.00439453125, 0.03271484375, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00341796875, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.001953125, 0.0, 0.08349609375, 0.0, 0.0, 0.0, 0.01318359375, 0.07763671875, 0.001953125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.06640625, 0.0, 0.09423828125, 0.0126953125, 0.00048828125, 0.00048828125, 0.0244140625, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0029296875, 0.00146484375, 0.00048828125, 0.0, 0.0, 0.0, 0.0009765625, 0.001953125, 0.01806640625, 0.00390625, 0.01123046875, 0.49609375, 0.0, 0.0, 0.0, 0.05029296875, 0.00439453125, 0.0, 0.0, 0.00390625, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.0, 0.0068359375, 0.00048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.099609375, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.00048828125, 0.0009765625, 0.03173828125, 0.0, 0.0, 0.08203125, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00244140625, 0.00048828125, 0.0, 0.001953125, 0.1767578125, 0.00048828125, 0.0, 0.0, 0.009765625, 0.00146484375, 0.00244140625, 0.0, 0.00341796875, 0.0, 0.064453125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.00048828125, 0.001953125, 0.00244140625, 0.3720703125, 0.13232421875, 0.0, 0.55810546875, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00927734375, 0.00048828125, 0.00048828125, 0.001953125, 0.0, 0.0205078125, 0.0, 0.33642578125, 0.0, 0.005859375, 0.4814453125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.12939453125, 0.00146484375, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0009765625, 0.00048828125, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00537109375, 0.00048828125, 0.00244140625, 0.0, 0.00048828125, 0.16015625, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.04296875, 0.00048828125, 0.0, 0.4453125, 0.0, 0.0, 0.0, 0.0, 0.00830078125, 0.0283203125, 0.01318359375, 0.009765625, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.0, 0.01171875, 0.0, 0.0029296875, 0.01513671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.025390625, 0.00146484375, 0.00048828125, 0.00048828125, 0.0, 0.00439453125, 0.0, 0.00146484375, 0.0078125, 0.0029296875, 0.00244140625, 0.00244140625, 0.04296875, 0.533203125, 0.46875, 0.0, 0.01171875, 0.0, 0.0, 0.001953125, 0.0029296875, 0.0, 0.00146484375, 0.0, 0.00146484375, 0.00146484375, 0.0, 0.00048828125, 0.00146484375, 0.00341796875, 0.0, 0.00048828125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.0, 0.0, 0.0390625, 0.0, 0.00634765625, 0.0, 0.0, 0.00244140625, 0.0, 0.0, 0.03466796875, 0.0, 0.00537109375, 0.0, 0.0029296875, 0.2392578125, 0.0, 0.0546875, 0.0, 0.0, 0.0, 0.36279296875, 0.0, 0.00048828125, 0.0029296875, 0.00244140625, 0.005859375, 0.0, 0.0, 0.00146484375, 0.00390625, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00048828125, 0.0, 0.01513671875, 0.001953125, 0.00048828125, 0.00048828125, 0.00341796875, 0.05712890625, 0.0, 0.0, 0.0, 0.06640625, 0.0, 0.00537109375, 0.0, 0.0, 0.0400390625, 0.095703125, 0.00048828125, 0.0, 0.0, 0.0, 0.06494140625, 0.0, 0.0009765625, 0.01611328125, 0.0009765625, 0.01318359375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00146484375, 0.0, 0.001953125, 0.00146484375, 0.00146484375, 0.0029296875, 0.01318359375, 0.00341796875, 0.0, 0.0, 0.0, 0.0009765625, 0.00146484375, 0.0185546875, 0.00146484375, 0.0029296875, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.01025390625, 0.0, 0.27490234375, 0.00537109375, 0.08251953125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00634765625, 0.0029296875, 0.00048828125, 0.0, 0.0, 0.0302734375, 0.00048828125, 0.0, 0.00048828125, 0.21923828125, 0.0, 0.001953125, 0.0068359375, 0.0, 0.001953125, 0.0341796875, 0.00390625, 0.0, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.00244140625, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0009765625, 0.87255859375, 0.05419921875, 0.0, 0.0, 0.0654296875, 0.00048828125, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00146484375, 0.0771484375, 0.00927734375, 0.00146484375, 0.0, 0.0, 0.0, 0.68603515625, 0.00048828125, 0.00244140625, 0.00048828125, 0.0048828125, 0.0009765625, 0.0087890625, 0.0, 0.00048828125, 0.27490234375, 0.0, 0.0, 0.1357421875, 0.00830078125, 0.0009765625, 0.00390625, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.02001953125, 0.0, 0.00048828125, 0.00537109375, 0.00048828125, 0.0, 0.0, 0.00439453125, 0.00244140625, 0.0, 0.0, 0.01318359375, 0.0, 0.01318359375, 0.00341796875, 0.8896484375, 0.796875, 0.19580078125, 0.1826171875, 0.9248046875, 0.37939453125, 0.99658203125, 0.99951171875, 0.99560546875, 0.9892578125, 0.9990234375, 0.92236328125, 0.951171875, 0.74462890625, 0.1318359375, 0.75341796875, 0.18603515625, 0.97802734375, 0.0107421875, 0.2421875, 0.93310546875, 0.16357421875, 0.95654296875, 0.072265625, 0.18017578125, 0.03271484375, 0.99951171875, 0.65673828125, 0.9931640625, 0.14599609375, 0.99755859375, 0.50390625, 0.00146484375, 0.662109375, 1.0, 0.91796875, 0.203125, 0.94580078125, 0.92578125, 0.96044921875, 0.25390625, 0.27197265625, 0.99853515625, 0.3564453125, 0.01318359375, 0.89111328125, 0.9736328125, 0.6943359375, 0.546875, 1.0, 0.501953125, 0.89990234375, 0.7705078125, 0.99853515625, 0.767578125, 0.0009765625, 0.0224609375, 0.880859375, 0.69921875, 0.86865234375, 0.99755859375, 0.9951171875, 0.994140625, 0.60595703125, 0.0302734375, 0.86328125, 0.8154296875, 0.92919921875, 0.88623046875, 0.291015625, 0.83544921875, 0.51171875, 0.7607421875, 0.638671875, 0.99267578125, 0.71875, 0.86474609375, 0.908203125, 0.97216796875, 0.7646484375, 0.99072265625, 0.35205078125, 0.9990234375, 0.33349609375, 1.0, 0.7880859375, 0.0, 0.0087890625, 0.9541015625, 0.42822265625, 0.341796875, 0.998046875, 0.02587890625, 0.8232421875, 0.93505859375, 0.11669921875, 0.9990234375, 0.189453125, 0.9326171875, 0.00048828125]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [00:32<00:05,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00439453125, 0.0009765625, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.00048828125, 0.00439453125, 0.0048828125, 0.00048828125, 0.009765625, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.455078125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.0146484375, 0.00146484375, 0.00146484375, 0.0, 0.0, 0.0009765625, 0.0078125, 0.0, 0.0107421875, 0.0009765625, 0.60400390625, 0.0, 0.00390625, 0.013671875, 0.0, 0.0, 0.0, 0.310546875, 0.017578125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09716796875, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.03955078125, 0.0, 0.0, 0.01025390625, 0.0, 0.0068359375, 0.00341796875, 0.06201171875, 0.0, 0.01513671875, 0.0, 0.01171875, 0.30419921875, 0.00244140625, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0146484375, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.00048828125, 0.0029296875, 0.0048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.02294921875, 0.0029296875, 0.1982421875, 0.0009765625, 0.06396484375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.06884765625, 0.00048828125, 0.0185546875, 0.0009765625, 0.0, 0.0, 0.001953125, 0.0, 0.00244140625, 0.0, 0.0, 0.55810546875, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.03466796875, 0.001953125, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0078125, 0.9794921875, 0.00439453125, 0.0, 0.00048828125, 0.0341796875, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0283203125, 0.0, 0.00146484375, 0.0009765625, 0.0087890625, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.00634765625, 0.00439453125, 0.0009765625, 0.0, 0.0, 0.00390625, 0.0, 0.0029296875, 0.00048828125, 0.0, 0.02783203125, 0.0, 0.0009765625, 0.0, 0.0, 0.033203125, 0.0, 0.0009765625, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.06298828125, 0.08447265625, 0.0, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.0498046875, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0634765625, 0.00244140625, 0.0, 0.0009765625, 0.0029296875, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.01220703125, 0.0, 0.03076171875, 0.01416015625, 0.00732421875, 0.00048828125, 0.01318359375, 0.0, 0.04296875, 0.0, 0.0, 0.0, 0.12109375, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.0009765625, 0.0029296875, 0.041015625, 0.0, 0.0, 0.00341796875, 0.1669921875, 0.00048828125, 0.00146484375, 0.0, 0.00048828125, 0.0, 0.00244140625, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.142578125, 0.0, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.0009765625, 0.06884765625, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.0, 0.01318359375, 0.01513671875, 0.01904296875, 0.0009765625, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0009765625, 0.00048828125, 0.00048828125, 0.0009765625, 0.00390625, 0.00244140625, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.02294921875, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.00830078125, 0.00048828125, 0.0, 0.0, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.26416015625, 0.28955078125, 0.0, 0.0, 0.0, 0.0732421875, 0.0009765625, 0.0009765625, 0.0009765625, 0.12939453125, 0.001953125, 0.00341796875, 0.0, 0.00048828125, 0.00048828125, 0.083984375, 0.0, 0.0, 0.03125, 0.0029296875, 0.0, 0.0029296875, 0.00537109375, 0.0, 0.00927734375, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.30224609375, 0.0, 0.0, 0.00927734375, 0.0, 0.00048828125, 0.11767578125, 0.0, 0.0, 0.0029296875, 0.00244140625, 0.00341796875, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0185546875, 0.0, 0.00048828125, 0.0009765625, 0.0, 0.00048828125, 0.06689453125, 0.00048828125, 0.00537109375, 0.0048828125, 0.0009765625, 0.0029296875, 0.00048828125, 0.00244140625, 0.0, 0.0634765625, 0.0, 0.00048828125, 0.005859375, 0.0, 0.00732421875, 0.0078125, 0.00341796875, 0.0, 0.0, 0.0087890625, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.0, 0.013671875, 0.02392578125, 0.0, 0.00537109375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.00830078125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.0, 0.00341796875, 0.00244140625, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.01806640625, 0.0, 0.0, 0.0029296875, 0.0048828125, 0.01220703125, 0.0, 0.00244140625, 0.0, 0.05712890625, 0.0, 0.0087890625, 0.00048828125, 0.0263671875, 0.03369140625, 0.1337890625, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0009765625, 0.0205078125, 0.0, 0.0087890625, 0.02587890625, 0.00146484375, 0.1943359375, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0087890625, 0.0, 0.0, 0.33447265625, 0.0078125, 0.0, 0.25927734375, 0.0, 0.0, 0.0, 0.00048828125, 0.0048828125, 0.0458984375, 0.0068359375, 0.0, 0.00537109375, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0322265625, 0.0, 0.0, 0.0, 0.357421875, 0.0, 0.01318359375, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.04541015625, 0.0, 0.0, 0.00146484375, 0.00244140625, 0.00927734375, 0.0009765625, 0.00244140625, 0.0, 0.037109375, 0.09619140625, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.00830078125, 0.00341796875, 0.0, 0.0, 0.03955078125, 0.00439453125, 0.0, 0.00244140625, 0.00439453125, 0.03271484375, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00341796875, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.001953125, 0.0, 0.08349609375, 0.0, 0.0, 0.0, 0.01318359375, 0.07763671875, 0.001953125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.06640625, 0.0, 0.09423828125, 0.0126953125, 0.00048828125, 0.00048828125, 0.0244140625, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0029296875, 0.00146484375, 0.00048828125, 0.0, 0.0, 0.0, 0.0009765625, 0.001953125, 0.01806640625, 0.00390625, 0.01123046875, 0.49609375, 0.0, 0.0, 0.0, 0.05029296875, 0.00439453125, 0.0, 0.0, 0.00390625, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.0, 0.0068359375, 0.00048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.099609375, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.00048828125, 0.0009765625, 0.03173828125, 0.0, 0.0, 0.08203125, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00244140625, 0.00048828125, 0.0, 0.001953125, 0.1767578125, 0.00048828125, 0.0, 0.0, 0.009765625, 0.00146484375, 0.00244140625, 0.0, 0.00341796875, 0.0, 0.064453125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.00048828125, 0.001953125, 0.00244140625, 0.3720703125, 0.13232421875, 0.0, 0.55810546875, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00927734375, 0.00048828125, 0.00048828125, 0.001953125, 0.0, 0.0205078125, 0.0, 0.33642578125, 0.0, 0.005859375, 0.4814453125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.12939453125, 0.00146484375, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0009765625, 0.00048828125, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00537109375, 0.00048828125, 0.00244140625, 0.0, 0.00048828125, 0.16015625, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.04296875, 0.00048828125, 0.0, 0.4453125, 0.0, 0.0, 0.0, 0.0, 0.00830078125, 0.0283203125, 0.01318359375, 0.009765625, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.0, 0.01171875, 0.0, 0.0029296875, 0.01513671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.025390625, 0.00146484375, 0.00048828125, 0.00048828125, 0.0, 0.00439453125, 0.0, 0.00146484375, 0.0078125, 0.0029296875, 0.00244140625, 0.00244140625, 0.04296875, 0.533203125, 0.46875, 0.0, 0.01171875, 0.0, 0.0, 0.001953125, 0.0029296875, 0.0, 0.00146484375, 0.0, 0.00146484375, 0.00146484375, 0.0, 0.00048828125, 0.00146484375, 0.00341796875, 0.0, 0.00048828125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.0, 0.0, 0.0390625, 0.0, 0.00634765625, 0.0, 0.0, 0.00244140625, 0.0, 0.0, 0.03466796875, 0.0, 0.00537109375, 0.0, 0.0029296875, 0.2392578125, 0.0, 0.0546875, 0.0, 0.0, 0.0, 0.36279296875, 0.0, 0.00048828125, 0.0029296875, 0.00244140625, 0.005859375, 0.0, 0.0, 0.00146484375, 0.00390625, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00048828125, 0.0, 0.01513671875, 0.001953125, 0.00048828125, 0.00048828125, 0.00341796875, 0.05712890625, 0.0, 0.0, 0.0, 0.06640625, 0.0, 0.00537109375, 0.0, 0.0, 0.0400390625, 0.095703125, 0.00048828125, 0.0, 0.0, 0.0, 0.06494140625, 0.0, 0.0009765625, 0.01611328125, 0.0009765625, 0.01318359375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00146484375, 0.0, 0.001953125, 0.00146484375, 0.00146484375, 0.0029296875, 0.01318359375, 0.00341796875, 0.0, 0.0, 0.0, 0.0009765625, 0.00146484375, 0.0185546875, 0.00146484375, 0.0029296875, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.01025390625, 0.0, 0.27490234375, 0.00537109375, 0.08251953125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00634765625, 0.0029296875, 0.00048828125, 0.0, 0.0, 0.0302734375, 0.00048828125, 0.0, 0.00048828125, 0.21923828125, 0.0, 0.001953125, 0.0068359375, 0.0, 0.001953125, 0.0341796875, 0.00390625, 0.0, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.00244140625, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0009765625, 0.87255859375, 0.05419921875, 0.0, 0.0, 0.0654296875, 0.00048828125, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00146484375, 0.0771484375, 0.00927734375, 0.00146484375, 0.0, 0.0, 0.0, 0.68603515625, 0.00048828125, 0.00244140625, 0.00048828125, 0.0048828125, 0.0009765625, 0.0087890625, 0.0, 0.00048828125, 0.27490234375, 0.0, 0.0, 0.1357421875, 0.00830078125, 0.0009765625, 0.00390625, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.02001953125, 0.0, 0.00048828125, 0.00537109375, 0.00048828125, 0.0, 0.0, 0.00439453125, 0.00244140625, 0.0, 0.0, 0.01318359375, 0.0, 0.01318359375, 0.00341796875, 0.8896484375, 0.796875, 0.19580078125, 0.1826171875, 0.9248046875, 0.37939453125, 0.99658203125, 0.99951171875, 0.99560546875, 0.9892578125, 0.9990234375, 0.92236328125, 0.951171875, 0.74462890625, 0.1318359375, 0.75341796875, 0.18603515625, 0.97802734375, 0.0107421875, 0.2421875, 0.93310546875, 0.16357421875, 0.95654296875, 0.072265625, 0.18017578125, 0.03271484375, 0.99951171875, 0.65673828125, 0.9931640625, 0.14599609375, 0.99755859375, 0.50390625, 0.00146484375, 0.662109375, 1.0, 0.91796875, 0.203125, 0.94580078125, 0.92578125, 0.96044921875, 0.25390625, 0.27197265625, 0.99853515625, 0.3564453125, 0.01318359375, 0.89111328125, 0.9736328125, 0.6943359375, 0.546875, 1.0, 0.501953125, 0.89990234375, 0.7705078125, 0.99853515625, 0.767578125, 0.0009765625, 0.0224609375, 0.880859375, 0.69921875, 0.86865234375, 0.99755859375, 0.9951171875, 0.994140625, 0.60595703125, 0.0302734375, 0.86328125, 0.8154296875, 0.92919921875, 0.88623046875, 0.291015625, 0.83544921875, 0.51171875, 0.7607421875, 0.638671875, 0.99267578125, 0.71875, 0.86474609375, 0.908203125, 0.97216796875, 0.7646484375, 0.99072265625, 0.35205078125, 0.9990234375, 0.33349609375, 1.0, 0.7880859375, 0.0, 0.0087890625, 0.9541015625, 0.42822265625, 0.341796875, 0.998046875, 0.02587890625, 0.8232421875, 0.93505859375, 0.11669921875, 0.9990234375, 0.189453125, 0.9326171875, 0.00048828125, 0.96826171875, 0.763671875, 0.4111328125, 0.9619140625, 0.12744140625, 0.93505859375, 0.7958984375, 0.89697265625, 0.06591796875, 0.9990234375, 0.75048828125, 0.96923828125, 0.4111328125, 0.5908203125, 0.509765625, 0.56005859375, 0.94384765625, 0.90380859375, 0.99658203125, 0.93017578125, 0.21337890625, 0.189453125, 0.00439453125, 0.96728515625, 0.884765625, 0.05419921875, 0.43212890625, 0.9765625, 0.04833984375, 0.89404296875, 0.82666015625, 0.7548828125, 0.0888671875, 0.00341796875, 0.99951171875, 0.98828125, 0.99853515625, 0.0341796875, 0.98193359375, 0.79052734375, 0.51171875, 0.9990234375, 0.89306640625, 0.986328125, 0.88427734375, 0.98828125, 0.951171875, 0.806640625, 0.505859375, 0.09814453125, 0.388671875, 0.9990234375, 0.96728515625, 0.99951171875, 0.79052734375, 0.9990234375, 0.998046875, 1.0, 0.99853515625, 0.9970703125, 0.9541015625, 0.0888671875, 0.001953125, 0.8671875, 0.9541015625, 0.99853515625, 0.99951171875, 0.98779296875, 0.111328125, 0.72802734375, 0.0849609375, 0.9345703125, 0.93603515625, 0.97021484375, 0.009765625, 0.00048828125, 0.97900390625, 0.9951171875, 0.01904296875, 0.74169921875, 0.9677734375, 0.97509765625, 0.392578125, 0.77734375, 0.91162109375, 0.9775390625, 1.0, 0.9775390625, 1.0, 0.77197265625, 0.99072265625, 0.17236328125, 0.93212890625, 0.1240234375, 0.533203125, 0.955078125, 0.91943359375, 0.451171875, 0.3056640625, 0.0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [00:33<00:04,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00439453125, 0.0009765625, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.00048828125, 0.00439453125, 0.0048828125, 0.00048828125, 0.009765625, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.455078125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.0146484375, 0.00146484375, 0.00146484375, 0.0, 0.0, 0.0009765625, 0.0078125, 0.0, 0.0107421875, 0.0009765625, 0.60400390625, 0.0, 0.00390625, 0.013671875, 0.0, 0.0, 0.0, 0.310546875, 0.017578125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09716796875, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.03955078125, 0.0, 0.0, 0.01025390625, 0.0, 0.0068359375, 0.00341796875, 0.06201171875, 0.0, 0.01513671875, 0.0, 0.01171875, 0.30419921875, 0.00244140625, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0146484375, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.00048828125, 0.0029296875, 0.0048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.02294921875, 0.0029296875, 0.1982421875, 0.0009765625, 0.06396484375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.06884765625, 0.00048828125, 0.0185546875, 0.0009765625, 0.0, 0.0, 0.001953125, 0.0, 0.00244140625, 0.0, 0.0, 0.55810546875, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.03466796875, 0.001953125, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0078125, 0.9794921875, 0.00439453125, 0.0, 0.00048828125, 0.0341796875, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0283203125, 0.0, 0.00146484375, 0.0009765625, 0.0087890625, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.00634765625, 0.00439453125, 0.0009765625, 0.0, 0.0, 0.00390625, 0.0, 0.0029296875, 0.00048828125, 0.0, 0.02783203125, 0.0, 0.0009765625, 0.0, 0.0, 0.033203125, 0.0, 0.0009765625, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.06298828125, 0.08447265625, 0.0, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.0498046875, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0634765625, 0.00244140625, 0.0, 0.0009765625, 0.0029296875, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.01220703125, 0.0, 0.03076171875, 0.01416015625, 0.00732421875, 0.00048828125, 0.01318359375, 0.0, 0.04296875, 0.0, 0.0, 0.0, 0.12109375, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.0009765625, 0.0029296875, 0.041015625, 0.0, 0.0, 0.00341796875, 0.1669921875, 0.00048828125, 0.00146484375, 0.0, 0.00048828125, 0.0, 0.00244140625, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.142578125, 0.0, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.0009765625, 0.06884765625, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.0, 0.01318359375, 0.01513671875, 0.01904296875, 0.0009765625, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0009765625, 0.00048828125, 0.00048828125, 0.0009765625, 0.00390625, 0.00244140625, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.02294921875, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.00830078125, 0.00048828125, 0.0, 0.0, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.26416015625, 0.28955078125, 0.0, 0.0, 0.0, 0.0732421875, 0.0009765625, 0.0009765625, 0.0009765625, 0.12939453125, 0.001953125, 0.00341796875, 0.0, 0.00048828125, 0.00048828125, 0.083984375, 0.0, 0.0, 0.03125, 0.0029296875, 0.0, 0.0029296875, 0.00537109375, 0.0, 0.00927734375, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.30224609375, 0.0, 0.0, 0.00927734375, 0.0, 0.00048828125, 0.11767578125, 0.0, 0.0, 0.0029296875, 0.00244140625, 0.00341796875, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0185546875, 0.0, 0.00048828125, 0.0009765625, 0.0, 0.00048828125, 0.06689453125, 0.00048828125, 0.00537109375, 0.0048828125, 0.0009765625, 0.0029296875, 0.00048828125, 0.00244140625, 0.0, 0.0634765625, 0.0, 0.00048828125, 0.005859375, 0.0, 0.00732421875, 0.0078125, 0.00341796875, 0.0, 0.0, 0.0087890625, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.0, 0.013671875, 0.02392578125, 0.0, 0.00537109375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.00830078125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.0, 0.00341796875, 0.00244140625, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.01806640625, 0.0, 0.0, 0.0029296875, 0.0048828125, 0.01220703125, 0.0, 0.00244140625, 0.0, 0.05712890625, 0.0, 0.0087890625, 0.00048828125, 0.0263671875, 0.03369140625, 0.1337890625, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0009765625, 0.0205078125, 0.0, 0.0087890625, 0.02587890625, 0.00146484375, 0.1943359375, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0087890625, 0.0, 0.0, 0.33447265625, 0.0078125, 0.0, 0.25927734375, 0.0, 0.0, 0.0, 0.00048828125, 0.0048828125, 0.0458984375, 0.0068359375, 0.0, 0.00537109375, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0322265625, 0.0, 0.0, 0.0, 0.357421875, 0.0, 0.01318359375, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.04541015625, 0.0, 0.0, 0.00146484375, 0.00244140625, 0.00927734375, 0.0009765625, 0.00244140625, 0.0, 0.037109375, 0.09619140625, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.00830078125, 0.00341796875, 0.0, 0.0, 0.03955078125, 0.00439453125, 0.0, 0.00244140625, 0.00439453125, 0.03271484375, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00341796875, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.001953125, 0.0, 0.08349609375, 0.0, 0.0, 0.0, 0.01318359375, 0.07763671875, 0.001953125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.06640625, 0.0, 0.09423828125, 0.0126953125, 0.00048828125, 0.00048828125, 0.0244140625, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0029296875, 0.00146484375, 0.00048828125, 0.0, 0.0, 0.0, 0.0009765625, 0.001953125, 0.01806640625, 0.00390625, 0.01123046875, 0.49609375, 0.0, 0.0, 0.0, 0.05029296875, 0.00439453125, 0.0, 0.0, 0.00390625, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.0, 0.0068359375, 0.00048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.099609375, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.00048828125, 0.0009765625, 0.03173828125, 0.0, 0.0, 0.08203125, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00244140625, 0.00048828125, 0.0, 0.001953125, 0.1767578125, 0.00048828125, 0.0, 0.0, 0.009765625, 0.00146484375, 0.00244140625, 0.0, 0.00341796875, 0.0, 0.064453125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.00048828125, 0.001953125, 0.00244140625, 0.3720703125, 0.13232421875, 0.0, 0.55810546875, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00927734375, 0.00048828125, 0.00048828125, 0.001953125, 0.0, 0.0205078125, 0.0, 0.33642578125, 0.0, 0.005859375, 0.4814453125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.12939453125, 0.00146484375, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0009765625, 0.00048828125, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00537109375, 0.00048828125, 0.00244140625, 0.0, 0.00048828125, 0.16015625, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.04296875, 0.00048828125, 0.0, 0.4453125, 0.0, 0.0, 0.0, 0.0, 0.00830078125, 0.0283203125, 0.01318359375, 0.009765625, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.0, 0.01171875, 0.0, 0.0029296875, 0.01513671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.025390625, 0.00146484375, 0.00048828125, 0.00048828125, 0.0, 0.00439453125, 0.0, 0.00146484375, 0.0078125, 0.0029296875, 0.00244140625, 0.00244140625, 0.04296875, 0.533203125, 0.46875, 0.0, 0.01171875, 0.0, 0.0, 0.001953125, 0.0029296875, 0.0, 0.00146484375, 0.0, 0.00146484375, 0.00146484375, 0.0, 0.00048828125, 0.00146484375, 0.00341796875, 0.0, 0.00048828125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.0, 0.0, 0.0390625, 0.0, 0.00634765625, 0.0, 0.0, 0.00244140625, 0.0, 0.0, 0.03466796875, 0.0, 0.00537109375, 0.0, 0.0029296875, 0.2392578125, 0.0, 0.0546875, 0.0, 0.0, 0.0, 0.36279296875, 0.0, 0.00048828125, 0.0029296875, 0.00244140625, 0.005859375, 0.0, 0.0, 0.00146484375, 0.00390625, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00048828125, 0.0, 0.01513671875, 0.001953125, 0.00048828125, 0.00048828125, 0.00341796875, 0.05712890625, 0.0, 0.0, 0.0, 0.06640625, 0.0, 0.00537109375, 0.0, 0.0, 0.0400390625, 0.095703125, 0.00048828125, 0.0, 0.0, 0.0, 0.06494140625, 0.0, 0.0009765625, 0.01611328125, 0.0009765625, 0.01318359375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00146484375, 0.0, 0.001953125, 0.00146484375, 0.00146484375, 0.0029296875, 0.01318359375, 0.00341796875, 0.0, 0.0, 0.0, 0.0009765625, 0.00146484375, 0.0185546875, 0.00146484375, 0.0029296875, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.01025390625, 0.0, 0.27490234375, 0.00537109375, 0.08251953125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00634765625, 0.0029296875, 0.00048828125, 0.0, 0.0, 0.0302734375, 0.00048828125, 0.0, 0.00048828125, 0.21923828125, 0.0, 0.001953125, 0.0068359375, 0.0, 0.001953125, 0.0341796875, 0.00390625, 0.0, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.00244140625, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0009765625, 0.87255859375, 0.05419921875, 0.0, 0.0, 0.0654296875, 0.00048828125, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00146484375, 0.0771484375, 0.00927734375, 0.00146484375, 0.0, 0.0, 0.0, 0.68603515625, 0.00048828125, 0.00244140625, 0.00048828125, 0.0048828125, 0.0009765625, 0.0087890625, 0.0, 0.00048828125, 0.27490234375, 0.0, 0.0, 0.1357421875, 0.00830078125, 0.0009765625, 0.00390625, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.02001953125, 0.0, 0.00048828125, 0.00537109375, 0.00048828125, 0.0, 0.0, 0.00439453125, 0.00244140625, 0.0, 0.0, 0.01318359375, 0.0, 0.01318359375, 0.00341796875, 0.8896484375, 0.796875, 0.19580078125, 0.1826171875, 0.9248046875, 0.37939453125, 0.99658203125, 0.99951171875, 0.99560546875, 0.9892578125, 0.9990234375, 0.92236328125, 0.951171875, 0.74462890625, 0.1318359375, 0.75341796875, 0.18603515625, 0.97802734375, 0.0107421875, 0.2421875, 0.93310546875, 0.16357421875, 0.95654296875, 0.072265625, 0.18017578125, 0.03271484375, 0.99951171875, 0.65673828125, 0.9931640625, 0.14599609375, 0.99755859375, 0.50390625, 0.00146484375, 0.662109375, 1.0, 0.91796875, 0.203125, 0.94580078125, 0.92578125, 0.96044921875, 0.25390625, 0.27197265625, 0.99853515625, 0.3564453125, 0.01318359375, 0.89111328125, 0.9736328125, 0.6943359375, 0.546875, 1.0, 0.501953125, 0.89990234375, 0.7705078125, 0.99853515625, 0.767578125, 0.0009765625, 0.0224609375, 0.880859375, 0.69921875, 0.86865234375, 0.99755859375, 0.9951171875, 0.994140625, 0.60595703125, 0.0302734375, 0.86328125, 0.8154296875, 0.92919921875, 0.88623046875, 0.291015625, 0.83544921875, 0.51171875, 0.7607421875, 0.638671875, 0.99267578125, 0.71875, 0.86474609375, 0.908203125, 0.97216796875, 0.7646484375, 0.99072265625, 0.35205078125, 0.9990234375, 0.33349609375, 1.0, 0.7880859375, 0.0, 0.0087890625, 0.9541015625, 0.42822265625, 0.341796875, 0.998046875, 0.02587890625, 0.8232421875, 0.93505859375, 0.11669921875, 0.9990234375, 0.189453125, 0.9326171875, 0.00048828125, 0.96826171875, 0.763671875, 0.4111328125, 0.9619140625, 0.12744140625, 0.93505859375, 0.7958984375, 0.89697265625, 0.06591796875, 0.9990234375, 0.75048828125, 0.96923828125, 0.4111328125, 0.5908203125, 0.509765625, 0.56005859375, 0.94384765625, 0.90380859375, 0.99658203125, 0.93017578125, 0.21337890625, 0.189453125, 0.00439453125, 0.96728515625, 0.884765625, 0.05419921875, 0.43212890625, 0.9765625, 0.04833984375, 0.89404296875, 0.82666015625, 0.7548828125, 0.0888671875, 0.00341796875, 0.99951171875, 0.98828125, 0.99853515625, 0.0341796875, 0.98193359375, 0.79052734375, 0.51171875, 0.9990234375, 0.89306640625, 0.986328125, 0.88427734375, 0.98828125, 0.951171875, 0.806640625, 0.505859375, 0.09814453125, 0.388671875, 0.9990234375, 0.96728515625, 0.99951171875, 0.79052734375, 0.9990234375, 0.998046875, 1.0, 0.99853515625, 0.9970703125, 0.9541015625, 0.0888671875, 0.001953125, 0.8671875, 0.9541015625, 0.99853515625, 0.99951171875, 0.98779296875, 0.111328125, 0.72802734375, 0.0849609375, 0.9345703125, 0.93603515625, 0.97021484375, 0.009765625, 0.00048828125, 0.97900390625, 0.9951171875, 0.01904296875, 0.74169921875, 0.9677734375, 0.97509765625, 0.392578125, 0.77734375, 0.91162109375, 0.9775390625, 1.0, 0.9775390625, 1.0, 0.77197265625, 0.99072265625, 0.17236328125, 0.93212890625, 0.1240234375, 0.533203125, 0.955078125, 0.91943359375, 0.451171875, 0.3056640625, 0.0, 0.998046875, 0.99462890625, 0.99462890625, 0.98828125, 0.51171875, 0.9091796875, 0.9990234375, 0.923828125, 0.2392578125, 0.8154296875, 0.63330078125, 0.974609375, 0.525390625, 0.9189453125, 0.99853515625, 0.01611328125, 0.9814453125, 0.99072265625, 0.00390625, 0.14892578125, 0.95947265625, 0.015625, 0.95361328125, 0.14111328125, 0.43798828125, 0.640625, 0.9970703125, 0.93310546875, 0.998046875, 0.98876953125, 0.93994140625, 0.98046875, 0.01025390625, 0.9833984375, 0.93896484375, 0.91259765625, 1.0, 0.845703125, 0.99951171875, 0.7265625, 0.99951171875, 0.85986328125, 0.0009765625, 0.9794921875, 0.3740234375, 0.2265625, 0.98388671875, 0.99560546875, 0.60009765625, 0.9912109375, 0.0380859375, 0.00048828125, 0.8916015625, 1.0, 0.0849609375, 0.556640625, 1.0, 0.08447265625, 1.0, 1.0, 0.92724609375, 0.94580078125, 0.1376953125, 0.0615234375, 0.498046875, 0.29443359375, 0.921875, 0.986328125, 0.9638671875, 0.451171875, 0.9375, 0.02099609375, 0.083984375, 0.87890625, 1.0, 0.96826171875, 0.95751953125, 0.65478515625, 0.80078125, 0.60205078125, 0.99169921875, 0.73583984375, 0.99951171875, 0.9736328125, 0.99267578125, 0.1259765625, 0.3486328125, 0.99755859375, 0.10888671875, 0.17333984375, 0.1044921875, 0.81884765625, 0.001953125, 0.2255859375, 1.0, 0.85107421875, 0.99658203125, 0.85595703125, 0.0947265625, 0.9638671875]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [00:33<00:03,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00439453125, 0.0009765625, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.00048828125, 0.00439453125, 0.0048828125, 0.00048828125, 0.009765625, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.455078125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.0146484375, 0.00146484375, 0.00146484375, 0.0, 0.0, 0.0009765625, 0.0078125, 0.0, 0.0107421875, 0.0009765625, 0.60400390625, 0.0, 0.00390625, 0.013671875, 0.0, 0.0, 0.0, 0.310546875, 0.017578125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09716796875, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.03955078125, 0.0, 0.0, 0.01025390625, 0.0, 0.0068359375, 0.00341796875, 0.06201171875, 0.0, 0.01513671875, 0.0, 0.01171875, 0.30419921875, 0.00244140625, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0146484375, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.00048828125, 0.0029296875, 0.0048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.02294921875, 0.0029296875, 0.1982421875, 0.0009765625, 0.06396484375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.06884765625, 0.00048828125, 0.0185546875, 0.0009765625, 0.0, 0.0, 0.001953125, 0.0, 0.00244140625, 0.0, 0.0, 0.55810546875, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.03466796875, 0.001953125, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0078125, 0.9794921875, 0.00439453125, 0.0, 0.00048828125, 0.0341796875, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0283203125, 0.0, 0.00146484375, 0.0009765625, 0.0087890625, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.00634765625, 0.00439453125, 0.0009765625, 0.0, 0.0, 0.00390625, 0.0, 0.0029296875, 0.00048828125, 0.0, 0.02783203125, 0.0, 0.0009765625, 0.0, 0.0, 0.033203125, 0.0, 0.0009765625, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.06298828125, 0.08447265625, 0.0, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.0498046875, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0634765625, 0.00244140625, 0.0, 0.0009765625, 0.0029296875, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.01220703125, 0.0, 0.03076171875, 0.01416015625, 0.00732421875, 0.00048828125, 0.01318359375, 0.0, 0.04296875, 0.0, 0.0, 0.0, 0.12109375, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.0009765625, 0.0029296875, 0.041015625, 0.0, 0.0, 0.00341796875, 0.1669921875, 0.00048828125, 0.00146484375, 0.0, 0.00048828125, 0.0, 0.00244140625, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.142578125, 0.0, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.0009765625, 0.06884765625, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.0, 0.01318359375, 0.01513671875, 0.01904296875, 0.0009765625, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0009765625, 0.00048828125, 0.00048828125, 0.0009765625, 0.00390625, 0.00244140625, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.02294921875, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.00830078125, 0.00048828125, 0.0, 0.0, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.26416015625, 0.28955078125, 0.0, 0.0, 0.0, 0.0732421875, 0.0009765625, 0.0009765625, 0.0009765625, 0.12939453125, 0.001953125, 0.00341796875, 0.0, 0.00048828125, 0.00048828125, 0.083984375, 0.0, 0.0, 0.03125, 0.0029296875, 0.0, 0.0029296875, 0.00537109375, 0.0, 0.00927734375, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.30224609375, 0.0, 0.0, 0.00927734375, 0.0, 0.00048828125, 0.11767578125, 0.0, 0.0, 0.0029296875, 0.00244140625, 0.00341796875, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0185546875, 0.0, 0.00048828125, 0.0009765625, 0.0, 0.00048828125, 0.06689453125, 0.00048828125, 0.00537109375, 0.0048828125, 0.0009765625, 0.0029296875, 0.00048828125, 0.00244140625, 0.0, 0.0634765625, 0.0, 0.00048828125, 0.005859375, 0.0, 0.00732421875, 0.0078125, 0.00341796875, 0.0, 0.0, 0.0087890625, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.0, 0.013671875, 0.02392578125, 0.0, 0.00537109375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.00830078125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.0, 0.00341796875, 0.00244140625, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.01806640625, 0.0, 0.0, 0.0029296875, 0.0048828125, 0.01220703125, 0.0, 0.00244140625, 0.0, 0.05712890625, 0.0, 0.0087890625, 0.00048828125, 0.0263671875, 0.03369140625, 0.1337890625, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0009765625, 0.0205078125, 0.0, 0.0087890625, 0.02587890625, 0.00146484375, 0.1943359375, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0087890625, 0.0, 0.0, 0.33447265625, 0.0078125, 0.0, 0.25927734375, 0.0, 0.0, 0.0, 0.00048828125, 0.0048828125, 0.0458984375, 0.0068359375, 0.0, 0.00537109375, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0322265625, 0.0, 0.0, 0.0, 0.357421875, 0.0, 0.01318359375, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.04541015625, 0.0, 0.0, 0.00146484375, 0.00244140625, 0.00927734375, 0.0009765625, 0.00244140625, 0.0, 0.037109375, 0.09619140625, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.00830078125, 0.00341796875, 0.0, 0.0, 0.03955078125, 0.00439453125, 0.0, 0.00244140625, 0.00439453125, 0.03271484375, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00341796875, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.001953125, 0.0, 0.08349609375, 0.0, 0.0, 0.0, 0.01318359375, 0.07763671875, 0.001953125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.06640625, 0.0, 0.09423828125, 0.0126953125, 0.00048828125, 0.00048828125, 0.0244140625, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0029296875, 0.00146484375, 0.00048828125, 0.0, 0.0, 0.0, 0.0009765625, 0.001953125, 0.01806640625, 0.00390625, 0.01123046875, 0.49609375, 0.0, 0.0, 0.0, 0.05029296875, 0.00439453125, 0.0, 0.0, 0.00390625, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.0, 0.0068359375, 0.00048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.099609375, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.00048828125, 0.0009765625, 0.03173828125, 0.0, 0.0, 0.08203125, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00244140625, 0.00048828125, 0.0, 0.001953125, 0.1767578125, 0.00048828125, 0.0, 0.0, 0.009765625, 0.00146484375, 0.00244140625, 0.0, 0.00341796875, 0.0, 0.064453125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.00048828125, 0.001953125, 0.00244140625, 0.3720703125, 0.13232421875, 0.0, 0.55810546875, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00927734375, 0.00048828125, 0.00048828125, 0.001953125, 0.0, 0.0205078125, 0.0, 0.33642578125, 0.0, 0.005859375, 0.4814453125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.12939453125, 0.00146484375, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0009765625, 0.00048828125, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00537109375, 0.00048828125, 0.00244140625, 0.0, 0.00048828125, 0.16015625, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.04296875, 0.00048828125, 0.0, 0.4453125, 0.0, 0.0, 0.0, 0.0, 0.00830078125, 0.0283203125, 0.01318359375, 0.009765625, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.0, 0.01171875, 0.0, 0.0029296875, 0.01513671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.025390625, 0.00146484375, 0.00048828125, 0.00048828125, 0.0, 0.00439453125, 0.0, 0.00146484375, 0.0078125, 0.0029296875, 0.00244140625, 0.00244140625, 0.04296875, 0.533203125, 0.46875, 0.0, 0.01171875, 0.0, 0.0, 0.001953125, 0.0029296875, 0.0, 0.00146484375, 0.0, 0.00146484375, 0.00146484375, 0.0, 0.00048828125, 0.00146484375, 0.00341796875, 0.0, 0.00048828125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.0, 0.0, 0.0390625, 0.0, 0.00634765625, 0.0, 0.0, 0.00244140625, 0.0, 0.0, 0.03466796875, 0.0, 0.00537109375, 0.0, 0.0029296875, 0.2392578125, 0.0, 0.0546875, 0.0, 0.0, 0.0, 0.36279296875, 0.0, 0.00048828125, 0.0029296875, 0.00244140625, 0.005859375, 0.0, 0.0, 0.00146484375, 0.00390625, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00048828125, 0.0, 0.01513671875, 0.001953125, 0.00048828125, 0.00048828125, 0.00341796875, 0.05712890625, 0.0, 0.0, 0.0, 0.06640625, 0.0, 0.00537109375, 0.0, 0.0, 0.0400390625, 0.095703125, 0.00048828125, 0.0, 0.0, 0.0, 0.06494140625, 0.0, 0.0009765625, 0.01611328125, 0.0009765625, 0.01318359375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00146484375, 0.0, 0.001953125, 0.00146484375, 0.00146484375, 0.0029296875, 0.01318359375, 0.00341796875, 0.0, 0.0, 0.0, 0.0009765625, 0.00146484375, 0.0185546875, 0.00146484375, 0.0029296875, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.01025390625, 0.0, 0.27490234375, 0.00537109375, 0.08251953125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00634765625, 0.0029296875, 0.00048828125, 0.0, 0.0, 0.0302734375, 0.00048828125, 0.0, 0.00048828125, 0.21923828125, 0.0, 0.001953125, 0.0068359375, 0.0, 0.001953125, 0.0341796875, 0.00390625, 0.0, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.00244140625, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0009765625, 0.87255859375, 0.05419921875, 0.0, 0.0, 0.0654296875, 0.00048828125, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00146484375, 0.0771484375, 0.00927734375, 0.00146484375, 0.0, 0.0, 0.0, 0.68603515625, 0.00048828125, 0.00244140625, 0.00048828125, 0.0048828125, 0.0009765625, 0.0087890625, 0.0, 0.00048828125, 0.27490234375, 0.0, 0.0, 0.1357421875, 0.00830078125, 0.0009765625, 0.00390625, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.02001953125, 0.0, 0.00048828125, 0.00537109375, 0.00048828125, 0.0, 0.0, 0.00439453125, 0.00244140625, 0.0, 0.0, 0.01318359375, 0.0, 0.01318359375, 0.00341796875, 0.8896484375, 0.796875, 0.19580078125, 0.1826171875, 0.9248046875, 0.37939453125, 0.99658203125, 0.99951171875, 0.99560546875, 0.9892578125, 0.9990234375, 0.92236328125, 0.951171875, 0.74462890625, 0.1318359375, 0.75341796875, 0.18603515625, 0.97802734375, 0.0107421875, 0.2421875, 0.93310546875, 0.16357421875, 0.95654296875, 0.072265625, 0.18017578125, 0.03271484375, 0.99951171875, 0.65673828125, 0.9931640625, 0.14599609375, 0.99755859375, 0.50390625, 0.00146484375, 0.662109375, 1.0, 0.91796875, 0.203125, 0.94580078125, 0.92578125, 0.96044921875, 0.25390625, 0.27197265625, 0.99853515625, 0.3564453125, 0.01318359375, 0.89111328125, 0.9736328125, 0.6943359375, 0.546875, 1.0, 0.501953125, 0.89990234375, 0.7705078125, 0.99853515625, 0.767578125, 0.0009765625, 0.0224609375, 0.880859375, 0.69921875, 0.86865234375, 0.99755859375, 0.9951171875, 0.994140625, 0.60595703125, 0.0302734375, 0.86328125, 0.8154296875, 0.92919921875, 0.88623046875, 0.291015625, 0.83544921875, 0.51171875, 0.7607421875, 0.638671875, 0.99267578125, 0.71875, 0.86474609375, 0.908203125, 0.97216796875, 0.7646484375, 0.99072265625, 0.35205078125, 0.9990234375, 0.33349609375, 1.0, 0.7880859375, 0.0, 0.0087890625, 0.9541015625, 0.42822265625, 0.341796875, 0.998046875, 0.02587890625, 0.8232421875, 0.93505859375, 0.11669921875, 0.9990234375, 0.189453125, 0.9326171875, 0.00048828125, 0.96826171875, 0.763671875, 0.4111328125, 0.9619140625, 0.12744140625, 0.93505859375, 0.7958984375, 0.89697265625, 0.06591796875, 0.9990234375, 0.75048828125, 0.96923828125, 0.4111328125, 0.5908203125, 0.509765625, 0.56005859375, 0.94384765625, 0.90380859375, 0.99658203125, 0.93017578125, 0.21337890625, 0.189453125, 0.00439453125, 0.96728515625, 0.884765625, 0.05419921875, 0.43212890625, 0.9765625, 0.04833984375, 0.89404296875, 0.82666015625, 0.7548828125, 0.0888671875, 0.00341796875, 0.99951171875, 0.98828125, 0.99853515625, 0.0341796875, 0.98193359375, 0.79052734375, 0.51171875, 0.9990234375, 0.89306640625, 0.986328125, 0.88427734375, 0.98828125, 0.951171875, 0.806640625, 0.505859375, 0.09814453125, 0.388671875, 0.9990234375, 0.96728515625, 0.99951171875, 0.79052734375, 0.9990234375, 0.998046875, 1.0, 0.99853515625, 0.9970703125, 0.9541015625, 0.0888671875, 0.001953125, 0.8671875, 0.9541015625, 0.99853515625, 0.99951171875, 0.98779296875, 0.111328125, 0.72802734375, 0.0849609375, 0.9345703125, 0.93603515625, 0.97021484375, 0.009765625, 0.00048828125, 0.97900390625, 0.9951171875, 0.01904296875, 0.74169921875, 0.9677734375, 0.97509765625, 0.392578125, 0.77734375, 0.91162109375, 0.9775390625, 1.0, 0.9775390625, 1.0, 0.77197265625, 0.99072265625, 0.17236328125, 0.93212890625, 0.1240234375, 0.533203125, 0.955078125, 0.91943359375, 0.451171875, 0.3056640625, 0.0, 0.998046875, 0.99462890625, 0.99462890625, 0.98828125, 0.51171875, 0.9091796875, 0.9990234375, 0.923828125, 0.2392578125, 0.8154296875, 0.63330078125, 0.974609375, 0.525390625, 0.9189453125, 0.99853515625, 0.01611328125, 0.9814453125, 0.99072265625, 0.00390625, 0.14892578125, 0.95947265625, 0.015625, 0.95361328125, 0.14111328125, 0.43798828125, 0.640625, 0.9970703125, 0.93310546875, 0.998046875, 0.98876953125, 0.93994140625, 0.98046875, 0.01025390625, 0.9833984375, 0.93896484375, 0.91259765625, 1.0, 0.845703125, 0.99951171875, 0.7265625, 0.99951171875, 0.85986328125, 0.0009765625, 0.9794921875, 0.3740234375, 0.2265625, 0.98388671875, 0.99560546875, 0.60009765625, 0.9912109375, 0.0380859375, 0.00048828125, 0.8916015625, 1.0, 0.0849609375, 0.556640625, 1.0, 0.08447265625, 1.0, 1.0, 0.92724609375, 0.94580078125, 0.1376953125, 0.0615234375, 0.498046875, 0.29443359375, 0.921875, 0.986328125, 0.9638671875, 0.451171875, 0.9375, 0.02099609375, 0.083984375, 0.87890625, 1.0, 0.96826171875, 0.95751953125, 0.65478515625, 0.80078125, 0.60205078125, 0.99169921875, 0.73583984375, 0.99951171875, 0.9736328125, 0.99267578125, 0.1259765625, 0.3486328125, 0.99755859375, 0.10888671875, 0.17333984375, 0.1044921875, 0.81884765625, 0.001953125, 0.2255859375, 1.0, 0.85107421875, 0.99658203125, 0.85595703125, 0.0947265625, 0.9638671875, 0.9638671875, 0.98388671875, 0.9970703125, 0.57958984375, 0.02587890625, 0.7744140625, 0.80419921875, 0.8642578125, 1.0, 0.99658203125, 0.974609375, 0.0322265625, 0.6689453125, 0.97998046875, 0.505859375, 0.9921875, 0.96875, 0.12744140625, 0.716796875, 0.00439453125, 0.845703125, 0.998046875, 0.93994140625, 0.93701171875, 0.94970703125, 0.0966796875, 0.91259765625, 0.04345703125, 0.99951171875, 0.57763671875, 0.77880859375, 1.0, 0.05322265625, 0.9111328125, 0.9970703125, 0.86083984375, 0.99462890625, 0.1611328125, 0.56005859375, 0.1572265625, 0.57958984375, 0.9873046875, 0.99951171875, 0.537109375, 0.99853515625, 0.9970703125, 0.9990234375, 0.99951171875, 0.5849609375, 0.8017578125, 0.97216796875, 0.83349609375, 0.98095703125, 0.0537109375, 0.8173828125, 0.990234375, 0.05810546875, 0.9267578125, 0.09423828125, 0.54296875, 1.0, 0.05810546875, 0.73876953125, 1.0, 0.9794921875, 0.0751953125, 0.00146484375, 0.0927734375, 0.7216796875, 0.00732421875, 0.00927734375, 0.91552734375, 0.98681640625, 0.9833984375, 0.5390625, 0.9970703125, 0.83642578125, 0.9072265625, 0.9970703125, 0.00732421875, 0.390625, 0.99072265625, 0.513671875, 0.31396484375, 1.0, 0.91259765625, 0.970703125, 0.2353515625, 0.39990234375, 0.03515625, 0.95703125, 0.95751953125, 0.85888671875, 0.9765625, 0.01220703125, 0.99072265625, 0.99462890625, 0.99365234375, 0.3681640625, 0.88330078125]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:34<00:02,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00439453125, 0.0009765625, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.00048828125, 0.00439453125, 0.0048828125, 0.00048828125, 0.009765625, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.455078125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.0146484375, 0.00146484375, 0.00146484375, 0.0, 0.0, 0.0009765625, 0.0078125, 0.0, 0.0107421875, 0.0009765625, 0.60400390625, 0.0, 0.00390625, 0.013671875, 0.0, 0.0, 0.0, 0.310546875, 0.017578125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09716796875, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.03955078125, 0.0, 0.0, 0.01025390625, 0.0, 0.0068359375, 0.00341796875, 0.06201171875, 0.0, 0.01513671875, 0.0, 0.01171875, 0.30419921875, 0.00244140625, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0146484375, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.00048828125, 0.0029296875, 0.0048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.02294921875, 0.0029296875, 0.1982421875, 0.0009765625, 0.06396484375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.06884765625, 0.00048828125, 0.0185546875, 0.0009765625, 0.0, 0.0, 0.001953125, 0.0, 0.00244140625, 0.0, 0.0, 0.55810546875, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.03466796875, 0.001953125, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0078125, 0.9794921875, 0.00439453125, 0.0, 0.00048828125, 0.0341796875, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0283203125, 0.0, 0.00146484375, 0.0009765625, 0.0087890625, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.00634765625, 0.00439453125, 0.0009765625, 0.0, 0.0, 0.00390625, 0.0, 0.0029296875, 0.00048828125, 0.0, 0.02783203125, 0.0, 0.0009765625, 0.0, 0.0, 0.033203125, 0.0, 0.0009765625, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.06298828125, 0.08447265625, 0.0, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.0498046875, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0634765625, 0.00244140625, 0.0, 0.0009765625, 0.0029296875, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.01220703125, 0.0, 0.03076171875, 0.01416015625, 0.00732421875, 0.00048828125, 0.01318359375, 0.0, 0.04296875, 0.0, 0.0, 0.0, 0.12109375, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.0009765625, 0.0029296875, 0.041015625, 0.0, 0.0, 0.00341796875, 0.1669921875, 0.00048828125, 0.00146484375, 0.0, 0.00048828125, 0.0, 0.00244140625, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.142578125, 0.0, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.0009765625, 0.06884765625, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.0, 0.01318359375, 0.01513671875, 0.01904296875, 0.0009765625, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0009765625, 0.00048828125, 0.00048828125, 0.0009765625, 0.00390625, 0.00244140625, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.02294921875, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.00830078125, 0.00048828125, 0.0, 0.0, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.26416015625, 0.28955078125, 0.0, 0.0, 0.0, 0.0732421875, 0.0009765625, 0.0009765625, 0.0009765625, 0.12939453125, 0.001953125, 0.00341796875, 0.0, 0.00048828125, 0.00048828125, 0.083984375, 0.0, 0.0, 0.03125, 0.0029296875, 0.0, 0.0029296875, 0.00537109375, 0.0, 0.00927734375, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.30224609375, 0.0, 0.0, 0.00927734375, 0.0, 0.00048828125, 0.11767578125, 0.0, 0.0, 0.0029296875, 0.00244140625, 0.00341796875, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0185546875, 0.0, 0.00048828125, 0.0009765625, 0.0, 0.00048828125, 0.06689453125, 0.00048828125, 0.00537109375, 0.0048828125, 0.0009765625, 0.0029296875, 0.00048828125, 0.00244140625, 0.0, 0.0634765625, 0.0, 0.00048828125, 0.005859375, 0.0, 0.00732421875, 0.0078125, 0.00341796875, 0.0, 0.0, 0.0087890625, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.0, 0.013671875, 0.02392578125, 0.0, 0.00537109375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.00830078125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.0, 0.00341796875, 0.00244140625, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.01806640625, 0.0, 0.0, 0.0029296875, 0.0048828125, 0.01220703125, 0.0, 0.00244140625, 0.0, 0.05712890625, 0.0, 0.0087890625, 0.00048828125, 0.0263671875, 0.03369140625, 0.1337890625, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0009765625, 0.0205078125, 0.0, 0.0087890625, 0.02587890625, 0.00146484375, 0.1943359375, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0087890625, 0.0, 0.0, 0.33447265625, 0.0078125, 0.0, 0.25927734375, 0.0, 0.0, 0.0, 0.00048828125, 0.0048828125, 0.0458984375, 0.0068359375, 0.0, 0.00537109375, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0322265625, 0.0, 0.0, 0.0, 0.357421875, 0.0, 0.01318359375, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.04541015625, 0.0, 0.0, 0.00146484375, 0.00244140625, 0.00927734375, 0.0009765625, 0.00244140625, 0.0, 0.037109375, 0.09619140625, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.00830078125, 0.00341796875, 0.0, 0.0, 0.03955078125, 0.00439453125, 0.0, 0.00244140625, 0.00439453125, 0.03271484375, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00341796875, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.001953125, 0.0, 0.08349609375, 0.0, 0.0, 0.0, 0.01318359375, 0.07763671875, 0.001953125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.06640625, 0.0, 0.09423828125, 0.0126953125, 0.00048828125, 0.00048828125, 0.0244140625, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0029296875, 0.00146484375, 0.00048828125, 0.0, 0.0, 0.0, 0.0009765625, 0.001953125, 0.01806640625, 0.00390625, 0.01123046875, 0.49609375, 0.0, 0.0, 0.0, 0.05029296875, 0.00439453125, 0.0, 0.0, 0.00390625, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.0, 0.0068359375, 0.00048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.099609375, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.00048828125, 0.0009765625, 0.03173828125, 0.0, 0.0, 0.08203125, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00244140625, 0.00048828125, 0.0, 0.001953125, 0.1767578125, 0.00048828125, 0.0, 0.0, 0.009765625, 0.00146484375, 0.00244140625, 0.0, 0.00341796875, 0.0, 0.064453125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.00048828125, 0.001953125, 0.00244140625, 0.3720703125, 0.13232421875, 0.0, 0.55810546875, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00927734375, 0.00048828125, 0.00048828125, 0.001953125, 0.0, 0.0205078125, 0.0, 0.33642578125, 0.0, 0.005859375, 0.4814453125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.12939453125, 0.00146484375, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0009765625, 0.00048828125, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00537109375, 0.00048828125, 0.00244140625, 0.0, 0.00048828125, 0.16015625, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.04296875, 0.00048828125, 0.0, 0.4453125, 0.0, 0.0, 0.0, 0.0, 0.00830078125, 0.0283203125, 0.01318359375, 0.009765625, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.0, 0.01171875, 0.0, 0.0029296875, 0.01513671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.025390625, 0.00146484375, 0.00048828125, 0.00048828125, 0.0, 0.00439453125, 0.0, 0.00146484375, 0.0078125, 0.0029296875, 0.00244140625, 0.00244140625, 0.04296875, 0.533203125, 0.46875, 0.0, 0.01171875, 0.0, 0.0, 0.001953125, 0.0029296875, 0.0, 0.00146484375, 0.0, 0.00146484375, 0.00146484375, 0.0, 0.00048828125, 0.00146484375, 0.00341796875, 0.0, 0.00048828125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.0, 0.0, 0.0390625, 0.0, 0.00634765625, 0.0, 0.0, 0.00244140625, 0.0, 0.0, 0.03466796875, 0.0, 0.00537109375, 0.0, 0.0029296875, 0.2392578125, 0.0, 0.0546875, 0.0, 0.0, 0.0, 0.36279296875, 0.0, 0.00048828125, 0.0029296875, 0.00244140625, 0.005859375, 0.0, 0.0, 0.00146484375, 0.00390625, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00048828125, 0.0, 0.01513671875, 0.001953125, 0.00048828125, 0.00048828125, 0.00341796875, 0.05712890625, 0.0, 0.0, 0.0, 0.06640625, 0.0, 0.00537109375, 0.0, 0.0, 0.0400390625, 0.095703125, 0.00048828125, 0.0, 0.0, 0.0, 0.06494140625, 0.0, 0.0009765625, 0.01611328125, 0.0009765625, 0.01318359375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00146484375, 0.0, 0.001953125, 0.00146484375, 0.00146484375, 0.0029296875, 0.01318359375, 0.00341796875, 0.0, 0.0, 0.0, 0.0009765625, 0.00146484375, 0.0185546875, 0.00146484375, 0.0029296875, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.01025390625, 0.0, 0.27490234375, 0.00537109375, 0.08251953125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00634765625, 0.0029296875, 0.00048828125, 0.0, 0.0, 0.0302734375, 0.00048828125, 0.0, 0.00048828125, 0.21923828125, 0.0, 0.001953125, 0.0068359375, 0.0, 0.001953125, 0.0341796875, 0.00390625, 0.0, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.00244140625, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0009765625, 0.87255859375, 0.05419921875, 0.0, 0.0, 0.0654296875, 0.00048828125, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00146484375, 0.0771484375, 0.00927734375, 0.00146484375, 0.0, 0.0, 0.0, 0.68603515625, 0.00048828125, 0.00244140625, 0.00048828125, 0.0048828125, 0.0009765625, 0.0087890625, 0.0, 0.00048828125, 0.27490234375, 0.0, 0.0, 0.1357421875, 0.00830078125, 0.0009765625, 0.00390625, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.02001953125, 0.0, 0.00048828125, 0.00537109375, 0.00048828125, 0.0, 0.0, 0.00439453125, 0.00244140625, 0.0, 0.0, 0.01318359375, 0.0, 0.01318359375, 0.00341796875, 0.8896484375, 0.796875, 0.19580078125, 0.1826171875, 0.9248046875, 0.37939453125, 0.99658203125, 0.99951171875, 0.99560546875, 0.9892578125, 0.9990234375, 0.92236328125, 0.951171875, 0.74462890625, 0.1318359375, 0.75341796875, 0.18603515625, 0.97802734375, 0.0107421875, 0.2421875, 0.93310546875, 0.16357421875, 0.95654296875, 0.072265625, 0.18017578125, 0.03271484375, 0.99951171875, 0.65673828125, 0.9931640625, 0.14599609375, 0.99755859375, 0.50390625, 0.00146484375, 0.662109375, 1.0, 0.91796875, 0.203125, 0.94580078125, 0.92578125, 0.96044921875, 0.25390625, 0.27197265625, 0.99853515625, 0.3564453125, 0.01318359375, 0.89111328125, 0.9736328125, 0.6943359375, 0.546875, 1.0, 0.501953125, 0.89990234375, 0.7705078125, 0.99853515625, 0.767578125, 0.0009765625, 0.0224609375, 0.880859375, 0.69921875, 0.86865234375, 0.99755859375, 0.9951171875, 0.994140625, 0.60595703125, 0.0302734375, 0.86328125, 0.8154296875, 0.92919921875, 0.88623046875, 0.291015625, 0.83544921875, 0.51171875, 0.7607421875, 0.638671875, 0.99267578125, 0.71875, 0.86474609375, 0.908203125, 0.97216796875, 0.7646484375, 0.99072265625, 0.35205078125, 0.9990234375, 0.33349609375, 1.0, 0.7880859375, 0.0, 0.0087890625, 0.9541015625, 0.42822265625, 0.341796875, 0.998046875, 0.02587890625, 0.8232421875, 0.93505859375, 0.11669921875, 0.9990234375, 0.189453125, 0.9326171875, 0.00048828125, 0.96826171875, 0.763671875, 0.4111328125, 0.9619140625, 0.12744140625, 0.93505859375, 0.7958984375, 0.89697265625, 0.06591796875, 0.9990234375, 0.75048828125, 0.96923828125, 0.4111328125, 0.5908203125, 0.509765625, 0.56005859375, 0.94384765625, 0.90380859375, 0.99658203125, 0.93017578125, 0.21337890625, 0.189453125, 0.00439453125, 0.96728515625, 0.884765625, 0.05419921875, 0.43212890625, 0.9765625, 0.04833984375, 0.89404296875, 0.82666015625, 0.7548828125, 0.0888671875, 0.00341796875, 0.99951171875, 0.98828125, 0.99853515625, 0.0341796875, 0.98193359375, 0.79052734375, 0.51171875, 0.9990234375, 0.89306640625, 0.986328125, 0.88427734375, 0.98828125, 0.951171875, 0.806640625, 0.505859375, 0.09814453125, 0.388671875, 0.9990234375, 0.96728515625, 0.99951171875, 0.79052734375, 0.9990234375, 0.998046875, 1.0, 0.99853515625, 0.9970703125, 0.9541015625, 0.0888671875, 0.001953125, 0.8671875, 0.9541015625, 0.99853515625, 0.99951171875, 0.98779296875, 0.111328125, 0.72802734375, 0.0849609375, 0.9345703125, 0.93603515625, 0.97021484375, 0.009765625, 0.00048828125, 0.97900390625, 0.9951171875, 0.01904296875, 0.74169921875, 0.9677734375, 0.97509765625, 0.392578125, 0.77734375, 0.91162109375, 0.9775390625, 1.0, 0.9775390625, 1.0, 0.77197265625, 0.99072265625, 0.17236328125, 0.93212890625, 0.1240234375, 0.533203125, 0.955078125, 0.91943359375, 0.451171875, 0.3056640625, 0.0, 0.998046875, 0.99462890625, 0.99462890625, 0.98828125, 0.51171875, 0.9091796875, 0.9990234375, 0.923828125, 0.2392578125, 0.8154296875, 0.63330078125, 0.974609375, 0.525390625, 0.9189453125, 0.99853515625, 0.01611328125, 0.9814453125, 0.99072265625, 0.00390625, 0.14892578125, 0.95947265625, 0.015625, 0.95361328125, 0.14111328125, 0.43798828125, 0.640625, 0.9970703125, 0.93310546875, 0.998046875, 0.98876953125, 0.93994140625, 0.98046875, 0.01025390625, 0.9833984375, 0.93896484375, 0.91259765625, 1.0, 0.845703125, 0.99951171875, 0.7265625, 0.99951171875, 0.85986328125, 0.0009765625, 0.9794921875, 0.3740234375, 0.2265625, 0.98388671875, 0.99560546875, 0.60009765625, 0.9912109375, 0.0380859375, 0.00048828125, 0.8916015625, 1.0, 0.0849609375, 0.556640625, 1.0, 0.08447265625, 1.0, 1.0, 0.92724609375, 0.94580078125, 0.1376953125, 0.0615234375, 0.498046875, 0.29443359375, 0.921875, 0.986328125, 0.9638671875, 0.451171875, 0.9375, 0.02099609375, 0.083984375, 0.87890625, 1.0, 0.96826171875, 0.95751953125, 0.65478515625, 0.80078125, 0.60205078125, 0.99169921875, 0.73583984375, 0.99951171875, 0.9736328125, 0.99267578125, 0.1259765625, 0.3486328125, 0.99755859375, 0.10888671875, 0.17333984375, 0.1044921875, 0.81884765625, 0.001953125, 0.2255859375, 1.0, 0.85107421875, 0.99658203125, 0.85595703125, 0.0947265625, 0.9638671875, 0.9638671875, 0.98388671875, 0.9970703125, 0.57958984375, 0.02587890625, 0.7744140625, 0.80419921875, 0.8642578125, 1.0, 0.99658203125, 0.974609375, 0.0322265625, 0.6689453125, 0.97998046875, 0.505859375, 0.9921875, 0.96875, 0.12744140625, 0.716796875, 0.00439453125, 0.845703125, 0.998046875, 0.93994140625, 0.93701171875, 0.94970703125, 0.0966796875, 0.91259765625, 0.04345703125, 0.99951171875, 0.57763671875, 0.77880859375, 1.0, 0.05322265625, 0.9111328125, 0.9970703125, 0.86083984375, 0.99462890625, 0.1611328125, 0.56005859375, 0.1572265625, 0.57958984375, 0.9873046875, 0.99951171875, 0.537109375, 0.99853515625, 0.9970703125, 0.9990234375, 0.99951171875, 0.5849609375, 0.8017578125, 0.97216796875, 0.83349609375, 0.98095703125, 0.0537109375, 0.8173828125, 0.990234375, 0.05810546875, 0.9267578125, 0.09423828125, 0.54296875, 1.0, 0.05810546875, 0.73876953125, 1.0, 0.9794921875, 0.0751953125, 0.00146484375, 0.0927734375, 0.7216796875, 0.00732421875, 0.00927734375, 0.91552734375, 0.98681640625, 0.9833984375, 0.5390625, 0.9970703125, 0.83642578125, 0.9072265625, 0.9970703125, 0.00732421875, 0.390625, 0.99072265625, 0.513671875, 0.31396484375, 1.0, 0.91259765625, 0.970703125, 0.2353515625, 0.39990234375, 0.03515625, 0.95703125, 0.95751953125, 0.85888671875, 0.9765625, 0.01220703125, 0.99072265625, 0.99462890625, 0.99365234375, 0.3681640625, 0.88330078125, 0.966796875, 0.87060546875, 0.9921875, 0.03271484375, 0.93896484375, 0.982421875, 1.0, 0.85888671875, 0.28955078125, 0.13134765625, 0.0966796875, 0.94580078125, 0.38330078125, 0.26416015625, 0.96875, 0.75341796875, 0.10107421875, 0.76904296875, 0.99853515625, 0.93994140625, 0.75048828125, 0.04833984375, 0.458984375, 0.06884765625, 0.43212890625, 0.00244140625, 0.9892578125, 0.9970703125, 0.232421875, 0.36279296875, 0.966796875, 0.93603515625, 0.9365234375, 0.24365234375, 0.28466796875, 0.9736328125, 0.00732421875, 0.845703125, 0.392578125, 0.7958984375, 0.9833984375, 0.0322265625, 0.96142578125, 0.50390625, 0.98974609375, 0.67236328125, 0.99072265625, 0.69287109375, 0.56591796875, 0.13037109375, 0.04150390625, 1.0, 0.7294921875, 0.97705078125, 0.9287109375, 0.8427734375, 0.6259765625, 0.9853515625, 0.990234375, 0.861328125, 0.515625, 0.91064453125, 0.82421875, 0.95361328125, 0.9892578125, 0.99609375, 0.9931640625, 0.0087890625, 0.99951171875, 0.8671875, 0.96728515625, 0.9912109375, 0.9990234375, 0.01416015625, 0.9921875, 0.474609375, 0.9521484375, 0.8974609375, 0.9931640625, 0.021484375, 0.865234375, 0.00439453125, 0.01953125, 1.0, 0.15625, 0.98828125, 0.83203125, 0.12353515625, 0.982421875, 0.1181640625, 0.57373046875, 0.9990234375, 0.16162109375, 0.994140625, 0.99951171875, 0.99169921875, 0.642578125, 0.1376953125, 0.998046875, 0.96337890625]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [00:34<00:02,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00439453125, 0.0009765625, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.00048828125, 0.00439453125, 0.0048828125, 0.00048828125, 0.009765625, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.455078125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.0146484375, 0.00146484375, 0.00146484375, 0.0, 0.0, 0.0009765625, 0.0078125, 0.0, 0.0107421875, 0.0009765625, 0.60400390625, 0.0, 0.00390625, 0.013671875, 0.0, 0.0, 0.0, 0.310546875, 0.017578125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09716796875, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.03955078125, 0.0, 0.0, 0.01025390625, 0.0, 0.0068359375, 0.00341796875, 0.06201171875, 0.0, 0.01513671875, 0.0, 0.01171875, 0.30419921875, 0.00244140625, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0146484375, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.00048828125, 0.0029296875, 0.0048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.02294921875, 0.0029296875, 0.1982421875, 0.0009765625, 0.06396484375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.06884765625, 0.00048828125, 0.0185546875, 0.0009765625, 0.0, 0.0, 0.001953125, 0.0, 0.00244140625, 0.0, 0.0, 0.55810546875, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.03466796875, 0.001953125, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0078125, 0.9794921875, 0.00439453125, 0.0, 0.00048828125, 0.0341796875, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0283203125, 0.0, 0.00146484375, 0.0009765625, 0.0087890625, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.00634765625, 0.00439453125, 0.0009765625, 0.0, 0.0, 0.00390625, 0.0, 0.0029296875, 0.00048828125, 0.0, 0.02783203125, 0.0, 0.0009765625, 0.0, 0.0, 0.033203125, 0.0, 0.0009765625, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.06298828125, 0.08447265625, 0.0, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.0498046875, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0634765625, 0.00244140625, 0.0, 0.0009765625, 0.0029296875, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.01220703125, 0.0, 0.03076171875, 0.01416015625, 0.00732421875, 0.00048828125, 0.01318359375, 0.0, 0.04296875, 0.0, 0.0, 0.0, 0.12109375, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.0009765625, 0.0029296875, 0.041015625, 0.0, 0.0, 0.00341796875, 0.1669921875, 0.00048828125, 0.00146484375, 0.0, 0.00048828125, 0.0, 0.00244140625, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.142578125, 0.0, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.0009765625, 0.06884765625, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.0, 0.01318359375, 0.01513671875, 0.01904296875, 0.0009765625, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0009765625, 0.00048828125, 0.00048828125, 0.0009765625, 0.00390625, 0.00244140625, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.02294921875, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.00830078125, 0.00048828125, 0.0, 0.0, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.26416015625, 0.28955078125, 0.0, 0.0, 0.0, 0.0732421875, 0.0009765625, 0.0009765625, 0.0009765625, 0.12939453125, 0.001953125, 0.00341796875, 0.0, 0.00048828125, 0.00048828125, 0.083984375, 0.0, 0.0, 0.03125, 0.0029296875, 0.0, 0.0029296875, 0.00537109375, 0.0, 0.00927734375, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.30224609375, 0.0, 0.0, 0.00927734375, 0.0, 0.00048828125, 0.11767578125, 0.0, 0.0, 0.0029296875, 0.00244140625, 0.00341796875, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0185546875, 0.0, 0.00048828125, 0.0009765625, 0.0, 0.00048828125, 0.06689453125, 0.00048828125, 0.00537109375, 0.0048828125, 0.0009765625, 0.0029296875, 0.00048828125, 0.00244140625, 0.0, 0.0634765625, 0.0, 0.00048828125, 0.005859375, 0.0, 0.00732421875, 0.0078125, 0.00341796875, 0.0, 0.0, 0.0087890625, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.0, 0.013671875, 0.02392578125, 0.0, 0.00537109375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.00830078125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.0, 0.00341796875, 0.00244140625, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.01806640625, 0.0, 0.0, 0.0029296875, 0.0048828125, 0.01220703125, 0.0, 0.00244140625, 0.0, 0.05712890625, 0.0, 0.0087890625, 0.00048828125, 0.0263671875, 0.03369140625, 0.1337890625, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0009765625, 0.0205078125, 0.0, 0.0087890625, 0.02587890625, 0.00146484375, 0.1943359375, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0087890625, 0.0, 0.0, 0.33447265625, 0.0078125, 0.0, 0.25927734375, 0.0, 0.0, 0.0, 0.00048828125, 0.0048828125, 0.0458984375, 0.0068359375, 0.0, 0.00537109375, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0322265625, 0.0, 0.0, 0.0, 0.357421875, 0.0, 0.01318359375, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.04541015625, 0.0, 0.0, 0.00146484375, 0.00244140625, 0.00927734375, 0.0009765625, 0.00244140625, 0.0, 0.037109375, 0.09619140625, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.00830078125, 0.00341796875, 0.0, 0.0, 0.03955078125, 0.00439453125, 0.0, 0.00244140625, 0.00439453125, 0.03271484375, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00341796875, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.001953125, 0.0, 0.08349609375, 0.0, 0.0, 0.0, 0.01318359375, 0.07763671875, 0.001953125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.06640625, 0.0, 0.09423828125, 0.0126953125, 0.00048828125, 0.00048828125, 0.0244140625, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0029296875, 0.00146484375, 0.00048828125, 0.0, 0.0, 0.0, 0.0009765625, 0.001953125, 0.01806640625, 0.00390625, 0.01123046875, 0.49609375, 0.0, 0.0, 0.0, 0.05029296875, 0.00439453125, 0.0, 0.0, 0.00390625, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.0, 0.0068359375, 0.00048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.099609375, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.00048828125, 0.0009765625, 0.03173828125, 0.0, 0.0, 0.08203125, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00244140625, 0.00048828125, 0.0, 0.001953125, 0.1767578125, 0.00048828125, 0.0, 0.0, 0.009765625, 0.00146484375, 0.00244140625, 0.0, 0.00341796875, 0.0, 0.064453125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.00048828125, 0.001953125, 0.00244140625, 0.3720703125, 0.13232421875, 0.0, 0.55810546875, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00927734375, 0.00048828125, 0.00048828125, 0.001953125, 0.0, 0.0205078125, 0.0, 0.33642578125, 0.0, 0.005859375, 0.4814453125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.12939453125, 0.00146484375, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0009765625, 0.00048828125, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00537109375, 0.00048828125, 0.00244140625, 0.0, 0.00048828125, 0.16015625, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.04296875, 0.00048828125, 0.0, 0.4453125, 0.0, 0.0, 0.0, 0.0, 0.00830078125, 0.0283203125, 0.01318359375, 0.009765625, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.0, 0.01171875, 0.0, 0.0029296875, 0.01513671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.025390625, 0.00146484375, 0.00048828125, 0.00048828125, 0.0, 0.00439453125, 0.0, 0.00146484375, 0.0078125, 0.0029296875, 0.00244140625, 0.00244140625, 0.04296875, 0.533203125, 0.46875, 0.0, 0.01171875, 0.0, 0.0, 0.001953125, 0.0029296875, 0.0, 0.00146484375, 0.0, 0.00146484375, 0.00146484375, 0.0, 0.00048828125, 0.00146484375, 0.00341796875, 0.0, 0.00048828125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.0, 0.0, 0.0390625, 0.0, 0.00634765625, 0.0, 0.0, 0.00244140625, 0.0, 0.0, 0.03466796875, 0.0, 0.00537109375, 0.0, 0.0029296875, 0.2392578125, 0.0, 0.0546875, 0.0, 0.0, 0.0, 0.36279296875, 0.0, 0.00048828125, 0.0029296875, 0.00244140625, 0.005859375, 0.0, 0.0, 0.00146484375, 0.00390625, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00048828125, 0.0, 0.01513671875, 0.001953125, 0.00048828125, 0.00048828125, 0.00341796875, 0.05712890625, 0.0, 0.0, 0.0, 0.06640625, 0.0, 0.00537109375, 0.0, 0.0, 0.0400390625, 0.095703125, 0.00048828125, 0.0, 0.0, 0.0, 0.06494140625, 0.0, 0.0009765625, 0.01611328125, 0.0009765625, 0.01318359375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00146484375, 0.0, 0.001953125, 0.00146484375, 0.00146484375, 0.0029296875, 0.01318359375, 0.00341796875, 0.0, 0.0, 0.0, 0.0009765625, 0.00146484375, 0.0185546875, 0.00146484375, 0.0029296875, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.01025390625, 0.0, 0.27490234375, 0.00537109375, 0.08251953125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00634765625, 0.0029296875, 0.00048828125, 0.0, 0.0, 0.0302734375, 0.00048828125, 0.0, 0.00048828125, 0.21923828125, 0.0, 0.001953125, 0.0068359375, 0.0, 0.001953125, 0.0341796875, 0.00390625, 0.0, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.00244140625, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0009765625, 0.87255859375, 0.05419921875, 0.0, 0.0, 0.0654296875, 0.00048828125, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00146484375, 0.0771484375, 0.00927734375, 0.00146484375, 0.0, 0.0, 0.0, 0.68603515625, 0.00048828125, 0.00244140625, 0.00048828125, 0.0048828125, 0.0009765625, 0.0087890625, 0.0, 0.00048828125, 0.27490234375, 0.0, 0.0, 0.1357421875, 0.00830078125, 0.0009765625, 0.00390625, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.02001953125, 0.0, 0.00048828125, 0.00537109375, 0.00048828125, 0.0, 0.0, 0.00439453125, 0.00244140625, 0.0, 0.0, 0.01318359375, 0.0, 0.01318359375, 0.00341796875, 0.8896484375, 0.796875, 0.19580078125, 0.1826171875, 0.9248046875, 0.37939453125, 0.99658203125, 0.99951171875, 0.99560546875, 0.9892578125, 0.9990234375, 0.92236328125, 0.951171875, 0.74462890625, 0.1318359375, 0.75341796875, 0.18603515625, 0.97802734375, 0.0107421875, 0.2421875, 0.93310546875, 0.16357421875, 0.95654296875, 0.072265625, 0.18017578125, 0.03271484375, 0.99951171875, 0.65673828125, 0.9931640625, 0.14599609375, 0.99755859375, 0.50390625, 0.00146484375, 0.662109375, 1.0, 0.91796875, 0.203125, 0.94580078125, 0.92578125, 0.96044921875, 0.25390625, 0.27197265625, 0.99853515625, 0.3564453125, 0.01318359375, 0.89111328125, 0.9736328125, 0.6943359375, 0.546875, 1.0, 0.501953125, 0.89990234375, 0.7705078125, 0.99853515625, 0.767578125, 0.0009765625, 0.0224609375, 0.880859375, 0.69921875, 0.86865234375, 0.99755859375, 0.9951171875, 0.994140625, 0.60595703125, 0.0302734375, 0.86328125, 0.8154296875, 0.92919921875, 0.88623046875, 0.291015625, 0.83544921875, 0.51171875, 0.7607421875, 0.638671875, 0.99267578125, 0.71875, 0.86474609375, 0.908203125, 0.97216796875, 0.7646484375, 0.99072265625, 0.35205078125, 0.9990234375, 0.33349609375, 1.0, 0.7880859375, 0.0, 0.0087890625, 0.9541015625, 0.42822265625, 0.341796875, 0.998046875, 0.02587890625, 0.8232421875, 0.93505859375, 0.11669921875, 0.9990234375, 0.189453125, 0.9326171875, 0.00048828125, 0.96826171875, 0.763671875, 0.4111328125, 0.9619140625, 0.12744140625, 0.93505859375, 0.7958984375, 0.89697265625, 0.06591796875, 0.9990234375, 0.75048828125, 0.96923828125, 0.4111328125, 0.5908203125, 0.509765625, 0.56005859375, 0.94384765625, 0.90380859375, 0.99658203125, 0.93017578125, 0.21337890625, 0.189453125, 0.00439453125, 0.96728515625, 0.884765625, 0.05419921875, 0.43212890625, 0.9765625, 0.04833984375, 0.89404296875, 0.82666015625, 0.7548828125, 0.0888671875, 0.00341796875, 0.99951171875, 0.98828125, 0.99853515625, 0.0341796875, 0.98193359375, 0.79052734375, 0.51171875, 0.9990234375, 0.89306640625, 0.986328125, 0.88427734375, 0.98828125, 0.951171875, 0.806640625, 0.505859375, 0.09814453125, 0.388671875, 0.9990234375, 0.96728515625, 0.99951171875, 0.79052734375, 0.9990234375, 0.998046875, 1.0, 0.99853515625, 0.9970703125, 0.9541015625, 0.0888671875, 0.001953125, 0.8671875, 0.9541015625, 0.99853515625, 0.99951171875, 0.98779296875, 0.111328125, 0.72802734375, 0.0849609375, 0.9345703125, 0.93603515625, 0.97021484375, 0.009765625, 0.00048828125, 0.97900390625, 0.9951171875, 0.01904296875, 0.74169921875, 0.9677734375, 0.97509765625, 0.392578125, 0.77734375, 0.91162109375, 0.9775390625, 1.0, 0.9775390625, 1.0, 0.77197265625, 0.99072265625, 0.17236328125, 0.93212890625, 0.1240234375, 0.533203125, 0.955078125, 0.91943359375, 0.451171875, 0.3056640625, 0.0, 0.998046875, 0.99462890625, 0.99462890625, 0.98828125, 0.51171875, 0.9091796875, 0.9990234375, 0.923828125, 0.2392578125, 0.8154296875, 0.63330078125, 0.974609375, 0.525390625, 0.9189453125, 0.99853515625, 0.01611328125, 0.9814453125, 0.99072265625, 0.00390625, 0.14892578125, 0.95947265625, 0.015625, 0.95361328125, 0.14111328125, 0.43798828125, 0.640625, 0.9970703125, 0.93310546875, 0.998046875, 0.98876953125, 0.93994140625, 0.98046875, 0.01025390625, 0.9833984375, 0.93896484375, 0.91259765625, 1.0, 0.845703125, 0.99951171875, 0.7265625, 0.99951171875, 0.85986328125, 0.0009765625, 0.9794921875, 0.3740234375, 0.2265625, 0.98388671875, 0.99560546875, 0.60009765625, 0.9912109375, 0.0380859375, 0.00048828125, 0.8916015625, 1.0, 0.0849609375, 0.556640625, 1.0, 0.08447265625, 1.0, 1.0, 0.92724609375, 0.94580078125, 0.1376953125, 0.0615234375, 0.498046875, 0.29443359375, 0.921875, 0.986328125, 0.9638671875, 0.451171875, 0.9375, 0.02099609375, 0.083984375, 0.87890625, 1.0, 0.96826171875, 0.95751953125, 0.65478515625, 0.80078125, 0.60205078125, 0.99169921875, 0.73583984375, 0.99951171875, 0.9736328125, 0.99267578125, 0.1259765625, 0.3486328125, 0.99755859375, 0.10888671875, 0.17333984375, 0.1044921875, 0.81884765625, 0.001953125, 0.2255859375, 1.0, 0.85107421875, 0.99658203125, 0.85595703125, 0.0947265625, 0.9638671875, 0.9638671875, 0.98388671875, 0.9970703125, 0.57958984375, 0.02587890625, 0.7744140625, 0.80419921875, 0.8642578125, 1.0, 0.99658203125, 0.974609375, 0.0322265625, 0.6689453125, 0.97998046875, 0.505859375, 0.9921875, 0.96875, 0.12744140625, 0.716796875, 0.00439453125, 0.845703125, 0.998046875, 0.93994140625, 0.93701171875, 0.94970703125, 0.0966796875, 0.91259765625, 0.04345703125, 0.99951171875, 0.57763671875, 0.77880859375, 1.0, 0.05322265625, 0.9111328125, 0.9970703125, 0.86083984375, 0.99462890625, 0.1611328125, 0.56005859375, 0.1572265625, 0.57958984375, 0.9873046875, 0.99951171875, 0.537109375, 0.99853515625, 0.9970703125, 0.9990234375, 0.99951171875, 0.5849609375, 0.8017578125, 0.97216796875, 0.83349609375, 0.98095703125, 0.0537109375, 0.8173828125, 0.990234375, 0.05810546875, 0.9267578125, 0.09423828125, 0.54296875, 1.0, 0.05810546875, 0.73876953125, 1.0, 0.9794921875, 0.0751953125, 0.00146484375, 0.0927734375, 0.7216796875, 0.00732421875, 0.00927734375, 0.91552734375, 0.98681640625, 0.9833984375, 0.5390625, 0.9970703125, 0.83642578125, 0.9072265625, 0.9970703125, 0.00732421875, 0.390625, 0.99072265625, 0.513671875, 0.31396484375, 1.0, 0.91259765625, 0.970703125, 0.2353515625, 0.39990234375, 0.03515625, 0.95703125, 0.95751953125, 0.85888671875, 0.9765625, 0.01220703125, 0.99072265625, 0.99462890625, 0.99365234375, 0.3681640625, 0.88330078125, 0.966796875, 0.87060546875, 0.9921875, 0.03271484375, 0.93896484375, 0.982421875, 1.0, 0.85888671875, 0.28955078125, 0.13134765625, 0.0966796875, 0.94580078125, 0.38330078125, 0.26416015625, 0.96875, 0.75341796875, 0.10107421875, 0.76904296875, 0.99853515625, 0.93994140625, 0.75048828125, 0.04833984375, 0.458984375, 0.06884765625, 0.43212890625, 0.00244140625, 0.9892578125, 0.9970703125, 0.232421875, 0.36279296875, 0.966796875, 0.93603515625, 0.9365234375, 0.24365234375, 0.28466796875, 0.9736328125, 0.00732421875, 0.845703125, 0.392578125, 0.7958984375, 0.9833984375, 0.0322265625, 0.96142578125, 0.50390625, 0.98974609375, 0.67236328125, 0.99072265625, 0.69287109375, 0.56591796875, 0.13037109375, 0.04150390625, 1.0, 0.7294921875, 0.97705078125, 0.9287109375, 0.8427734375, 0.6259765625, 0.9853515625, 0.990234375, 0.861328125, 0.515625, 0.91064453125, 0.82421875, 0.95361328125, 0.9892578125, 0.99609375, 0.9931640625, 0.0087890625, 0.99951171875, 0.8671875, 0.96728515625, 0.9912109375, 0.9990234375, 0.01416015625, 0.9921875, 0.474609375, 0.9521484375, 0.8974609375, 0.9931640625, 0.021484375, 0.865234375, 0.00439453125, 0.01953125, 1.0, 0.15625, 0.98828125, 0.83203125, 0.12353515625, 0.982421875, 0.1181640625, 0.57373046875, 0.9990234375, 0.16162109375, 0.994140625, 0.99951171875, 0.99169921875, 0.642578125, 0.1376953125, 0.998046875, 0.96337890625, 0.931640625, 0.9990234375, 0.03173828125, 0.98486328125, 0.99462890625, 0.998046875, 0.9873046875, 0.001953125, 0.43603515625, 0.76220703125, 0.56982421875, 0.61669921875, 0.99951171875, 0.97412109375, 0.39697265625, 0.69580078125, 0.87255859375, 0.13671875, 0.865234375, 0.841796875, 0.0400390625, 0.064453125, 0.015625, 0.998046875, 0.4052734375, 0.98828125, 0.4921875, 0.99853515625, 0.9677734375, 0.78271484375, 0.9326171875, 0.9990234375, 0.43212890625, 0.986328125, 0.9716796875, 0.455078125, 0.267578125, 0.99365234375, 1.0, 0.22021484375, 0.12158203125, 0.58154296875, 0.810546875, 0.73388671875, 0.01611328125, 0.99658203125, 0.01904296875, 0.9482421875, 0.01708984375, 0.958984375, 0.45703125, 0.70068359375, 0.99951171875, 0.2958984375, 1.0, 0.99853515625, 0.998046875, 0.9931640625, 0.953125, 0.958984375, 0.98974609375, 0.291015625, 0.501953125, 0.94873046875, 0.71533203125, 0.7744140625, 0.97900390625, 0.96337890625, 0.49609375, 0.072265625, 0.53125, 0.9609375, 0.5869140625, 0.96923828125, 1.0, 0.013671875, 0.865234375, 0.021484375, 0.9111328125, 0.947265625, 0.9921875, 0.0244140625, 0.796875, 0.91650390625, 1.0, 1.0, 1.0, 0.99951171875, 0.5927734375, 0.99951171875, 0.82763671875, 0.99755859375, 0.99462890625, 0.986328125, 0.8447265625, 0.99951171875, 1.0, 0.9248046875, 1.0, 0.04443359375]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [00:35<00:01,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00439453125, 0.0009765625, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.00048828125, 0.00439453125, 0.0048828125, 0.00048828125, 0.009765625, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.455078125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.0146484375, 0.00146484375, 0.00146484375, 0.0, 0.0, 0.0009765625, 0.0078125, 0.0, 0.0107421875, 0.0009765625, 0.60400390625, 0.0, 0.00390625, 0.013671875, 0.0, 0.0, 0.0, 0.310546875, 0.017578125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09716796875, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.03955078125, 0.0, 0.0, 0.01025390625, 0.0, 0.0068359375, 0.00341796875, 0.06201171875, 0.0, 0.01513671875, 0.0, 0.01171875, 0.30419921875, 0.00244140625, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0146484375, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.00048828125, 0.0029296875, 0.0048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.02294921875, 0.0029296875, 0.1982421875, 0.0009765625, 0.06396484375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.06884765625, 0.00048828125, 0.0185546875, 0.0009765625, 0.0, 0.0, 0.001953125, 0.0, 0.00244140625, 0.0, 0.0, 0.55810546875, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.03466796875, 0.001953125, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0078125, 0.9794921875, 0.00439453125, 0.0, 0.00048828125, 0.0341796875, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0283203125, 0.0, 0.00146484375, 0.0009765625, 0.0087890625, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.00634765625, 0.00439453125, 0.0009765625, 0.0, 0.0, 0.00390625, 0.0, 0.0029296875, 0.00048828125, 0.0, 0.02783203125, 0.0, 0.0009765625, 0.0, 0.0, 0.033203125, 0.0, 0.0009765625, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.06298828125, 0.08447265625, 0.0, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.0498046875, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0634765625, 0.00244140625, 0.0, 0.0009765625, 0.0029296875, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.01220703125, 0.0, 0.03076171875, 0.01416015625, 0.00732421875, 0.00048828125, 0.01318359375, 0.0, 0.04296875, 0.0, 0.0, 0.0, 0.12109375, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.0009765625, 0.0029296875, 0.041015625, 0.0, 0.0, 0.00341796875, 0.1669921875, 0.00048828125, 0.00146484375, 0.0, 0.00048828125, 0.0, 0.00244140625, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.142578125, 0.0, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.0009765625, 0.06884765625, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.0, 0.01318359375, 0.01513671875, 0.01904296875, 0.0009765625, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0009765625, 0.00048828125, 0.00048828125, 0.0009765625, 0.00390625, 0.00244140625, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.02294921875, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.00830078125, 0.00048828125, 0.0, 0.0, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.26416015625, 0.28955078125, 0.0, 0.0, 0.0, 0.0732421875, 0.0009765625, 0.0009765625, 0.0009765625, 0.12939453125, 0.001953125, 0.00341796875, 0.0, 0.00048828125, 0.00048828125, 0.083984375, 0.0, 0.0, 0.03125, 0.0029296875, 0.0, 0.0029296875, 0.00537109375, 0.0, 0.00927734375, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.30224609375, 0.0, 0.0, 0.00927734375, 0.0, 0.00048828125, 0.11767578125, 0.0, 0.0, 0.0029296875, 0.00244140625, 0.00341796875, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0185546875, 0.0, 0.00048828125, 0.0009765625, 0.0, 0.00048828125, 0.06689453125, 0.00048828125, 0.00537109375, 0.0048828125, 0.0009765625, 0.0029296875, 0.00048828125, 0.00244140625, 0.0, 0.0634765625, 0.0, 0.00048828125, 0.005859375, 0.0, 0.00732421875, 0.0078125, 0.00341796875, 0.0, 0.0, 0.0087890625, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.0, 0.013671875, 0.02392578125, 0.0, 0.00537109375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.00830078125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.0, 0.00341796875, 0.00244140625, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.01806640625, 0.0, 0.0, 0.0029296875, 0.0048828125, 0.01220703125, 0.0, 0.00244140625, 0.0, 0.05712890625, 0.0, 0.0087890625, 0.00048828125, 0.0263671875, 0.03369140625, 0.1337890625, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0009765625, 0.0205078125, 0.0, 0.0087890625, 0.02587890625, 0.00146484375, 0.1943359375, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0087890625, 0.0, 0.0, 0.33447265625, 0.0078125, 0.0, 0.25927734375, 0.0, 0.0, 0.0, 0.00048828125, 0.0048828125, 0.0458984375, 0.0068359375, 0.0, 0.00537109375, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0322265625, 0.0, 0.0, 0.0, 0.357421875, 0.0, 0.01318359375, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.04541015625, 0.0, 0.0, 0.00146484375, 0.00244140625, 0.00927734375, 0.0009765625, 0.00244140625, 0.0, 0.037109375, 0.09619140625, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.00830078125, 0.00341796875, 0.0, 0.0, 0.03955078125, 0.00439453125, 0.0, 0.00244140625, 0.00439453125, 0.03271484375, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00341796875, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.001953125, 0.0, 0.08349609375, 0.0, 0.0, 0.0, 0.01318359375, 0.07763671875, 0.001953125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.06640625, 0.0, 0.09423828125, 0.0126953125, 0.00048828125, 0.00048828125, 0.0244140625, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0029296875, 0.00146484375, 0.00048828125, 0.0, 0.0, 0.0, 0.0009765625, 0.001953125, 0.01806640625, 0.00390625, 0.01123046875, 0.49609375, 0.0, 0.0, 0.0, 0.05029296875, 0.00439453125, 0.0, 0.0, 0.00390625, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.0, 0.0068359375, 0.00048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.099609375, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.00048828125, 0.0009765625, 0.03173828125, 0.0, 0.0, 0.08203125, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00244140625, 0.00048828125, 0.0, 0.001953125, 0.1767578125, 0.00048828125, 0.0, 0.0, 0.009765625, 0.00146484375, 0.00244140625, 0.0, 0.00341796875, 0.0, 0.064453125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.00048828125, 0.001953125, 0.00244140625, 0.3720703125, 0.13232421875, 0.0, 0.55810546875, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00927734375, 0.00048828125, 0.00048828125, 0.001953125, 0.0, 0.0205078125, 0.0, 0.33642578125, 0.0, 0.005859375, 0.4814453125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.12939453125, 0.00146484375, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0009765625, 0.00048828125, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00537109375, 0.00048828125, 0.00244140625, 0.0, 0.00048828125, 0.16015625, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.04296875, 0.00048828125, 0.0, 0.4453125, 0.0, 0.0, 0.0, 0.0, 0.00830078125, 0.0283203125, 0.01318359375, 0.009765625, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.0, 0.01171875, 0.0, 0.0029296875, 0.01513671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.025390625, 0.00146484375, 0.00048828125, 0.00048828125, 0.0, 0.00439453125, 0.0, 0.00146484375, 0.0078125, 0.0029296875, 0.00244140625, 0.00244140625, 0.04296875, 0.533203125, 0.46875, 0.0, 0.01171875, 0.0, 0.0, 0.001953125, 0.0029296875, 0.0, 0.00146484375, 0.0, 0.00146484375, 0.00146484375, 0.0, 0.00048828125, 0.00146484375, 0.00341796875, 0.0, 0.00048828125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.0, 0.0, 0.0390625, 0.0, 0.00634765625, 0.0, 0.0, 0.00244140625, 0.0, 0.0, 0.03466796875, 0.0, 0.00537109375, 0.0, 0.0029296875, 0.2392578125, 0.0, 0.0546875, 0.0, 0.0, 0.0, 0.36279296875, 0.0, 0.00048828125, 0.0029296875, 0.00244140625, 0.005859375, 0.0, 0.0, 0.00146484375, 0.00390625, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00048828125, 0.0, 0.01513671875, 0.001953125, 0.00048828125, 0.00048828125, 0.00341796875, 0.05712890625, 0.0, 0.0, 0.0, 0.06640625, 0.0, 0.00537109375, 0.0, 0.0, 0.0400390625, 0.095703125, 0.00048828125, 0.0, 0.0, 0.0, 0.06494140625, 0.0, 0.0009765625, 0.01611328125, 0.0009765625, 0.01318359375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00146484375, 0.0, 0.001953125, 0.00146484375, 0.00146484375, 0.0029296875, 0.01318359375, 0.00341796875, 0.0, 0.0, 0.0, 0.0009765625, 0.00146484375, 0.0185546875, 0.00146484375, 0.0029296875, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.01025390625, 0.0, 0.27490234375, 0.00537109375, 0.08251953125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00634765625, 0.0029296875, 0.00048828125, 0.0, 0.0, 0.0302734375, 0.00048828125, 0.0, 0.00048828125, 0.21923828125, 0.0, 0.001953125, 0.0068359375, 0.0, 0.001953125, 0.0341796875, 0.00390625, 0.0, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.00244140625, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0009765625, 0.87255859375, 0.05419921875, 0.0, 0.0, 0.0654296875, 0.00048828125, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00146484375, 0.0771484375, 0.00927734375, 0.00146484375, 0.0, 0.0, 0.0, 0.68603515625, 0.00048828125, 0.00244140625, 0.00048828125, 0.0048828125, 0.0009765625, 0.0087890625, 0.0, 0.00048828125, 0.27490234375, 0.0, 0.0, 0.1357421875, 0.00830078125, 0.0009765625, 0.00390625, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.02001953125, 0.0, 0.00048828125, 0.00537109375, 0.00048828125, 0.0, 0.0, 0.00439453125, 0.00244140625, 0.0, 0.0, 0.01318359375, 0.0, 0.01318359375, 0.00341796875, 0.8896484375, 0.796875, 0.19580078125, 0.1826171875, 0.9248046875, 0.37939453125, 0.99658203125, 0.99951171875, 0.99560546875, 0.9892578125, 0.9990234375, 0.92236328125, 0.951171875, 0.74462890625, 0.1318359375, 0.75341796875, 0.18603515625, 0.97802734375, 0.0107421875, 0.2421875, 0.93310546875, 0.16357421875, 0.95654296875, 0.072265625, 0.18017578125, 0.03271484375, 0.99951171875, 0.65673828125, 0.9931640625, 0.14599609375, 0.99755859375, 0.50390625, 0.00146484375, 0.662109375, 1.0, 0.91796875, 0.203125, 0.94580078125, 0.92578125, 0.96044921875, 0.25390625, 0.27197265625, 0.99853515625, 0.3564453125, 0.01318359375, 0.89111328125, 0.9736328125, 0.6943359375, 0.546875, 1.0, 0.501953125, 0.89990234375, 0.7705078125, 0.99853515625, 0.767578125, 0.0009765625, 0.0224609375, 0.880859375, 0.69921875, 0.86865234375, 0.99755859375, 0.9951171875, 0.994140625, 0.60595703125, 0.0302734375, 0.86328125, 0.8154296875, 0.92919921875, 0.88623046875, 0.291015625, 0.83544921875, 0.51171875, 0.7607421875, 0.638671875, 0.99267578125, 0.71875, 0.86474609375, 0.908203125, 0.97216796875, 0.7646484375, 0.99072265625, 0.35205078125, 0.9990234375, 0.33349609375, 1.0, 0.7880859375, 0.0, 0.0087890625, 0.9541015625, 0.42822265625, 0.341796875, 0.998046875, 0.02587890625, 0.8232421875, 0.93505859375, 0.11669921875, 0.9990234375, 0.189453125, 0.9326171875, 0.00048828125, 0.96826171875, 0.763671875, 0.4111328125, 0.9619140625, 0.12744140625, 0.93505859375, 0.7958984375, 0.89697265625, 0.06591796875, 0.9990234375, 0.75048828125, 0.96923828125, 0.4111328125, 0.5908203125, 0.509765625, 0.56005859375, 0.94384765625, 0.90380859375, 0.99658203125, 0.93017578125, 0.21337890625, 0.189453125, 0.00439453125, 0.96728515625, 0.884765625, 0.05419921875, 0.43212890625, 0.9765625, 0.04833984375, 0.89404296875, 0.82666015625, 0.7548828125, 0.0888671875, 0.00341796875, 0.99951171875, 0.98828125, 0.99853515625, 0.0341796875, 0.98193359375, 0.79052734375, 0.51171875, 0.9990234375, 0.89306640625, 0.986328125, 0.88427734375, 0.98828125, 0.951171875, 0.806640625, 0.505859375, 0.09814453125, 0.388671875, 0.9990234375, 0.96728515625, 0.99951171875, 0.79052734375, 0.9990234375, 0.998046875, 1.0, 0.99853515625, 0.9970703125, 0.9541015625, 0.0888671875, 0.001953125, 0.8671875, 0.9541015625, 0.99853515625, 0.99951171875, 0.98779296875, 0.111328125, 0.72802734375, 0.0849609375, 0.9345703125, 0.93603515625, 0.97021484375, 0.009765625, 0.00048828125, 0.97900390625, 0.9951171875, 0.01904296875, 0.74169921875, 0.9677734375, 0.97509765625, 0.392578125, 0.77734375, 0.91162109375, 0.9775390625, 1.0, 0.9775390625, 1.0, 0.77197265625, 0.99072265625, 0.17236328125, 0.93212890625, 0.1240234375, 0.533203125, 0.955078125, 0.91943359375, 0.451171875, 0.3056640625, 0.0, 0.998046875, 0.99462890625, 0.99462890625, 0.98828125, 0.51171875, 0.9091796875, 0.9990234375, 0.923828125, 0.2392578125, 0.8154296875, 0.63330078125, 0.974609375, 0.525390625, 0.9189453125, 0.99853515625, 0.01611328125, 0.9814453125, 0.99072265625, 0.00390625, 0.14892578125, 0.95947265625, 0.015625, 0.95361328125, 0.14111328125, 0.43798828125, 0.640625, 0.9970703125, 0.93310546875, 0.998046875, 0.98876953125, 0.93994140625, 0.98046875, 0.01025390625, 0.9833984375, 0.93896484375, 0.91259765625, 1.0, 0.845703125, 0.99951171875, 0.7265625, 0.99951171875, 0.85986328125, 0.0009765625, 0.9794921875, 0.3740234375, 0.2265625, 0.98388671875, 0.99560546875, 0.60009765625, 0.9912109375, 0.0380859375, 0.00048828125, 0.8916015625, 1.0, 0.0849609375, 0.556640625, 1.0, 0.08447265625, 1.0, 1.0, 0.92724609375, 0.94580078125, 0.1376953125, 0.0615234375, 0.498046875, 0.29443359375, 0.921875, 0.986328125, 0.9638671875, 0.451171875, 0.9375, 0.02099609375, 0.083984375, 0.87890625, 1.0, 0.96826171875, 0.95751953125, 0.65478515625, 0.80078125, 0.60205078125, 0.99169921875, 0.73583984375, 0.99951171875, 0.9736328125, 0.99267578125, 0.1259765625, 0.3486328125, 0.99755859375, 0.10888671875, 0.17333984375, 0.1044921875, 0.81884765625, 0.001953125, 0.2255859375, 1.0, 0.85107421875, 0.99658203125, 0.85595703125, 0.0947265625, 0.9638671875, 0.9638671875, 0.98388671875, 0.9970703125, 0.57958984375, 0.02587890625, 0.7744140625, 0.80419921875, 0.8642578125, 1.0, 0.99658203125, 0.974609375, 0.0322265625, 0.6689453125, 0.97998046875, 0.505859375, 0.9921875, 0.96875, 0.12744140625, 0.716796875, 0.00439453125, 0.845703125, 0.998046875, 0.93994140625, 0.93701171875, 0.94970703125, 0.0966796875, 0.91259765625, 0.04345703125, 0.99951171875, 0.57763671875, 0.77880859375, 1.0, 0.05322265625, 0.9111328125, 0.9970703125, 0.86083984375, 0.99462890625, 0.1611328125, 0.56005859375, 0.1572265625, 0.57958984375, 0.9873046875, 0.99951171875, 0.537109375, 0.99853515625, 0.9970703125, 0.9990234375, 0.99951171875, 0.5849609375, 0.8017578125, 0.97216796875, 0.83349609375, 0.98095703125, 0.0537109375, 0.8173828125, 0.990234375, 0.05810546875, 0.9267578125, 0.09423828125, 0.54296875, 1.0, 0.05810546875, 0.73876953125, 1.0, 0.9794921875, 0.0751953125, 0.00146484375, 0.0927734375, 0.7216796875, 0.00732421875, 0.00927734375, 0.91552734375, 0.98681640625, 0.9833984375, 0.5390625, 0.9970703125, 0.83642578125, 0.9072265625, 0.9970703125, 0.00732421875, 0.390625, 0.99072265625, 0.513671875, 0.31396484375, 1.0, 0.91259765625, 0.970703125, 0.2353515625, 0.39990234375, 0.03515625, 0.95703125, 0.95751953125, 0.85888671875, 0.9765625, 0.01220703125, 0.99072265625, 0.99462890625, 0.99365234375, 0.3681640625, 0.88330078125, 0.966796875, 0.87060546875, 0.9921875, 0.03271484375, 0.93896484375, 0.982421875, 1.0, 0.85888671875, 0.28955078125, 0.13134765625, 0.0966796875, 0.94580078125, 0.38330078125, 0.26416015625, 0.96875, 0.75341796875, 0.10107421875, 0.76904296875, 0.99853515625, 0.93994140625, 0.75048828125, 0.04833984375, 0.458984375, 0.06884765625, 0.43212890625, 0.00244140625, 0.9892578125, 0.9970703125, 0.232421875, 0.36279296875, 0.966796875, 0.93603515625, 0.9365234375, 0.24365234375, 0.28466796875, 0.9736328125, 0.00732421875, 0.845703125, 0.392578125, 0.7958984375, 0.9833984375, 0.0322265625, 0.96142578125, 0.50390625, 0.98974609375, 0.67236328125, 0.99072265625, 0.69287109375, 0.56591796875, 0.13037109375, 0.04150390625, 1.0, 0.7294921875, 0.97705078125, 0.9287109375, 0.8427734375, 0.6259765625, 0.9853515625, 0.990234375, 0.861328125, 0.515625, 0.91064453125, 0.82421875, 0.95361328125, 0.9892578125, 0.99609375, 0.9931640625, 0.0087890625, 0.99951171875, 0.8671875, 0.96728515625, 0.9912109375, 0.9990234375, 0.01416015625, 0.9921875, 0.474609375, 0.9521484375, 0.8974609375, 0.9931640625, 0.021484375, 0.865234375, 0.00439453125, 0.01953125, 1.0, 0.15625, 0.98828125, 0.83203125, 0.12353515625, 0.982421875, 0.1181640625, 0.57373046875, 0.9990234375, 0.16162109375, 0.994140625, 0.99951171875, 0.99169921875, 0.642578125, 0.1376953125, 0.998046875, 0.96337890625, 0.931640625, 0.9990234375, 0.03173828125, 0.98486328125, 0.99462890625, 0.998046875, 0.9873046875, 0.001953125, 0.43603515625, 0.76220703125, 0.56982421875, 0.61669921875, 0.99951171875, 0.97412109375, 0.39697265625, 0.69580078125, 0.87255859375, 0.13671875, 0.865234375, 0.841796875, 0.0400390625, 0.064453125, 0.015625, 0.998046875, 0.4052734375, 0.98828125, 0.4921875, 0.99853515625, 0.9677734375, 0.78271484375, 0.9326171875, 0.9990234375, 0.43212890625, 0.986328125, 0.9716796875, 0.455078125, 0.267578125, 0.99365234375, 1.0, 0.22021484375, 0.12158203125, 0.58154296875, 0.810546875, 0.73388671875, 0.01611328125, 0.99658203125, 0.01904296875, 0.9482421875, 0.01708984375, 0.958984375, 0.45703125, 0.70068359375, 0.99951171875, 0.2958984375, 1.0, 0.99853515625, 0.998046875, 0.9931640625, 0.953125, 0.958984375, 0.98974609375, 0.291015625, 0.501953125, 0.94873046875, 0.71533203125, 0.7744140625, 0.97900390625, 0.96337890625, 0.49609375, 0.072265625, 0.53125, 0.9609375, 0.5869140625, 0.96923828125, 1.0, 0.013671875, 0.865234375, 0.021484375, 0.9111328125, 0.947265625, 0.9921875, 0.0244140625, 0.796875, 0.91650390625, 1.0, 1.0, 1.0, 0.99951171875, 0.5927734375, 0.99951171875, 0.82763671875, 0.99755859375, 0.99462890625, 0.986328125, 0.8447265625, 0.99951171875, 1.0, 0.9248046875, 1.0, 0.04443359375, 0.9873046875, 0.9970703125, 0.0009765625, 0.998046875, 0.00390625, 1.0, 0.00390625, 0.814453125, 0.7607421875, 0.92041015625, 0.9833984375, 0.85400390625, 0.9501953125, 0.994140625, 0.998046875, 0.9921875, 0.68408203125, 0.7216796875, 0.15185546875, 0.01416015625, 0.18359375, 0.97021484375, 0.505859375, 0.541015625, 0.18701171875, 0.6826171875, 0.00048828125, 1.0, 0.99365234375, 1.0, 0.99609375, 0.99609375, 0.91748046875, 0.4130859375, 0.90478515625, 0.42431640625, 0.466796875, 0.08251953125, 0.95751953125, 0.0126953125, 0.89013671875, 0.12841796875, 0.1142578125, 0.00341796875, 0.541015625, 0.8017578125, 0.541015625, 0.9951171875, 0.76904296875, 0.97314453125, 0.99609375, 1.0, 0.3330078125, 0.986328125, 0.71240234375, 1.0, 0.984375, 0.068359375, 0.90185546875, 0.9091796875, 0.998046875, 0.99853515625, 0.013671875, 0.83544921875, 0.7890625, 0.14697265625, 0.06494140625, 0.35205078125, 0.43017578125, 0.32763671875, 0.947265625, 0.00830078125, 0.36474609375, 0.203125, 0.43408203125, 0.8427734375, 1.0, 0.7705078125, 0.01904296875, 0.39990234375, 0.9384765625, 0.921875, 0.640625, 0.95166015625, 0.97265625, 0.99658203125, 0.9619140625, 0.93994140625, 0.9833984375, 0.06787109375, 0.7880859375, 0.99951171875, 1.0, 0.4072265625, 0.09716796875, 0.96728515625, 0.9404296875, 0.4150390625, 0.0048828125, 0.337890625]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [00:35<00:01,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00439453125, 0.0009765625, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.00048828125, 0.00439453125, 0.0048828125, 0.00048828125, 0.009765625, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.455078125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.0146484375, 0.00146484375, 0.00146484375, 0.0, 0.0, 0.0009765625, 0.0078125, 0.0, 0.0107421875, 0.0009765625, 0.60400390625, 0.0, 0.00390625, 0.013671875, 0.0, 0.0, 0.0, 0.310546875, 0.017578125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09716796875, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.03955078125, 0.0, 0.0, 0.01025390625, 0.0, 0.0068359375, 0.00341796875, 0.06201171875, 0.0, 0.01513671875, 0.0, 0.01171875, 0.30419921875, 0.00244140625, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0146484375, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.00048828125, 0.0029296875, 0.0048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.02294921875, 0.0029296875, 0.1982421875, 0.0009765625, 0.06396484375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.06884765625, 0.00048828125, 0.0185546875, 0.0009765625, 0.0, 0.0, 0.001953125, 0.0, 0.00244140625, 0.0, 0.0, 0.55810546875, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.03466796875, 0.001953125, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0078125, 0.9794921875, 0.00439453125, 0.0, 0.00048828125, 0.0341796875, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0283203125, 0.0, 0.00146484375, 0.0009765625, 0.0087890625, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.00634765625, 0.00439453125, 0.0009765625, 0.0, 0.0, 0.00390625, 0.0, 0.0029296875, 0.00048828125, 0.0, 0.02783203125, 0.0, 0.0009765625, 0.0, 0.0, 0.033203125, 0.0, 0.0009765625, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.06298828125, 0.08447265625, 0.0, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.0498046875, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0634765625, 0.00244140625, 0.0, 0.0009765625, 0.0029296875, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.01220703125, 0.0, 0.03076171875, 0.01416015625, 0.00732421875, 0.00048828125, 0.01318359375, 0.0, 0.04296875, 0.0, 0.0, 0.0, 0.12109375, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.0009765625, 0.0029296875, 0.041015625, 0.0, 0.0, 0.00341796875, 0.1669921875, 0.00048828125, 0.00146484375, 0.0, 0.00048828125, 0.0, 0.00244140625, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.142578125, 0.0, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.0009765625, 0.06884765625, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.0, 0.01318359375, 0.01513671875, 0.01904296875, 0.0009765625, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0009765625, 0.00048828125, 0.00048828125, 0.0009765625, 0.00390625, 0.00244140625, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.02294921875, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.00830078125, 0.00048828125, 0.0, 0.0, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.26416015625, 0.28955078125, 0.0, 0.0, 0.0, 0.0732421875, 0.0009765625, 0.0009765625, 0.0009765625, 0.12939453125, 0.001953125, 0.00341796875, 0.0, 0.00048828125, 0.00048828125, 0.083984375, 0.0, 0.0, 0.03125, 0.0029296875, 0.0, 0.0029296875, 0.00537109375, 0.0, 0.00927734375, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.30224609375, 0.0, 0.0, 0.00927734375, 0.0, 0.00048828125, 0.11767578125, 0.0, 0.0, 0.0029296875, 0.00244140625, 0.00341796875, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0185546875, 0.0, 0.00048828125, 0.0009765625, 0.0, 0.00048828125, 0.06689453125, 0.00048828125, 0.00537109375, 0.0048828125, 0.0009765625, 0.0029296875, 0.00048828125, 0.00244140625, 0.0, 0.0634765625, 0.0, 0.00048828125, 0.005859375, 0.0, 0.00732421875, 0.0078125, 0.00341796875, 0.0, 0.0, 0.0087890625, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.0, 0.013671875, 0.02392578125, 0.0, 0.00537109375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.00830078125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.0, 0.00341796875, 0.00244140625, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.01806640625, 0.0, 0.0, 0.0029296875, 0.0048828125, 0.01220703125, 0.0, 0.00244140625, 0.0, 0.05712890625, 0.0, 0.0087890625, 0.00048828125, 0.0263671875, 0.03369140625, 0.1337890625, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0009765625, 0.0205078125, 0.0, 0.0087890625, 0.02587890625, 0.00146484375, 0.1943359375, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0087890625, 0.0, 0.0, 0.33447265625, 0.0078125, 0.0, 0.25927734375, 0.0, 0.0, 0.0, 0.00048828125, 0.0048828125, 0.0458984375, 0.0068359375, 0.0, 0.00537109375, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0322265625, 0.0, 0.0, 0.0, 0.357421875, 0.0, 0.01318359375, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.04541015625, 0.0, 0.0, 0.00146484375, 0.00244140625, 0.00927734375, 0.0009765625, 0.00244140625, 0.0, 0.037109375, 0.09619140625, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.00830078125, 0.00341796875, 0.0, 0.0, 0.03955078125, 0.00439453125, 0.0, 0.00244140625, 0.00439453125, 0.03271484375, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00341796875, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.001953125, 0.0, 0.08349609375, 0.0, 0.0, 0.0, 0.01318359375, 0.07763671875, 0.001953125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.06640625, 0.0, 0.09423828125, 0.0126953125, 0.00048828125, 0.00048828125, 0.0244140625, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0029296875, 0.00146484375, 0.00048828125, 0.0, 0.0, 0.0, 0.0009765625, 0.001953125, 0.01806640625, 0.00390625, 0.01123046875, 0.49609375, 0.0, 0.0, 0.0, 0.05029296875, 0.00439453125, 0.0, 0.0, 0.00390625, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.0, 0.0068359375, 0.00048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.099609375, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.00048828125, 0.0009765625, 0.03173828125, 0.0, 0.0, 0.08203125, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00244140625, 0.00048828125, 0.0, 0.001953125, 0.1767578125, 0.00048828125, 0.0, 0.0, 0.009765625, 0.00146484375, 0.00244140625, 0.0, 0.00341796875, 0.0, 0.064453125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.00048828125, 0.001953125, 0.00244140625, 0.3720703125, 0.13232421875, 0.0, 0.55810546875, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00927734375, 0.00048828125, 0.00048828125, 0.001953125, 0.0, 0.0205078125, 0.0, 0.33642578125, 0.0, 0.005859375, 0.4814453125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.12939453125, 0.00146484375, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0009765625, 0.00048828125, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00537109375, 0.00048828125, 0.00244140625, 0.0, 0.00048828125, 0.16015625, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.04296875, 0.00048828125, 0.0, 0.4453125, 0.0, 0.0, 0.0, 0.0, 0.00830078125, 0.0283203125, 0.01318359375, 0.009765625, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.0, 0.01171875, 0.0, 0.0029296875, 0.01513671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.025390625, 0.00146484375, 0.00048828125, 0.00048828125, 0.0, 0.00439453125, 0.0, 0.00146484375, 0.0078125, 0.0029296875, 0.00244140625, 0.00244140625, 0.04296875, 0.533203125, 0.46875, 0.0, 0.01171875, 0.0, 0.0, 0.001953125, 0.0029296875, 0.0, 0.00146484375, 0.0, 0.00146484375, 0.00146484375, 0.0, 0.00048828125, 0.00146484375, 0.00341796875, 0.0, 0.00048828125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.0, 0.0, 0.0390625, 0.0, 0.00634765625, 0.0, 0.0, 0.00244140625, 0.0, 0.0, 0.03466796875, 0.0, 0.00537109375, 0.0, 0.0029296875, 0.2392578125, 0.0, 0.0546875, 0.0, 0.0, 0.0, 0.36279296875, 0.0, 0.00048828125, 0.0029296875, 0.00244140625, 0.005859375, 0.0, 0.0, 0.00146484375, 0.00390625, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00048828125, 0.0, 0.01513671875, 0.001953125, 0.00048828125, 0.00048828125, 0.00341796875, 0.05712890625, 0.0, 0.0, 0.0, 0.06640625, 0.0, 0.00537109375, 0.0, 0.0, 0.0400390625, 0.095703125, 0.00048828125, 0.0, 0.0, 0.0, 0.06494140625, 0.0, 0.0009765625, 0.01611328125, 0.0009765625, 0.01318359375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00146484375, 0.0, 0.001953125, 0.00146484375, 0.00146484375, 0.0029296875, 0.01318359375, 0.00341796875, 0.0, 0.0, 0.0, 0.0009765625, 0.00146484375, 0.0185546875, 0.00146484375, 0.0029296875, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.01025390625, 0.0, 0.27490234375, 0.00537109375, 0.08251953125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00634765625, 0.0029296875, 0.00048828125, 0.0, 0.0, 0.0302734375, 0.00048828125, 0.0, 0.00048828125, 0.21923828125, 0.0, 0.001953125, 0.0068359375, 0.0, 0.001953125, 0.0341796875, 0.00390625, 0.0, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.00244140625, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0009765625, 0.87255859375, 0.05419921875, 0.0, 0.0, 0.0654296875, 0.00048828125, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00146484375, 0.0771484375, 0.00927734375, 0.00146484375, 0.0, 0.0, 0.0, 0.68603515625, 0.00048828125, 0.00244140625, 0.00048828125, 0.0048828125, 0.0009765625, 0.0087890625, 0.0, 0.00048828125, 0.27490234375, 0.0, 0.0, 0.1357421875, 0.00830078125, 0.0009765625, 0.00390625, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.02001953125, 0.0, 0.00048828125, 0.00537109375, 0.00048828125, 0.0, 0.0, 0.00439453125, 0.00244140625, 0.0, 0.0, 0.01318359375, 0.0, 0.01318359375, 0.00341796875, 0.8896484375, 0.796875, 0.19580078125, 0.1826171875, 0.9248046875, 0.37939453125, 0.99658203125, 0.99951171875, 0.99560546875, 0.9892578125, 0.9990234375, 0.92236328125, 0.951171875, 0.74462890625, 0.1318359375, 0.75341796875, 0.18603515625, 0.97802734375, 0.0107421875, 0.2421875, 0.93310546875, 0.16357421875, 0.95654296875, 0.072265625, 0.18017578125, 0.03271484375, 0.99951171875, 0.65673828125, 0.9931640625, 0.14599609375, 0.99755859375, 0.50390625, 0.00146484375, 0.662109375, 1.0, 0.91796875, 0.203125, 0.94580078125, 0.92578125, 0.96044921875, 0.25390625, 0.27197265625, 0.99853515625, 0.3564453125, 0.01318359375, 0.89111328125, 0.9736328125, 0.6943359375, 0.546875, 1.0, 0.501953125, 0.89990234375, 0.7705078125, 0.99853515625, 0.767578125, 0.0009765625, 0.0224609375, 0.880859375, 0.69921875, 0.86865234375, 0.99755859375, 0.9951171875, 0.994140625, 0.60595703125, 0.0302734375, 0.86328125, 0.8154296875, 0.92919921875, 0.88623046875, 0.291015625, 0.83544921875, 0.51171875, 0.7607421875, 0.638671875, 0.99267578125, 0.71875, 0.86474609375, 0.908203125, 0.97216796875, 0.7646484375, 0.99072265625, 0.35205078125, 0.9990234375, 0.33349609375, 1.0, 0.7880859375, 0.0, 0.0087890625, 0.9541015625, 0.42822265625, 0.341796875, 0.998046875, 0.02587890625, 0.8232421875, 0.93505859375, 0.11669921875, 0.9990234375, 0.189453125, 0.9326171875, 0.00048828125, 0.96826171875, 0.763671875, 0.4111328125, 0.9619140625, 0.12744140625, 0.93505859375, 0.7958984375, 0.89697265625, 0.06591796875, 0.9990234375, 0.75048828125, 0.96923828125, 0.4111328125, 0.5908203125, 0.509765625, 0.56005859375, 0.94384765625, 0.90380859375, 0.99658203125, 0.93017578125, 0.21337890625, 0.189453125, 0.00439453125, 0.96728515625, 0.884765625, 0.05419921875, 0.43212890625, 0.9765625, 0.04833984375, 0.89404296875, 0.82666015625, 0.7548828125, 0.0888671875, 0.00341796875, 0.99951171875, 0.98828125, 0.99853515625, 0.0341796875, 0.98193359375, 0.79052734375, 0.51171875, 0.9990234375, 0.89306640625, 0.986328125, 0.88427734375, 0.98828125, 0.951171875, 0.806640625, 0.505859375, 0.09814453125, 0.388671875, 0.9990234375, 0.96728515625, 0.99951171875, 0.79052734375, 0.9990234375, 0.998046875, 1.0, 0.99853515625, 0.9970703125, 0.9541015625, 0.0888671875, 0.001953125, 0.8671875, 0.9541015625, 0.99853515625, 0.99951171875, 0.98779296875, 0.111328125, 0.72802734375, 0.0849609375, 0.9345703125, 0.93603515625, 0.97021484375, 0.009765625, 0.00048828125, 0.97900390625, 0.9951171875, 0.01904296875, 0.74169921875, 0.9677734375, 0.97509765625, 0.392578125, 0.77734375, 0.91162109375, 0.9775390625, 1.0, 0.9775390625, 1.0, 0.77197265625, 0.99072265625, 0.17236328125, 0.93212890625, 0.1240234375, 0.533203125, 0.955078125, 0.91943359375, 0.451171875, 0.3056640625, 0.0, 0.998046875, 0.99462890625, 0.99462890625, 0.98828125, 0.51171875, 0.9091796875, 0.9990234375, 0.923828125, 0.2392578125, 0.8154296875, 0.63330078125, 0.974609375, 0.525390625, 0.9189453125, 0.99853515625, 0.01611328125, 0.9814453125, 0.99072265625, 0.00390625, 0.14892578125, 0.95947265625, 0.015625, 0.95361328125, 0.14111328125, 0.43798828125, 0.640625, 0.9970703125, 0.93310546875, 0.998046875, 0.98876953125, 0.93994140625, 0.98046875, 0.01025390625, 0.9833984375, 0.93896484375, 0.91259765625, 1.0, 0.845703125, 0.99951171875, 0.7265625, 0.99951171875, 0.85986328125, 0.0009765625, 0.9794921875, 0.3740234375, 0.2265625, 0.98388671875, 0.99560546875, 0.60009765625, 0.9912109375, 0.0380859375, 0.00048828125, 0.8916015625, 1.0, 0.0849609375, 0.556640625, 1.0, 0.08447265625, 1.0, 1.0, 0.92724609375, 0.94580078125, 0.1376953125, 0.0615234375, 0.498046875, 0.29443359375, 0.921875, 0.986328125, 0.9638671875, 0.451171875, 0.9375, 0.02099609375, 0.083984375, 0.87890625, 1.0, 0.96826171875, 0.95751953125, 0.65478515625, 0.80078125, 0.60205078125, 0.99169921875, 0.73583984375, 0.99951171875, 0.9736328125, 0.99267578125, 0.1259765625, 0.3486328125, 0.99755859375, 0.10888671875, 0.17333984375, 0.1044921875, 0.81884765625, 0.001953125, 0.2255859375, 1.0, 0.85107421875, 0.99658203125, 0.85595703125, 0.0947265625, 0.9638671875, 0.9638671875, 0.98388671875, 0.9970703125, 0.57958984375, 0.02587890625, 0.7744140625, 0.80419921875, 0.8642578125, 1.0, 0.99658203125, 0.974609375, 0.0322265625, 0.6689453125, 0.97998046875, 0.505859375, 0.9921875, 0.96875, 0.12744140625, 0.716796875, 0.00439453125, 0.845703125, 0.998046875, 0.93994140625, 0.93701171875, 0.94970703125, 0.0966796875, 0.91259765625, 0.04345703125, 0.99951171875, 0.57763671875, 0.77880859375, 1.0, 0.05322265625, 0.9111328125, 0.9970703125, 0.86083984375, 0.99462890625, 0.1611328125, 0.56005859375, 0.1572265625, 0.57958984375, 0.9873046875, 0.99951171875, 0.537109375, 0.99853515625, 0.9970703125, 0.9990234375, 0.99951171875, 0.5849609375, 0.8017578125, 0.97216796875, 0.83349609375, 0.98095703125, 0.0537109375, 0.8173828125, 0.990234375, 0.05810546875, 0.9267578125, 0.09423828125, 0.54296875, 1.0, 0.05810546875, 0.73876953125, 1.0, 0.9794921875, 0.0751953125, 0.00146484375, 0.0927734375, 0.7216796875, 0.00732421875, 0.00927734375, 0.91552734375, 0.98681640625, 0.9833984375, 0.5390625, 0.9970703125, 0.83642578125, 0.9072265625, 0.9970703125, 0.00732421875, 0.390625, 0.99072265625, 0.513671875, 0.31396484375, 1.0, 0.91259765625, 0.970703125, 0.2353515625, 0.39990234375, 0.03515625, 0.95703125, 0.95751953125, 0.85888671875, 0.9765625, 0.01220703125, 0.99072265625, 0.99462890625, 0.99365234375, 0.3681640625, 0.88330078125, 0.966796875, 0.87060546875, 0.9921875, 0.03271484375, 0.93896484375, 0.982421875, 1.0, 0.85888671875, 0.28955078125, 0.13134765625, 0.0966796875, 0.94580078125, 0.38330078125, 0.26416015625, 0.96875, 0.75341796875, 0.10107421875, 0.76904296875, 0.99853515625, 0.93994140625, 0.75048828125, 0.04833984375, 0.458984375, 0.06884765625, 0.43212890625, 0.00244140625, 0.9892578125, 0.9970703125, 0.232421875, 0.36279296875, 0.966796875, 0.93603515625, 0.9365234375, 0.24365234375, 0.28466796875, 0.9736328125, 0.00732421875, 0.845703125, 0.392578125, 0.7958984375, 0.9833984375, 0.0322265625, 0.96142578125, 0.50390625, 0.98974609375, 0.67236328125, 0.99072265625, 0.69287109375, 0.56591796875, 0.13037109375, 0.04150390625, 1.0, 0.7294921875, 0.97705078125, 0.9287109375, 0.8427734375, 0.6259765625, 0.9853515625, 0.990234375, 0.861328125, 0.515625, 0.91064453125, 0.82421875, 0.95361328125, 0.9892578125, 0.99609375, 0.9931640625, 0.0087890625, 0.99951171875, 0.8671875, 0.96728515625, 0.9912109375, 0.9990234375, 0.01416015625, 0.9921875, 0.474609375, 0.9521484375, 0.8974609375, 0.9931640625, 0.021484375, 0.865234375, 0.00439453125, 0.01953125, 1.0, 0.15625, 0.98828125, 0.83203125, 0.12353515625, 0.982421875, 0.1181640625, 0.57373046875, 0.9990234375, 0.16162109375, 0.994140625, 0.99951171875, 0.99169921875, 0.642578125, 0.1376953125, 0.998046875, 0.96337890625, 0.931640625, 0.9990234375, 0.03173828125, 0.98486328125, 0.99462890625, 0.998046875, 0.9873046875, 0.001953125, 0.43603515625, 0.76220703125, 0.56982421875, 0.61669921875, 0.99951171875, 0.97412109375, 0.39697265625, 0.69580078125, 0.87255859375, 0.13671875, 0.865234375, 0.841796875, 0.0400390625, 0.064453125, 0.015625, 0.998046875, 0.4052734375, 0.98828125, 0.4921875, 0.99853515625, 0.9677734375, 0.78271484375, 0.9326171875, 0.9990234375, 0.43212890625, 0.986328125, 0.9716796875, 0.455078125, 0.267578125, 0.99365234375, 1.0, 0.22021484375, 0.12158203125, 0.58154296875, 0.810546875, 0.73388671875, 0.01611328125, 0.99658203125, 0.01904296875, 0.9482421875, 0.01708984375, 0.958984375, 0.45703125, 0.70068359375, 0.99951171875, 0.2958984375, 1.0, 0.99853515625, 0.998046875, 0.9931640625, 0.953125, 0.958984375, 0.98974609375, 0.291015625, 0.501953125, 0.94873046875, 0.71533203125, 0.7744140625, 0.97900390625, 0.96337890625, 0.49609375, 0.072265625, 0.53125, 0.9609375, 0.5869140625, 0.96923828125, 1.0, 0.013671875, 0.865234375, 0.021484375, 0.9111328125, 0.947265625, 0.9921875, 0.0244140625, 0.796875, 0.91650390625, 1.0, 1.0, 1.0, 0.99951171875, 0.5927734375, 0.99951171875, 0.82763671875, 0.99755859375, 0.99462890625, 0.986328125, 0.8447265625, 0.99951171875, 1.0, 0.9248046875, 1.0, 0.04443359375, 0.9873046875, 0.9970703125, 0.0009765625, 0.998046875, 0.00390625, 1.0, 0.00390625, 0.814453125, 0.7607421875, 0.92041015625, 0.9833984375, 0.85400390625, 0.9501953125, 0.994140625, 0.998046875, 0.9921875, 0.68408203125, 0.7216796875, 0.15185546875, 0.01416015625, 0.18359375, 0.97021484375, 0.505859375, 0.541015625, 0.18701171875, 0.6826171875, 0.00048828125, 1.0, 0.99365234375, 1.0, 0.99609375, 0.99609375, 0.91748046875, 0.4130859375, 0.90478515625, 0.42431640625, 0.466796875, 0.08251953125, 0.95751953125, 0.0126953125, 0.89013671875, 0.12841796875, 0.1142578125, 0.00341796875, 0.541015625, 0.8017578125, 0.541015625, 0.9951171875, 0.76904296875, 0.97314453125, 0.99609375, 1.0, 0.3330078125, 0.986328125, 0.71240234375, 1.0, 0.984375, 0.068359375, 0.90185546875, 0.9091796875, 0.998046875, 0.99853515625, 0.013671875, 0.83544921875, 0.7890625, 0.14697265625, 0.06494140625, 0.35205078125, 0.43017578125, 0.32763671875, 0.947265625, 0.00830078125, 0.36474609375, 0.203125, 0.43408203125, 0.8427734375, 1.0, 0.7705078125, 0.01904296875, 0.39990234375, 0.9384765625, 0.921875, 0.640625, 0.95166015625, 0.97265625, 0.99658203125, 0.9619140625, 0.93994140625, 0.9833984375, 0.06787109375, 0.7880859375, 0.99951171875, 1.0, 0.4072265625, 0.09716796875, 0.96728515625, 0.9404296875, 0.4150390625, 0.0048828125, 0.337890625, 1.0, 0.81640625, 0.29931640625, 0.998046875, 0.99853515625, 0.59619140625, 0.1357421875, 1.0, 0.990234375, 0.1015625, 0.93115234375, 0.927734375, 0.00927734375, 0.27197265625, 0.14501953125, 1.0, 0.91552734375, 0.71875, 0.61865234375, 0.078125, 0.98046875, 0.14697265625, 0.99169921875, 0.064453125, 0.25537109375, 0.732421875, 0.99853515625, 1.0, 0.1689453125, 0.966796875, 0.05126953125, 0.01708984375, 0.04345703125, 0.9833984375, 0.9814453125, 0.9833984375, 0.99462890625, 0.001953125, 1.0, 0.9970703125, 0.4150390625, 0.99755859375, 0.9091796875, 0.93994140625, 0.99951171875, 0.99951171875, 0.04345703125, 0.9755859375, 0.67919921875, 0.025390625, 0.52734375, 0.98779296875, 0.001953125, 1.0, 0.00537109375, 0.00439453125, 0.8818359375, 1.0, 0.103515625, 0.1240234375, 0.9970703125, 1.0, 0.15283203125, 0.74462890625, 0.998046875, 1.0, 0.42822265625, 0.12353515625, 0.923828125, 0.029296875, 0.99560546875, 0.16796875, 0.9990234375, 0.03076171875, 0.9921875, 0.82080078125, 0.0986328125, 0.9990234375, 0.02685546875, 0.99951171875, 0.9306640625, 0.0400390625, 0.01220703125, 0.93505859375, 0.9990234375, 0.01025390625, 0.0087890625, 0.8115234375, 0.7265625, 0.7431640625, 0.88427734375, 0.9990234375, 0.8916015625, 0.61669921875, 0.99951171875, 0.92529296875, 0.56591796875, 0.32080078125, 0.9990234375, 0.99169921875]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [00:36<00:00,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00439453125, 0.0009765625, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.00048828125, 0.00439453125, 0.0048828125, 0.00048828125, 0.009765625, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.455078125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.0146484375, 0.00146484375, 0.00146484375, 0.0, 0.0, 0.0009765625, 0.0078125, 0.0, 0.0107421875, 0.0009765625, 0.60400390625, 0.0, 0.00390625, 0.013671875, 0.0, 0.0, 0.0, 0.310546875, 0.017578125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09716796875, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.03955078125, 0.0, 0.0, 0.01025390625, 0.0, 0.0068359375, 0.00341796875, 0.06201171875, 0.0, 0.01513671875, 0.0, 0.01171875, 0.30419921875, 0.00244140625, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0146484375, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.00048828125, 0.0029296875, 0.0048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.02294921875, 0.0029296875, 0.1982421875, 0.0009765625, 0.06396484375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.06884765625, 0.00048828125, 0.0185546875, 0.0009765625, 0.0, 0.0, 0.001953125, 0.0, 0.00244140625, 0.0, 0.0, 0.55810546875, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.03466796875, 0.001953125, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0078125, 0.9794921875, 0.00439453125, 0.0, 0.00048828125, 0.0341796875, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0283203125, 0.0, 0.00146484375, 0.0009765625, 0.0087890625, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.00634765625, 0.00439453125, 0.0009765625, 0.0, 0.0, 0.00390625, 0.0, 0.0029296875, 0.00048828125, 0.0, 0.02783203125, 0.0, 0.0009765625, 0.0, 0.0, 0.033203125, 0.0, 0.0009765625, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.06298828125, 0.08447265625, 0.0, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.0498046875, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0634765625, 0.00244140625, 0.0, 0.0009765625, 0.0029296875, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.01220703125, 0.0, 0.03076171875, 0.01416015625, 0.00732421875, 0.00048828125, 0.01318359375, 0.0, 0.04296875, 0.0, 0.0, 0.0, 0.12109375, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.0009765625, 0.0029296875, 0.041015625, 0.0, 0.0, 0.00341796875, 0.1669921875, 0.00048828125, 0.00146484375, 0.0, 0.00048828125, 0.0, 0.00244140625, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.142578125, 0.0, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.0009765625, 0.06884765625, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.0, 0.01318359375, 0.01513671875, 0.01904296875, 0.0009765625, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0009765625, 0.00048828125, 0.00048828125, 0.0009765625, 0.00390625, 0.00244140625, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.02294921875, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.00830078125, 0.00048828125, 0.0, 0.0, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.26416015625, 0.28955078125, 0.0, 0.0, 0.0, 0.0732421875, 0.0009765625, 0.0009765625, 0.0009765625, 0.12939453125, 0.001953125, 0.00341796875, 0.0, 0.00048828125, 0.00048828125, 0.083984375, 0.0, 0.0, 0.03125, 0.0029296875, 0.0, 0.0029296875, 0.00537109375, 0.0, 0.00927734375, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.30224609375, 0.0, 0.0, 0.00927734375, 0.0, 0.00048828125, 0.11767578125, 0.0, 0.0, 0.0029296875, 0.00244140625, 0.00341796875, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0185546875, 0.0, 0.00048828125, 0.0009765625, 0.0, 0.00048828125, 0.06689453125, 0.00048828125, 0.00537109375, 0.0048828125, 0.0009765625, 0.0029296875, 0.00048828125, 0.00244140625, 0.0, 0.0634765625, 0.0, 0.00048828125, 0.005859375, 0.0, 0.00732421875, 0.0078125, 0.00341796875, 0.0, 0.0, 0.0087890625, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.0, 0.013671875, 0.02392578125, 0.0, 0.00537109375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.00830078125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.0, 0.00341796875, 0.00244140625, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.01806640625, 0.0, 0.0, 0.0029296875, 0.0048828125, 0.01220703125, 0.0, 0.00244140625, 0.0, 0.05712890625, 0.0, 0.0087890625, 0.00048828125, 0.0263671875, 0.03369140625, 0.1337890625, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0009765625, 0.0205078125, 0.0, 0.0087890625, 0.02587890625, 0.00146484375, 0.1943359375, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0087890625, 0.0, 0.0, 0.33447265625, 0.0078125, 0.0, 0.25927734375, 0.0, 0.0, 0.0, 0.00048828125, 0.0048828125, 0.0458984375, 0.0068359375, 0.0, 0.00537109375, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0322265625, 0.0, 0.0, 0.0, 0.357421875, 0.0, 0.01318359375, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.04541015625, 0.0, 0.0, 0.00146484375, 0.00244140625, 0.00927734375, 0.0009765625, 0.00244140625, 0.0, 0.037109375, 0.09619140625, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.00830078125, 0.00341796875, 0.0, 0.0, 0.03955078125, 0.00439453125, 0.0, 0.00244140625, 0.00439453125, 0.03271484375, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00341796875, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.001953125, 0.0, 0.08349609375, 0.0, 0.0, 0.0, 0.01318359375, 0.07763671875, 0.001953125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.06640625, 0.0, 0.09423828125, 0.0126953125, 0.00048828125, 0.00048828125, 0.0244140625, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0029296875, 0.00146484375, 0.00048828125, 0.0, 0.0, 0.0, 0.0009765625, 0.001953125, 0.01806640625, 0.00390625, 0.01123046875, 0.49609375, 0.0, 0.0, 0.0, 0.05029296875, 0.00439453125, 0.0, 0.0, 0.00390625, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.0, 0.0068359375, 0.00048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.099609375, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.00048828125, 0.0009765625, 0.03173828125, 0.0, 0.0, 0.08203125, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00244140625, 0.00048828125, 0.0, 0.001953125, 0.1767578125, 0.00048828125, 0.0, 0.0, 0.009765625, 0.00146484375, 0.00244140625, 0.0, 0.00341796875, 0.0, 0.064453125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.00048828125, 0.001953125, 0.00244140625, 0.3720703125, 0.13232421875, 0.0, 0.55810546875, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00927734375, 0.00048828125, 0.00048828125, 0.001953125, 0.0, 0.0205078125, 0.0, 0.33642578125, 0.0, 0.005859375, 0.4814453125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.12939453125, 0.00146484375, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0009765625, 0.00048828125, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00537109375, 0.00048828125, 0.00244140625, 0.0, 0.00048828125, 0.16015625, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.04296875, 0.00048828125, 0.0, 0.4453125, 0.0, 0.0, 0.0, 0.0, 0.00830078125, 0.0283203125, 0.01318359375, 0.009765625, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.0, 0.01171875, 0.0, 0.0029296875, 0.01513671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.025390625, 0.00146484375, 0.00048828125, 0.00048828125, 0.0, 0.00439453125, 0.0, 0.00146484375, 0.0078125, 0.0029296875, 0.00244140625, 0.00244140625, 0.04296875, 0.533203125, 0.46875, 0.0, 0.01171875, 0.0, 0.0, 0.001953125, 0.0029296875, 0.0, 0.00146484375, 0.0, 0.00146484375, 0.00146484375, 0.0, 0.00048828125, 0.00146484375, 0.00341796875, 0.0, 0.00048828125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.0, 0.0, 0.0390625, 0.0, 0.00634765625, 0.0, 0.0, 0.00244140625, 0.0, 0.0, 0.03466796875, 0.0, 0.00537109375, 0.0, 0.0029296875, 0.2392578125, 0.0, 0.0546875, 0.0, 0.0, 0.0, 0.36279296875, 0.0, 0.00048828125, 0.0029296875, 0.00244140625, 0.005859375, 0.0, 0.0, 0.00146484375, 0.00390625, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00048828125, 0.0, 0.01513671875, 0.001953125, 0.00048828125, 0.00048828125, 0.00341796875, 0.05712890625, 0.0, 0.0, 0.0, 0.06640625, 0.0, 0.00537109375, 0.0, 0.0, 0.0400390625, 0.095703125, 0.00048828125, 0.0, 0.0, 0.0, 0.06494140625, 0.0, 0.0009765625, 0.01611328125, 0.0009765625, 0.01318359375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00146484375, 0.0, 0.001953125, 0.00146484375, 0.00146484375, 0.0029296875, 0.01318359375, 0.00341796875, 0.0, 0.0, 0.0, 0.0009765625, 0.00146484375, 0.0185546875, 0.00146484375, 0.0029296875, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.01025390625, 0.0, 0.27490234375, 0.00537109375, 0.08251953125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00634765625, 0.0029296875, 0.00048828125, 0.0, 0.0, 0.0302734375, 0.00048828125, 0.0, 0.00048828125, 0.21923828125, 0.0, 0.001953125, 0.0068359375, 0.0, 0.001953125, 0.0341796875, 0.00390625, 0.0, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.00244140625, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0009765625, 0.87255859375, 0.05419921875, 0.0, 0.0, 0.0654296875, 0.00048828125, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00146484375, 0.0771484375, 0.00927734375, 0.00146484375, 0.0, 0.0, 0.0, 0.68603515625, 0.00048828125, 0.00244140625, 0.00048828125, 0.0048828125, 0.0009765625, 0.0087890625, 0.0, 0.00048828125, 0.27490234375, 0.0, 0.0, 0.1357421875, 0.00830078125, 0.0009765625, 0.00390625, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.02001953125, 0.0, 0.00048828125, 0.00537109375, 0.00048828125, 0.0, 0.0, 0.00439453125, 0.00244140625, 0.0, 0.0, 0.01318359375, 0.0, 0.01318359375, 0.00341796875, 0.8896484375, 0.796875, 0.19580078125, 0.1826171875, 0.9248046875, 0.37939453125, 0.99658203125, 0.99951171875, 0.99560546875, 0.9892578125, 0.9990234375, 0.92236328125, 0.951171875, 0.74462890625, 0.1318359375, 0.75341796875, 0.18603515625, 0.97802734375, 0.0107421875, 0.2421875, 0.93310546875, 0.16357421875, 0.95654296875, 0.072265625, 0.18017578125, 0.03271484375, 0.99951171875, 0.65673828125, 0.9931640625, 0.14599609375, 0.99755859375, 0.50390625, 0.00146484375, 0.662109375, 1.0, 0.91796875, 0.203125, 0.94580078125, 0.92578125, 0.96044921875, 0.25390625, 0.27197265625, 0.99853515625, 0.3564453125, 0.01318359375, 0.89111328125, 0.9736328125, 0.6943359375, 0.546875, 1.0, 0.501953125, 0.89990234375, 0.7705078125, 0.99853515625, 0.767578125, 0.0009765625, 0.0224609375, 0.880859375, 0.69921875, 0.86865234375, 0.99755859375, 0.9951171875, 0.994140625, 0.60595703125, 0.0302734375, 0.86328125, 0.8154296875, 0.92919921875, 0.88623046875, 0.291015625, 0.83544921875, 0.51171875, 0.7607421875, 0.638671875, 0.99267578125, 0.71875, 0.86474609375, 0.908203125, 0.97216796875, 0.7646484375, 0.99072265625, 0.35205078125, 0.9990234375, 0.33349609375, 1.0, 0.7880859375, 0.0, 0.0087890625, 0.9541015625, 0.42822265625, 0.341796875, 0.998046875, 0.02587890625, 0.8232421875, 0.93505859375, 0.11669921875, 0.9990234375, 0.189453125, 0.9326171875, 0.00048828125, 0.96826171875, 0.763671875, 0.4111328125, 0.9619140625, 0.12744140625, 0.93505859375, 0.7958984375, 0.89697265625, 0.06591796875, 0.9990234375, 0.75048828125, 0.96923828125, 0.4111328125, 0.5908203125, 0.509765625, 0.56005859375, 0.94384765625, 0.90380859375, 0.99658203125, 0.93017578125, 0.21337890625, 0.189453125, 0.00439453125, 0.96728515625, 0.884765625, 0.05419921875, 0.43212890625, 0.9765625, 0.04833984375, 0.89404296875, 0.82666015625, 0.7548828125, 0.0888671875, 0.00341796875, 0.99951171875, 0.98828125, 0.99853515625, 0.0341796875, 0.98193359375, 0.79052734375, 0.51171875, 0.9990234375, 0.89306640625, 0.986328125, 0.88427734375, 0.98828125, 0.951171875, 0.806640625, 0.505859375, 0.09814453125, 0.388671875, 0.9990234375, 0.96728515625, 0.99951171875, 0.79052734375, 0.9990234375, 0.998046875, 1.0, 0.99853515625, 0.9970703125, 0.9541015625, 0.0888671875, 0.001953125, 0.8671875, 0.9541015625, 0.99853515625, 0.99951171875, 0.98779296875, 0.111328125, 0.72802734375, 0.0849609375, 0.9345703125, 0.93603515625, 0.97021484375, 0.009765625, 0.00048828125, 0.97900390625, 0.9951171875, 0.01904296875, 0.74169921875, 0.9677734375, 0.97509765625, 0.392578125, 0.77734375, 0.91162109375, 0.9775390625, 1.0, 0.9775390625, 1.0, 0.77197265625, 0.99072265625, 0.17236328125, 0.93212890625, 0.1240234375, 0.533203125, 0.955078125, 0.91943359375, 0.451171875, 0.3056640625, 0.0, 0.998046875, 0.99462890625, 0.99462890625, 0.98828125, 0.51171875, 0.9091796875, 0.9990234375, 0.923828125, 0.2392578125, 0.8154296875, 0.63330078125, 0.974609375, 0.525390625, 0.9189453125, 0.99853515625, 0.01611328125, 0.9814453125, 0.99072265625, 0.00390625, 0.14892578125, 0.95947265625, 0.015625, 0.95361328125, 0.14111328125, 0.43798828125, 0.640625, 0.9970703125, 0.93310546875, 0.998046875, 0.98876953125, 0.93994140625, 0.98046875, 0.01025390625, 0.9833984375, 0.93896484375, 0.91259765625, 1.0, 0.845703125, 0.99951171875, 0.7265625, 0.99951171875, 0.85986328125, 0.0009765625, 0.9794921875, 0.3740234375, 0.2265625, 0.98388671875, 0.99560546875, 0.60009765625, 0.9912109375, 0.0380859375, 0.00048828125, 0.8916015625, 1.0, 0.0849609375, 0.556640625, 1.0, 0.08447265625, 1.0, 1.0, 0.92724609375, 0.94580078125, 0.1376953125, 0.0615234375, 0.498046875, 0.29443359375, 0.921875, 0.986328125, 0.9638671875, 0.451171875, 0.9375, 0.02099609375, 0.083984375, 0.87890625, 1.0, 0.96826171875, 0.95751953125, 0.65478515625, 0.80078125, 0.60205078125, 0.99169921875, 0.73583984375, 0.99951171875, 0.9736328125, 0.99267578125, 0.1259765625, 0.3486328125, 0.99755859375, 0.10888671875, 0.17333984375, 0.1044921875, 0.81884765625, 0.001953125, 0.2255859375, 1.0, 0.85107421875, 0.99658203125, 0.85595703125, 0.0947265625, 0.9638671875, 0.9638671875, 0.98388671875, 0.9970703125, 0.57958984375, 0.02587890625, 0.7744140625, 0.80419921875, 0.8642578125, 1.0, 0.99658203125, 0.974609375, 0.0322265625, 0.6689453125, 0.97998046875, 0.505859375, 0.9921875, 0.96875, 0.12744140625, 0.716796875, 0.00439453125, 0.845703125, 0.998046875, 0.93994140625, 0.93701171875, 0.94970703125, 0.0966796875, 0.91259765625, 0.04345703125, 0.99951171875, 0.57763671875, 0.77880859375, 1.0, 0.05322265625, 0.9111328125, 0.9970703125, 0.86083984375, 0.99462890625, 0.1611328125, 0.56005859375, 0.1572265625, 0.57958984375, 0.9873046875, 0.99951171875, 0.537109375, 0.99853515625, 0.9970703125, 0.9990234375, 0.99951171875, 0.5849609375, 0.8017578125, 0.97216796875, 0.83349609375, 0.98095703125, 0.0537109375, 0.8173828125, 0.990234375, 0.05810546875, 0.9267578125, 0.09423828125, 0.54296875, 1.0, 0.05810546875, 0.73876953125, 1.0, 0.9794921875, 0.0751953125, 0.00146484375, 0.0927734375, 0.7216796875, 0.00732421875, 0.00927734375, 0.91552734375, 0.98681640625, 0.9833984375, 0.5390625, 0.9970703125, 0.83642578125, 0.9072265625, 0.9970703125, 0.00732421875, 0.390625, 0.99072265625, 0.513671875, 0.31396484375, 1.0, 0.91259765625, 0.970703125, 0.2353515625, 0.39990234375, 0.03515625, 0.95703125, 0.95751953125, 0.85888671875, 0.9765625, 0.01220703125, 0.99072265625, 0.99462890625, 0.99365234375, 0.3681640625, 0.88330078125, 0.966796875, 0.87060546875, 0.9921875, 0.03271484375, 0.93896484375, 0.982421875, 1.0, 0.85888671875, 0.28955078125, 0.13134765625, 0.0966796875, 0.94580078125, 0.38330078125, 0.26416015625, 0.96875, 0.75341796875, 0.10107421875, 0.76904296875, 0.99853515625, 0.93994140625, 0.75048828125, 0.04833984375, 0.458984375, 0.06884765625, 0.43212890625, 0.00244140625, 0.9892578125, 0.9970703125, 0.232421875, 0.36279296875, 0.966796875, 0.93603515625, 0.9365234375, 0.24365234375, 0.28466796875, 0.9736328125, 0.00732421875, 0.845703125, 0.392578125, 0.7958984375, 0.9833984375, 0.0322265625, 0.96142578125, 0.50390625, 0.98974609375, 0.67236328125, 0.99072265625, 0.69287109375, 0.56591796875, 0.13037109375, 0.04150390625, 1.0, 0.7294921875, 0.97705078125, 0.9287109375, 0.8427734375, 0.6259765625, 0.9853515625, 0.990234375, 0.861328125, 0.515625, 0.91064453125, 0.82421875, 0.95361328125, 0.9892578125, 0.99609375, 0.9931640625, 0.0087890625, 0.99951171875, 0.8671875, 0.96728515625, 0.9912109375, 0.9990234375, 0.01416015625, 0.9921875, 0.474609375, 0.9521484375, 0.8974609375, 0.9931640625, 0.021484375, 0.865234375, 0.00439453125, 0.01953125, 1.0, 0.15625, 0.98828125, 0.83203125, 0.12353515625, 0.982421875, 0.1181640625, 0.57373046875, 0.9990234375, 0.16162109375, 0.994140625, 0.99951171875, 0.99169921875, 0.642578125, 0.1376953125, 0.998046875, 0.96337890625, 0.931640625, 0.9990234375, 0.03173828125, 0.98486328125, 0.99462890625, 0.998046875, 0.9873046875, 0.001953125, 0.43603515625, 0.76220703125, 0.56982421875, 0.61669921875, 0.99951171875, 0.97412109375, 0.39697265625, 0.69580078125, 0.87255859375, 0.13671875, 0.865234375, 0.841796875, 0.0400390625, 0.064453125, 0.015625, 0.998046875, 0.4052734375, 0.98828125, 0.4921875, 0.99853515625, 0.9677734375, 0.78271484375, 0.9326171875, 0.9990234375, 0.43212890625, 0.986328125, 0.9716796875, 0.455078125, 0.267578125, 0.99365234375, 1.0, 0.22021484375, 0.12158203125, 0.58154296875, 0.810546875, 0.73388671875, 0.01611328125, 0.99658203125, 0.01904296875, 0.9482421875, 0.01708984375, 0.958984375, 0.45703125, 0.70068359375, 0.99951171875, 0.2958984375, 1.0, 0.99853515625, 0.998046875, 0.9931640625, 0.953125, 0.958984375, 0.98974609375, 0.291015625, 0.501953125, 0.94873046875, 0.71533203125, 0.7744140625, 0.97900390625, 0.96337890625, 0.49609375, 0.072265625, 0.53125, 0.9609375, 0.5869140625, 0.96923828125, 1.0, 0.013671875, 0.865234375, 0.021484375, 0.9111328125, 0.947265625, 0.9921875, 0.0244140625, 0.796875, 0.91650390625, 1.0, 1.0, 1.0, 0.99951171875, 0.5927734375, 0.99951171875, 0.82763671875, 0.99755859375, 0.99462890625, 0.986328125, 0.8447265625, 0.99951171875, 1.0, 0.9248046875, 1.0, 0.04443359375, 0.9873046875, 0.9970703125, 0.0009765625, 0.998046875, 0.00390625, 1.0, 0.00390625, 0.814453125, 0.7607421875, 0.92041015625, 0.9833984375, 0.85400390625, 0.9501953125, 0.994140625, 0.998046875, 0.9921875, 0.68408203125, 0.7216796875, 0.15185546875, 0.01416015625, 0.18359375, 0.97021484375, 0.505859375, 0.541015625, 0.18701171875, 0.6826171875, 0.00048828125, 1.0, 0.99365234375, 1.0, 0.99609375, 0.99609375, 0.91748046875, 0.4130859375, 0.90478515625, 0.42431640625, 0.466796875, 0.08251953125, 0.95751953125, 0.0126953125, 0.89013671875, 0.12841796875, 0.1142578125, 0.00341796875, 0.541015625, 0.8017578125, 0.541015625, 0.9951171875, 0.76904296875, 0.97314453125, 0.99609375, 1.0, 0.3330078125, 0.986328125, 0.71240234375, 1.0, 0.984375, 0.068359375, 0.90185546875, 0.9091796875, 0.998046875, 0.99853515625, 0.013671875, 0.83544921875, 0.7890625, 0.14697265625, 0.06494140625, 0.35205078125, 0.43017578125, 0.32763671875, 0.947265625, 0.00830078125, 0.36474609375, 0.203125, 0.43408203125, 0.8427734375, 1.0, 0.7705078125, 0.01904296875, 0.39990234375, 0.9384765625, 0.921875, 0.640625, 0.95166015625, 0.97265625, 0.99658203125, 0.9619140625, 0.93994140625, 0.9833984375, 0.06787109375, 0.7880859375, 0.99951171875, 1.0, 0.4072265625, 0.09716796875, 0.96728515625, 0.9404296875, 0.4150390625, 0.0048828125, 0.337890625, 1.0, 0.81640625, 0.29931640625, 0.998046875, 0.99853515625, 0.59619140625, 0.1357421875, 1.0, 0.990234375, 0.1015625, 0.93115234375, 0.927734375, 0.00927734375, 0.27197265625, 0.14501953125, 1.0, 0.91552734375, 0.71875, 0.61865234375, 0.078125, 0.98046875, 0.14697265625, 0.99169921875, 0.064453125, 0.25537109375, 0.732421875, 0.99853515625, 1.0, 0.1689453125, 0.966796875, 0.05126953125, 0.01708984375, 0.04345703125, 0.9833984375, 0.9814453125, 0.9833984375, 0.99462890625, 0.001953125, 1.0, 0.9970703125, 0.4150390625, 0.99755859375, 0.9091796875, 0.93994140625, 0.99951171875, 0.99951171875, 0.04345703125, 0.9755859375, 0.67919921875, 0.025390625, 0.52734375, 0.98779296875, 0.001953125, 1.0, 0.00537109375, 0.00439453125, 0.8818359375, 1.0, 0.103515625, 0.1240234375, 0.9970703125, 1.0, 0.15283203125, 0.74462890625, 0.998046875, 1.0, 0.42822265625, 0.12353515625, 0.923828125, 0.029296875, 0.99560546875, 0.16796875, 0.9990234375, 0.03076171875, 0.9921875, 0.82080078125, 0.0986328125, 0.9990234375, 0.02685546875, 0.99951171875, 0.9306640625, 0.0400390625, 0.01220703125, 0.93505859375, 0.9990234375, 0.01025390625, 0.0087890625, 0.8115234375, 0.7265625, 0.7431640625, 0.88427734375, 0.9990234375, 0.8916015625, 0.61669921875, 0.99951171875, 0.92529296875, 0.56591796875, 0.32080078125, 0.9990234375, 0.99169921875, 0.99951171875, 0.98291015625, 0.15283203125, 0.19189453125, 0.99609375, 0.20947265625, 0.9287109375, 0.08837890625, 0.99951171875, 0.0146484375, 1.0, 0.00048828125, 0.96826171875, 0.9990234375, 0.0966796875, 0.14111328125, 0.552734375, 0.18017578125, 1.0, 0.99072265625, 0.70263671875, 0.732421875, 0.00732421875, 0.98876953125, 0.24072265625, 0.98828125, 0.236328125, 0.05517578125, 0.01904296875, 0.96044921875, 0.20166015625, 0.9931640625, 0.99658203125, 0.9384765625, 0.6533203125, 0.544921875, 0.05810546875, 0.94384765625, 0.04248046875, 0.04052734375, 0.8916015625, 0.9970703125, 0.28759765625, 0.998046875, 0.998046875, 0.99609375, 0.95849609375, 0.36474609375, 0.9990234375, 1.0, 0.04150390625, 0.9990234375, 0.0673828125, 0.99560546875, 0.92919921875, 0.93505859375, 0.61328125, 0.1982421875, 0.05322265625, 0.81982421875, 0.2265625, 0.78515625, 0.9072265625, 0.57373046875, 0.50390625, 0.44921875, 0.9345703125, 0.94873046875, 0.99169921875, 0.32080078125, 0.61669921875, 0.9833984375, 0.99365234375, 0.638671875, 1.0, 0.9990234375, 0.94970703125, 0.1240234375, 0.658203125, 0.974609375, 0.0087890625, 0.07373046875, 0.998046875, 0.58154296875, 0.896484375, 0.77734375, 0.521484375, 0.98779296875, 0.11376953125, 0.90380859375, 0.05517578125, 0.1611328125, 0.99951171875, 0.15087890625, 0.3193359375, 0.99462890625, 0.9921875, 0.9619140625, 0.7431640625, 0.96435546875]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:36<00:00,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00439453125, 0.0009765625, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.00048828125, 0.00439453125, 0.0048828125, 0.00048828125, 0.009765625, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.455078125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.0146484375, 0.00146484375, 0.00146484375, 0.0, 0.0, 0.0009765625, 0.0078125, 0.0, 0.0107421875, 0.0009765625, 0.60400390625, 0.0, 0.00390625, 0.013671875, 0.0, 0.0, 0.0, 0.310546875, 0.017578125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09716796875, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.03955078125, 0.0, 0.0, 0.01025390625, 0.0, 0.0068359375, 0.00341796875, 0.06201171875, 0.0, 0.01513671875, 0.0, 0.01171875, 0.30419921875, 0.00244140625, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0146484375, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.00048828125, 0.0029296875, 0.0048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.02294921875, 0.0029296875, 0.1982421875, 0.0009765625, 0.06396484375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.06884765625, 0.00048828125, 0.0185546875, 0.0009765625, 0.0, 0.0, 0.001953125, 0.0, 0.00244140625, 0.0, 0.0, 0.55810546875, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.03466796875, 0.001953125, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0078125, 0.9794921875, 0.00439453125, 0.0, 0.00048828125, 0.0341796875, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0283203125, 0.0, 0.00146484375, 0.0009765625, 0.0087890625, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.00634765625, 0.00439453125, 0.0009765625, 0.0, 0.0, 0.00390625, 0.0, 0.0029296875, 0.00048828125, 0.0, 0.02783203125, 0.0, 0.0009765625, 0.0, 0.0, 0.033203125, 0.0, 0.0009765625, 0.0, 0.00732421875, 0.0, 0.0, 0.0, 0.06298828125, 0.08447265625, 0.0, 0.0, 0.0, 0.0029296875, 0.00048828125, 0.0498046875, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0634765625, 0.00244140625, 0.0, 0.0009765625, 0.0029296875, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.01220703125, 0.0, 0.03076171875, 0.01416015625, 0.00732421875, 0.00048828125, 0.01318359375, 0.0, 0.04296875, 0.0, 0.0, 0.0, 0.12109375, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.0009765625, 0.0029296875, 0.041015625, 0.0, 0.0, 0.00341796875, 0.1669921875, 0.00048828125, 0.00146484375, 0.0, 0.00048828125, 0.0, 0.00244140625, 0.00048828125, 0.00048828125, 0.00048828125, 0.0, 0.142578125, 0.0, 0.0, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.00244140625, 0.0, 0.0, 0.0029296875, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.0009765625, 0.06884765625, 0.0, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0, 0.0, 0.01318359375, 0.01513671875, 0.01904296875, 0.0009765625, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00146484375, 0.0009765625, 0.00048828125, 0.00048828125, 0.0009765625, 0.00390625, 0.00244140625, 0.00048828125, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.02294921875, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.00830078125, 0.00048828125, 0.0, 0.0, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.26416015625, 0.28955078125, 0.0, 0.0, 0.0, 0.0732421875, 0.0009765625, 0.0009765625, 0.0009765625, 0.12939453125, 0.001953125, 0.00341796875, 0.0, 0.00048828125, 0.00048828125, 0.083984375, 0.0, 0.0, 0.03125, 0.0029296875, 0.0, 0.0029296875, 0.00537109375, 0.0, 0.00927734375, 0.00048828125, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.30224609375, 0.0, 0.0, 0.00927734375, 0.0, 0.00048828125, 0.11767578125, 0.0, 0.0, 0.0029296875, 0.00244140625, 0.00341796875, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0185546875, 0.0, 0.00048828125, 0.0009765625, 0.0, 0.00048828125, 0.06689453125, 0.00048828125, 0.00537109375, 0.0048828125, 0.0009765625, 0.0029296875, 0.00048828125, 0.00244140625, 0.0, 0.0634765625, 0.0, 0.00048828125, 0.005859375, 0.0, 0.00732421875, 0.0078125, 0.00341796875, 0.0, 0.0, 0.0087890625, 0.0009765625, 0.0009765625, 0.0, 0.0, 0.0, 0.013671875, 0.02392578125, 0.0, 0.00537109375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.00830078125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00048828125, 0.0, 0.00341796875, 0.00244140625, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.01806640625, 0.0, 0.0, 0.0029296875, 0.0048828125, 0.01220703125, 0.0, 0.00244140625, 0.0, 0.05712890625, 0.0, 0.0087890625, 0.00048828125, 0.0263671875, 0.03369140625, 0.1337890625, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0009765625, 0.0205078125, 0.0, 0.0087890625, 0.02587890625, 0.00146484375, 0.1943359375, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.00048828125, 0.0087890625, 0.0, 0.0, 0.33447265625, 0.0078125, 0.0, 0.25927734375, 0.0, 0.0, 0.0, 0.00048828125, 0.0048828125, 0.0458984375, 0.0068359375, 0.0, 0.00537109375, 0.0, 0.00634765625, 0.0, 0.0, 0.0, 0.0322265625, 0.0, 0.0, 0.0, 0.357421875, 0.0, 0.01318359375, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.04541015625, 0.0, 0.0, 0.00146484375, 0.00244140625, 0.00927734375, 0.0009765625, 0.00244140625, 0.0, 0.037109375, 0.09619140625, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.00830078125, 0.00341796875, 0.0, 0.0, 0.03955078125, 0.00439453125, 0.0, 0.00244140625, 0.00439453125, 0.03271484375, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.00341796875, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.001953125, 0.0, 0.08349609375, 0.0, 0.0, 0.0, 0.01318359375, 0.07763671875, 0.001953125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.06640625, 0.0, 0.09423828125, 0.0126953125, 0.00048828125, 0.00048828125, 0.0244140625, 0.0, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0029296875, 0.00146484375, 0.00048828125, 0.0, 0.0, 0.0, 0.0009765625, 0.001953125, 0.01806640625, 0.00390625, 0.01123046875, 0.49609375, 0.0, 0.0, 0.0, 0.05029296875, 0.00439453125, 0.0, 0.0, 0.00390625, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.00048828125, 0.0, 0.0, 0.0068359375, 0.00048828125, 0.0009765625, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00048828125, 0.099609375, 0.0, 0.0009765625, 0.0, 0.0, 0.0, 0.00048828125, 0.0009765625, 0.03173828125, 0.0, 0.0, 0.08203125, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00244140625, 0.00048828125, 0.0, 0.001953125, 0.1767578125, 0.00048828125, 0.0, 0.0, 0.009765625, 0.00146484375, 0.00244140625, 0.0, 0.00341796875, 0.0, 0.064453125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.00048828125, 0.001953125, 0.00244140625, 0.3720703125, 0.13232421875, 0.0, 0.55810546875, 0.00244140625, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00927734375, 0.00048828125, 0.00048828125, 0.001953125, 0.0, 0.0205078125, 0.0, 0.33642578125, 0.0, 0.005859375, 0.4814453125, 0.0009765625, 0.0, 0.00048828125, 0.0, 0.12939453125, 0.00146484375, 0.0, 0.001953125, 0.00048828125, 0.0, 0.00244140625, 0.0, 0.0009765625, 0.00048828125, 0.0, 0.00048828125, 0.00048828125, 0.0, 0.0, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00537109375, 0.00048828125, 0.00244140625, 0.0, 0.00048828125, 0.16015625, 0.0, 0.0, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.04296875, 0.00048828125, 0.0, 0.4453125, 0.0, 0.0, 0.0, 0.0, 0.00830078125, 0.0283203125, 0.01318359375, 0.009765625, 0.00048828125, 0.0, 0.0029296875, 0.0, 0.0, 0.01171875, 0.0, 0.0029296875, 0.01513671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00146484375, 0.0, 0.0, 0.025390625, 0.00146484375, 0.00048828125, 0.00048828125, 0.0, 0.00439453125, 0.0, 0.00146484375, 0.0078125, 0.0029296875, 0.00244140625, 0.00244140625, 0.04296875, 0.533203125, 0.46875, 0.0, 0.01171875, 0.0, 0.0, 0.001953125, 0.0029296875, 0.0, 0.00146484375, 0.0, 0.00146484375, 0.00146484375, 0.0, 0.00048828125, 0.00146484375, 0.00341796875, 0.0, 0.00048828125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00341796875, 0.00048828125, 0.0, 0.0, 0.0390625, 0.0, 0.00634765625, 0.0, 0.0, 0.00244140625, 0.0, 0.0, 0.03466796875, 0.0, 0.00537109375, 0.0, 0.0029296875, 0.2392578125, 0.0, 0.0546875, 0.0, 0.0, 0.0, 0.36279296875, 0.0, 0.00048828125, 0.0029296875, 0.00244140625, 0.005859375, 0.0, 0.0, 0.00146484375, 0.00390625, 0.0, 0.0009765625, 0.0, 0.00048828125, 0.001953125, 0.0, 0.0, 0.00048828125, 0.0, 0.01513671875, 0.001953125, 0.00048828125, 0.00048828125, 0.00341796875, 0.05712890625, 0.0, 0.0, 0.0, 0.06640625, 0.0, 0.00537109375, 0.0, 0.0, 0.0400390625, 0.095703125, 0.00048828125, 0.0, 0.0, 0.0, 0.06494140625, 0.0, 0.0009765625, 0.01611328125, 0.0009765625, 0.01318359375, 0.0, 0.0, 0.0, 0.0, 0.00048828125, 0.0, 0.0, 0.00146484375, 0.0, 0.001953125, 0.00146484375, 0.00146484375, 0.0029296875, 0.01318359375, 0.00341796875, 0.0, 0.0, 0.0, 0.0009765625, 0.00146484375, 0.0185546875, 0.00146484375, 0.0029296875, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.01025390625, 0.0, 0.27490234375, 0.00537109375, 0.08251953125, 0.00048828125, 0.0, 0.0, 0.0, 0.0, 0.00634765625, 0.0029296875, 0.00048828125, 0.0, 0.0, 0.0302734375, 0.00048828125, 0.0, 0.00048828125, 0.21923828125, 0.0, 0.001953125, 0.0068359375, 0.0, 0.001953125, 0.0341796875, 0.00390625, 0.0, 0.0, 0.0009765625, 0.0, 0.00634765625, 0.00244140625, 0.0, 0.00048828125, 0.0, 0.0009765625, 0.0009765625, 0.87255859375, 0.05419921875, 0.0, 0.0, 0.0654296875, 0.00048828125, 0.00244140625, 0.0, 0.0, 0.00048828125, 0.0, 0.00048828125, 0.0, 0.0029296875, 0.00048828125, 0.00146484375, 0.0771484375, 0.00927734375, 0.00146484375, 0.0, 0.0, 0.0, 0.68603515625, 0.00048828125, 0.00244140625, 0.00048828125, 0.0048828125, 0.0009765625, 0.0087890625, 0.0, 0.00048828125, 0.27490234375, 0.0, 0.0, 0.1357421875, 0.00830078125, 0.0009765625, 0.00390625, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.02001953125, 0.0, 0.00048828125, 0.00537109375, 0.00048828125, 0.0, 0.0, 0.00439453125, 0.00244140625, 0.0, 0.0, 0.01318359375, 0.0, 0.01318359375, 0.00341796875, 0.8896484375, 0.796875, 0.19580078125, 0.1826171875, 0.9248046875, 0.37939453125, 0.99658203125, 0.99951171875, 0.99560546875, 0.9892578125, 0.9990234375, 0.92236328125, 0.951171875, 0.74462890625, 0.1318359375, 0.75341796875, 0.18603515625, 0.97802734375, 0.0107421875, 0.2421875, 0.93310546875, 0.16357421875, 0.95654296875, 0.072265625, 0.18017578125, 0.03271484375, 0.99951171875, 0.65673828125, 0.9931640625, 0.14599609375, 0.99755859375, 0.50390625, 0.00146484375, 0.662109375, 1.0, 0.91796875, 0.203125, 0.94580078125, 0.92578125, 0.96044921875, 0.25390625, 0.27197265625, 0.99853515625, 0.3564453125, 0.01318359375, 0.89111328125, 0.9736328125, 0.6943359375, 0.546875, 1.0, 0.501953125, 0.89990234375, 0.7705078125, 0.99853515625, 0.767578125, 0.0009765625, 0.0224609375, 0.880859375, 0.69921875, 0.86865234375, 0.99755859375, 0.9951171875, 0.994140625, 0.60595703125, 0.0302734375, 0.86328125, 0.8154296875, 0.92919921875, 0.88623046875, 0.291015625, 0.83544921875, 0.51171875, 0.7607421875, 0.638671875, 0.99267578125, 0.71875, 0.86474609375, 0.908203125, 0.97216796875, 0.7646484375, 0.99072265625, 0.35205078125, 0.9990234375, 0.33349609375, 1.0, 0.7880859375, 0.0, 0.0087890625, 0.9541015625, 0.42822265625, 0.341796875, 0.998046875, 0.02587890625, 0.8232421875, 0.93505859375, 0.11669921875, 0.9990234375, 0.189453125, 0.9326171875, 0.00048828125, 0.96826171875, 0.763671875, 0.4111328125, 0.9619140625, 0.12744140625, 0.93505859375, 0.7958984375, 0.89697265625, 0.06591796875, 0.9990234375, 0.75048828125, 0.96923828125, 0.4111328125, 0.5908203125, 0.509765625, 0.56005859375, 0.94384765625, 0.90380859375, 0.99658203125, 0.93017578125, 0.21337890625, 0.189453125, 0.00439453125, 0.96728515625, 0.884765625, 0.05419921875, 0.43212890625, 0.9765625, 0.04833984375, 0.89404296875, 0.82666015625, 0.7548828125, 0.0888671875, 0.00341796875, 0.99951171875, 0.98828125, 0.99853515625, 0.0341796875, 0.98193359375, 0.79052734375, 0.51171875, 0.9990234375, 0.89306640625, 0.986328125, 0.88427734375, 0.98828125, 0.951171875, 0.806640625, 0.505859375, 0.09814453125, 0.388671875, 0.9990234375, 0.96728515625, 0.99951171875, 0.79052734375, 0.9990234375, 0.998046875, 1.0, 0.99853515625, 0.9970703125, 0.9541015625, 0.0888671875, 0.001953125, 0.8671875, 0.9541015625, 0.99853515625, 0.99951171875, 0.98779296875, 0.111328125, 0.72802734375, 0.0849609375, 0.9345703125, 0.93603515625, 0.97021484375, 0.009765625, 0.00048828125, 0.97900390625, 0.9951171875, 0.01904296875, 0.74169921875, 0.9677734375, 0.97509765625, 0.392578125, 0.77734375, 0.91162109375, 0.9775390625, 1.0, 0.9775390625, 1.0, 0.77197265625, 0.99072265625, 0.17236328125, 0.93212890625, 0.1240234375, 0.533203125, 0.955078125, 0.91943359375, 0.451171875, 0.3056640625, 0.0, 0.998046875, 0.99462890625, 0.99462890625, 0.98828125, 0.51171875, 0.9091796875, 0.9990234375, 0.923828125, 0.2392578125, 0.8154296875, 0.63330078125, 0.974609375, 0.525390625, 0.9189453125, 0.99853515625, 0.01611328125, 0.9814453125, 0.99072265625, 0.00390625, 0.14892578125, 0.95947265625, 0.015625, 0.95361328125, 0.14111328125, 0.43798828125, 0.640625, 0.9970703125, 0.93310546875, 0.998046875, 0.98876953125, 0.93994140625, 0.98046875, 0.01025390625, 0.9833984375, 0.93896484375, 0.91259765625, 1.0, 0.845703125, 0.99951171875, 0.7265625, 0.99951171875, 0.85986328125, 0.0009765625, 0.9794921875, 0.3740234375, 0.2265625, 0.98388671875, 0.99560546875, 0.60009765625, 0.9912109375, 0.0380859375, 0.00048828125, 0.8916015625, 1.0, 0.0849609375, 0.556640625, 1.0, 0.08447265625, 1.0, 1.0, 0.92724609375, 0.94580078125, 0.1376953125, 0.0615234375, 0.498046875, 0.29443359375, 0.921875, 0.986328125, 0.9638671875, 0.451171875, 0.9375, 0.02099609375, 0.083984375, 0.87890625, 1.0, 0.96826171875, 0.95751953125, 0.65478515625, 0.80078125, 0.60205078125, 0.99169921875, 0.73583984375, 0.99951171875, 0.9736328125, 0.99267578125, 0.1259765625, 0.3486328125, 0.99755859375, 0.10888671875, 0.17333984375, 0.1044921875, 0.81884765625, 0.001953125, 0.2255859375, 1.0, 0.85107421875, 0.99658203125, 0.85595703125, 0.0947265625, 0.9638671875, 0.9638671875, 0.98388671875, 0.9970703125, 0.57958984375, 0.02587890625, 0.7744140625, 0.80419921875, 0.8642578125, 1.0, 0.99658203125, 0.974609375, 0.0322265625, 0.6689453125, 0.97998046875, 0.505859375, 0.9921875, 0.96875, 0.12744140625, 0.716796875, 0.00439453125, 0.845703125, 0.998046875, 0.93994140625, 0.93701171875, 0.94970703125, 0.0966796875, 0.91259765625, 0.04345703125, 0.99951171875, 0.57763671875, 0.77880859375, 1.0, 0.05322265625, 0.9111328125, 0.9970703125, 0.86083984375, 0.99462890625, 0.1611328125, 0.56005859375, 0.1572265625, 0.57958984375, 0.9873046875, 0.99951171875, 0.537109375, 0.99853515625, 0.9970703125, 0.9990234375, 0.99951171875, 0.5849609375, 0.8017578125, 0.97216796875, 0.83349609375, 0.98095703125, 0.0537109375, 0.8173828125, 0.990234375, 0.05810546875, 0.9267578125, 0.09423828125, 0.54296875, 1.0, 0.05810546875, 0.73876953125, 1.0, 0.9794921875, 0.0751953125, 0.00146484375, 0.0927734375, 0.7216796875, 0.00732421875, 0.00927734375, 0.91552734375, 0.98681640625, 0.9833984375, 0.5390625, 0.9970703125, 0.83642578125, 0.9072265625, 0.9970703125, 0.00732421875, 0.390625, 0.99072265625, 0.513671875, 0.31396484375, 1.0, 0.91259765625, 0.970703125, 0.2353515625, 0.39990234375, 0.03515625, 0.95703125, 0.95751953125, 0.85888671875, 0.9765625, 0.01220703125, 0.99072265625, 0.99462890625, 0.99365234375, 0.3681640625, 0.88330078125, 0.966796875, 0.87060546875, 0.9921875, 0.03271484375, 0.93896484375, 0.982421875, 1.0, 0.85888671875, 0.28955078125, 0.13134765625, 0.0966796875, 0.94580078125, 0.38330078125, 0.26416015625, 0.96875, 0.75341796875, 0.10107421875, 0.76904296875, 0.99853515625, 0.93994140625, 0.75048828125, 0.04833984375, 0.458984375, 0.06884765625, 0.43212890625, 0.00244140625, 0.9892578125, 0.9970703125, 0.232421875, 0.36279296875, 0.966796875, 0.93603515625, 0.9365234375, 0.24365234375, 0.28466796875, 0.9736328125, 0.00732421875, 0.845703125, 0.392578125, 0.7958984375, 0.9833984375, 0.0322265625, 0.96142578125, 0.50390625, 0.98974609375, 0.67236328125, 0.99072265625, 0.69287109375, 0.56591796875, 0.13037109375, 0.04150390625, 1.0, 0.7294921875, 0.97705078125, 0.9287109375, 0.8427734375, 0.6259765625, 0.9853515625, 0.990234375, 0.861328125, 0.515625, 0.91064453125, 0.82421875, 0.95361328125, 0.9892578125, 0.99609375, 0.9931640625, 0.0087890625, 0.99951171875, 0.8671875, 0.96728515625, 0.9912109375, 0.9990234375, 0.01416015625, 0.9921875, 0.474609375, 0.9521484375, 0.8974609375, 0.9931640625, 0.021484375, 0.865234375, 0.00439453125, 0.01953125, 1.0, 0.15625, 0.98828125, 0.83203125, 0.12353515625, 0.982421875, 0.1181640625, 0.57373046875, 0.9990234375, 0.16162109375, 0.994140625, 0.99951171875, 0.99169921875, 0.642578125, 0.1376953125, 0.998046875, 0.96337890625, 0.931640625, 0.9990234375, 0.03173828125, 0.98486328125, 0.99462890625, 0.998046875, 0.9873046875, 0.001953125, 0.43603515625, 0.76220703125, 0.56982421875, 0.61669921875, 0.99951171875, 0.97412109375, 0.39697265625, 0.69580078125, 0.87255859375, 0.13671875, 0.865234375, 0.841796875, 0.0400390625, 0.064453125, 0.015625, 0.998046875, 0.4052734375, 0.98828125, 0.4921875, 0.99853515625, 0.9677734375, 0.78271484375, 0.9326171875, 0.9990234375, 0.43212890625, 0.986328125, 0.9716796875, 0.455078125, 0.267578125, 0.99365234375, 1.0, 0.22021484375, 0.12158203125, 0.58154296875, 0.810546875, 0.73388671875, 0.01611328125, 0.99658203125, 0.01904296875, 0.9482421875, 0.01708984375, 0.958984375, 0.45703125, 0.70068359375, 0.99951171875, 0.2958984375, 1.0, 0.99853515625, 0.998046875, 0.9931640625, 0.953125, 0.958984375, 0.98974609375, 0.291015625, 0.501953125, 0.94873046875, 0.71533203125, 0.7744140625, 0.97900390625, 0.96337890625, 0.49609375, 0.072265625, 0.53125, 0.9609375, 0.5869140625, 0.96923828125, 1.0, 0.013671875, 0.865234375, 0.021484375, 0.9111328125, 0.947265625, 0.9921875, 0.0244140625, 0.796875, 0.91650390625, 1.0, 1.0, 1.0, 0.99951171875, 0.5927734375, 0.99951171875, 0.82763671875, 0.99755859375, 0.99462890625, 0.986328125, 0.8447265625, 0.99951171875, 1.0, 0.9248046875, 1.0, 0.04443359375, 0.9873046875, 0.9970703125, 0.0009765625, 0.998046875, 0.00390625, 1.0, 0.00390625, 0.814453125, 0.7607421875, 0.92041015625, 0.9833984375, 0.85400390625, 0.9501953125, 0.994140625, 0.998046875, 0.9921875, 0.68408203125, 0.7216796875, 0.15185546875, 0.01416015625, 0.18359375, 0.97021484375, 0.505859375, 0.541015625, 0.18701171875, 0.6826171875, 0.00048828125, 1.0, 0.99365234375, 1.0, 0.99609375, 0.99609375, 0.91748046875, 0.4130859375, 0.90478515625, 0.42431640625, 0.466796875, 0.08251953125, 0.95751953125, 0.0126953125, 0.89013671875, 0.12841796875, 0.1142578125, 0.00341796875, 0.541015625, 0.8017578125, 0.541015625, 0.9951171875, 0.76904296875, 0.97314453125, 0.99609375, 1.0, 0.3330078125, 0.986328125, 0.71240234375, 1.0, 0.984375, 0.068359375, 0.90185546875, 0.9091796875, 0.998046875, 0.99853515625, 0.013671875, 0.83544921875, 0.7890625, 0.14697265625, 0.06494140625, 0.35205078125, 0.43017578125, 0.32763671875, 0.947265625, 0.00830078125, 0.36474609375, 0.203125, 0.43408203125, 0.8427734375, 1.0, 0.7705078125, 0.01904296875, 0.39990234375, 0.9384765625, 0.921875, 0.640625, 0.95166015625, 0.97265625, 0.99658203125, 0.9619140625, 0.93994140625, 0.9833984375, 0.06787109375, 0.7880859375, 0.99951171875, 1.0, 0.4072265625, 0.09716796875, 0.96728515625, 0.9404296875, 0.4150390625, 0.0048828125, 0.337890625, 1.0, 0.81640625, 0.29931640625, 0.998046875, 0.99853515625, 0.59619140625, 0.1357421875, 1.0, 0.990234375, 0.1015625, 0.93115234375, 0.927734375, 0.00927734375, 0.27197265625, 0.14501953125, 1.0, 0.91552734375, 0.71875, 0.61865234375, 0.078125, 0.98046875, 0.14697265625, 0.99169921875, 0.064453125, 0.25537109375, 0.732421875, 0.99853515625, 1.0, 0.1689453125, 0.966796875, 0.05126953125, 0.01708984375, 0.04345703125, 0.9833984375, 0.9814453125, 0.9833984375, 0.99462890625, 0.001953125, 1.0, 0.9970703125, 0.4150390625, 0.99755859375, 0.9091796875, 0.93994140625, 0.99951171875, 0.99951171875, 0.04345703125, 0.9755859375, 0.67919921875, 0.025390625, 0.52734375, 0.98779296875, 0.001953125, 1.0, 0.00537109375, 0.00439453125, 0.8818359375, 1.0, 0.103515625, 0.1240234375, 0.9970703125, 1.0, 0.15283203125, 0.74462890625, 0.998046875, 1.0, 0.42822265625, 0.12353515625, 0.923828125, 0.029296875, 0.99560546875, 0.16796875, 0.9990234375, 0.03076171875, 0.9921875, 0.82080078125, 0.0986328125, 0.9990234375, 0.02685546875, 0.99951171875, 0.9306640625, 0.0400390625, 0.01220703125, 0.93505859375, 0.9990234375, 0.01025390625, 0.0087890625, 0.8115234375, 0.7265625, 0.7431640625, 0.88427734375, 0.9990234375, 0.8916015625, 0.61669921875, 0.99951171875, 0.92529296875, 0.56591796875, 0.32080078125, 0.9990234375, 0.99169921875, 0.99951171875, 0.98291015625, 0.15283203125, 0.19189453125, 0.99609375, 0.20947265625, 0.9287109375, 0.08837890625, 0.99951171875, 0.0146484375, 1.0, 0.00048828125, 0.96826171875, 0.9990234375, 0.0966796875, 0.14111328125, 0.552734375, 0.18017578125, 1.0, 0.99072265625, 0.70263671875, 0.732421875, 0.00732421875, 0.98876953125, 0.24072265625, 0.98828125, 0.236328125, 0.05517578125, 0.01904296875, 0.96044921875, 0.20166015625, 0.9931640625, 0.99658203125, 0.9384765625, 0.6533203125, 0.544921875, 0.05810546875, 0.94384765625, 0.04248046875, 0.04052734375, 0.8916015625, 0.9970703125, 0.28759765625, 0.998046875, 0.998046875, 0.99609375, 0.95849609375, 0.36474609375, 0.9990234375, 1.0, 0.04150390625, 0.9990234375, 0.0673828125, 0.99560546875, 0.92919921875, 0.93505859375, 0.61328125, 0.1982421875, 0.05322265625, 0.81982421875, 0.2265625, 0.78515625, 0.9072265625, 0.57373046875, 0.50390625, 0.44921875, 0.9345703125, 0.94873046875, 0.99169921875, 0.32080078125, 0.61669921875, 0.9833984375, 0.99365234375, 0.638671875, 1.0, 0.9990234375, 0.94970703125, 0.1240234375, 0.658203125, 0.974609375, 0.0087890625, 0.07373046875, 0.998046875, 0.58154296875, 0.896484375, 0.77734375, 0.521484375, 0.98779296875, 0.11376953125, 0.90380859375, 0.05517578125, 0.1611328125, 0.99951171875, 0.15087890625, 0.3193359375, 0.99462890625, 0.9921875, 0.9619140625, 0.7431640625, 0.96435546875, 0.00927734375, 0.89501953125, 0.998046875, 0.38134765625, 0.9775390625, 0.85595703125, 0.97265625, 0.9990234375, 0.99755859375, 0.0673828125, 0.75927734375, 0.99853515625, 0.505859375, 0.99951171875, 0.99951171875, 0.97705078125, 0.77734375, 0.9560546875, 0.96484375, 0.11181640625, 0.9970703125, 0.17578125, 0.998046875, 0.2626953125, 0.98974609375, 0.76611328125, 0.998046875, 0.97265625, 0.8759765625, 0.955078125, 0.94970703125, 0.986328125, 0.7890625, 0.822265625, 0.00048828125, 0.9990234375, 0.0361328125, 0.67578125, 0.16015625, 0.80908203125, 0.99951171875, 0.9658203125, 0.7490234375, 0.99267578125, 0.0009765625, 0.97119140625, 0.0009765625, 0.99951171875, 0.9892578125, 0.998046875, 0.9990234375, 0.86865234375, 0.99365234375, 0.99951171875, 0.99951171875, 0.11767578125, 0.99560546875, 0.99658203125, 0.375, 0.8681640625, 0.9873046875, 0.984375, 0.9853515625, 0.03271484375, 0.2451171875, 0.0, 0.998046875, 0.06689453125, 1.0, 0.9970703125, 0.85595703125, 0.75634765625, 0.99951171875, 0.6533203125, 0.99951171875, 0.1259765625, 0.8955078125, 0.9873046875, 0.2705078125, 0.8115234375, 0.99169921875, 0.99609375, 1.0, 0.0, 0.990234375, 0.486328125, 0.91748046875, 0.9873046875, 0.787109375, 0.91845703125, 0.99072265625, 0.0234375, 0.26611328125, 1.0, 0.998046875, 0.70751953125, 0.27978515625, 0.3486328125, 1.0, 0.982421875]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:37<00:00,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 2,000\n",
      "* correct: 1,686\n",
      "* average_precision: 97.4%\n",
      "* accuracy: 84.3%\n",
      "* error: 15.7%\n",
      "* macro_f1: 83.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results, results_dict = trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21e5a543-dc15-4f5d-9314-c1781439333d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('accuracy', 78.44699777613047),\n",
       "             ('error_rate', 21.553002223869527),\n",
       "             ('macro_f1', 77.47484357151382),\n",
       "             ('average_precision', 92.59029414434582)])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dda6b947-2a22-4bed-a970-4ac1192329e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = args.model_dir.split('/')[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1d2ceeb-e998-4971-9907-a499818c9677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store model evaluations\n",
    "model_evaluations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "143af3df-b47f-47db-a952-b2d0a9dae7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def update_and_save_evaluation(model_name, dataset_name, accuracy, f1_score, average_precision):\n",
    "    global model_evaluations\n",
    "    \n",
    "    # Check if the model key exists in the dictionary\n",
    "    if model_name not in model_evaluations:\n",
    "        model_evaluations[model_name] = {}\n",
    "    \n",
    "    # Check if the dataset key exists for the given model\n",
    "    if dataset_name not in model_evaluations[model_name]:\n",
    "        model_evaluations[model_name][dataset_name] = {}\n",
    "    \n",
    "    # Update evaluation results\n",
    "    model_evaluations[model_name][dataset_name][\"accuracy\"] = accuracy\n",
    "    model_evaluations[model_name][dataset_name][\"f1_score\"] = f1_score\n",
    "    model_evaluations[model_name][dataset_name][\"average_precision\"] = average_precision\n",
    "    \n",
    "    # Save the updated dictionary to a JSON file\n",
    "    with open(\"model_evaluations.json\", \"w\") as json_file:\n",
    "        json.dump(model_evaluations, json_file, indent=2)\n",
    "\n",
    "# # Example usage after evaluating a model\n",
    "# update_and_save_evaluation(\"model1\", \"dataset1\", 0.85, 0.78, 0.92)\n",
    "# update_and_save_evaluation(\"model1\", \"dataset2\", 0.92, 0.89, 0.95)\n",
    "\n",
    "# update_and_save_evaluation(\"model2\", \"dataset1\", 0.78, 0.67, 0.88)\n",
    "# update_and_save_evaluation(\"model2\", \"dataset2\", 0.91, 0.87, 0.93)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12c55e6a-2138-4f6b-bdce-59dac5b91f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_and_save_evaluation(cfg['TRAINER']['NAME']+str('_')+model_name, 'gaugan', results_dict['accuracy'], results_dict['macro_f1'], results_dict['average_precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "294ec445-a71e-4d69-8d5c-cd8d82184f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 99.65,\n",
       " 'f1_score': 99.64999571244748,\n",
       " 'average_precision': 99.99919799075577}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_evaluations['CoOp_100000_16context_best_until_now']['gaugan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cae19db6-6826-4e8d-a679-609718417802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CoOp'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg['TRAINER']['NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c711fc9d-bfad-4a57-834c-331cf17d2443",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = trainer.model(torch.rand(1, 3, 224, 224).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5611423-b173-46cb-875b-64ed8e4ad7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13.9297, 14.6406]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ef6c25f-440c-41cc-830a-1851d8edfc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    Resize, Compose, ToTensor, Normalize, CenterCrop, RandomCrop, ColorJitter,\n",
    "    RandomApply, GaussianBlur, RandomGrayscale, RandomResizedCrop,\n",
    "    RandomHorizontalFlip\n",
    ")\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "INTERPOLATION_MODES = {\n",
    "    \"bilinear\": InterpolationMode.BILINEAR,\n",
    "    \"bicubic\": InterpolationMode.BICUBIC,\n",
    "    \"nearest\": InterpolationMode.NEAREST,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5811801e-09a9-4437-a485-f05af007dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_transform_test(cfg, choices):\n",
    "    print(\"Building transform_test\")\n",
    "    tfm_test = []\n",
    "    target_size = 224\n",
    "    interp_mode = INTERPOLATION_MODES[cfg.INPUT.INTERPOLATION]\n",
    "    input_size = cfg.INPUT.SIZE\n",
    "\n",
    "    print(f\"+ resize the smaller edge to {max(input_size)}\")\n",
    "    tfm_test += [Resize(max(input_size), interpolation=interp_mode)]\n",
    "\n",
    "    print(f\"+ {target_size} center crop\")\n",
    "    tfm_test += [CenterCrop(input_size)]\n",
    "\n",
    "    print(\"+ to torch tensor of range [0, 1]\")\n",
    "    tfm_test += [ToTensor()]\n",
    "    \n",
    "    normalize = Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "\n",
    "    if \"normalize\" in choices:\n",
    "        print(\n",
    "            f\"+ normalization (mean={cfg.INPUT.PIXEL_MEAN}, std={cfg.INPUT.PIXEL_STD})\"\n",
    "        )\n",
    "        tfm_test += [normalize]\n",
    "\n",
    "    tfm_test = Compose(tfm_test)\n",
    "\n",
    "    return tfm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eae3d9f0-f6ed-47e3-afa7-69cabe4c2bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building transform_test\n",
    "# + resize the smaller edge to 224\n",
    "# + 224x224 center crop\n",
    "# + to torch tensor of range [0, 1]\n",
    "# + normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e928600-310c-4ddd-a065-182be6f58811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n"
     ]
    }
   ],
   "source": [
    "tfms = _build_transform_test(cfg, choices=['normalize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b204fa9e-f076-43b5-8487-4043dd33898f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomCLIP(\n",
       "  (prompt_learner): PromptLearner()\n",
       "  (image_encoder): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "    (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (16): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (17): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (18): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (19): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (20): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (21): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (22): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (23): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_encoder): TextEncoder(\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3d3363a-a5c3-4772-8766-b6cd69239ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2698\n",
      "2698\n",
      "tensor([[16.1250,  9.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.0469,  9.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.0938,  9.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.4219, 11.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5703, 10.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2578,  9.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7812,  9.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9688,  7.2148]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0781,  8.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7422,  9.0469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.4844, 6.7383]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.7969, 5.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.5781, 6.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.5977, 7.7188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0234,  6.5273]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1172,  9.7031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1484,  9.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3203,  9.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.7500,  8.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6016,  8.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3203,  9.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4531, 10.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2734,  9.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3828,  9.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2734,  8.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1875,  8.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1719,  8.9453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4531,  8.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6719,  9.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.8516,  9.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0156,  8.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2578,  9.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.1250, 11.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8750,  7.3633]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0469,  7.4102]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3672,  5.7383]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2969,  6.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7500,  6.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0703,  8.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1562,  7.4023]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0781,  6.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2734, 11.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5703, 10.3828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3047, 11.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8438, 12.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6094, 12.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.5156, 6.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.3984, 3.8242]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.4023, 7.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1250,  8.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.8008, 4.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.7422, 6.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1172,  8.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2109,  9.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9297,  9.1484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7734,  8.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6562,  9.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2031,  9.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9531,  8.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4297,  8.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8047,  8.6484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.7734, 10.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1875,  8.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3047,  9.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8906,  9.3594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4688,  8.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4297,  9.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6328, 11.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6875, 10.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7266, 11.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9375,  7.7383]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.6094, 8.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.6797, 6.9648]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2734,  6.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9609, 5.6523]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 8.3672, 10.1641]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9531, 9.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.4219, 7.7930]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1719,  9.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0156,  9.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0469,  8.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9219,  8.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7422,  7.2773]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7031,  6.6211]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8203,  6.0469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7109,  8.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4531,  7.4023]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.6641, 7.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3359,  9.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.4062, 11.0469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.5859, 11.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7734, 11.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1953, 11.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0625, 12.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.8672, 11.3828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.7188, 13.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.8906, 10.6484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.0156, 11.6250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1250, 12.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.1250, 3.8398]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.0938, 3.0645]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2969,  6.3945]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.0469, 3.1523]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4219,  3.4102]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0703,  3.8789]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.2422, 6.9805]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.7891, 8.8672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.4844, 5.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.9180, 9.4922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.1094, 5.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.0781, 5.5586]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.4062, 5.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.3594, 6.1133]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.7891, 5.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.9531, 6.5586]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.9023, 6.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4688,  9.1484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8359,  8.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0625,  8.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7422,  8.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8750,  7.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.5859, 12.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.7188, 11.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.1719, 12.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.4062, 11.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.1719, 11.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6250, 10.3906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3984, 11.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.8359, 11.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3359, 10.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.1875, 12.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.9219, 11.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0859, 11.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.5078, 11.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.7734, 10.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.9453, 10.8750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6484,  9.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6172,  9.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5938,  8.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6328, 10.3828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1250,  9.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4219, 11.0469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5312, 10.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0625, 10.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8438, 11.7188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3672, 10.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3828, 10.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7734, 10.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.4062, 10.5859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.0625,  9.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.0625,  8.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.8594,  9.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.6016,  8.8359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.0078,  7.7695]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.1641,  6.7695]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8203, 10.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[19.3594, 13.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.6328,  9.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.0781, 11.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3516, 12.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5391,  7.3320]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5391,  8.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2188,  8.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2500,  9.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0469,  7.4258]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5156,  8.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2891,  8.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0547,  7.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6016,  7.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4609,  7.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6875,  8.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1953,  8.8984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8125,  7.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1562,  6.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4141,  7.8008]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4141,  7.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.8828, 7.7852]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8594,  8.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2344,  7.2578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7188,  6.6211]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3984,  7.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1406,  9.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3125,  7.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4453,  7.2773]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9609,  8.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8359,  8.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.6953, 8.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.5781, 8.6562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3047,  8.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.8594, 7.9336]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2344,  8.7188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9922,  9.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4688,  9.8359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4453, 10.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4766,  9.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5938,  8.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5391,  8.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8125,  8.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4219,  7.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4531, 10.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6484, 10.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7969,  9.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3984,  9.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9766,  9.8359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6953,  9.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5391,  8.8984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3594,  9.2578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2500,  8.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2969,  9.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7812,  8.8359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.6797, 10.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.8672,  9.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4141,  9.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4453,  9.5547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5234,  8.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1953,  8.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8906,  8.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3359,  7.2617]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2344,  8.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1406,  8.8750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6797,  6.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4922,  7.0586]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1953,  8.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2109,  8.3984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4688, 10.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3750,  9.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0781,  9.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6719, 10.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7500,  6.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8203,  8.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2500,  7.9492]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4609,  8.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0000,  8.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6484,  8.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7500,  8.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6641,  7.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7109,  9.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0781,  9.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8672,  9.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7031,  8.6562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6172,  9.1094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3359,  9.3359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7500,  7.6914]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5859,  9.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1719, 10.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3516,  8.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4453,  9.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7734, 10.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7656, 10.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6953,  8.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7422,  9.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3359,  8.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6328,  9.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5781,  8.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9062,  8.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9844,  8.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8359,  9.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2500,  9.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8281, 10.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0547, 10.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6172,  7.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7734, 10.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6719,  7.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4141, 11.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4531, 10.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9844,  8.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0547, 10.8438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7500, 11.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0078,  9.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6328,  8.8750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9062,  9.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1016, 10.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3516,  8.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0391,  9.4688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4219,  8.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3047,  8.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0391,  8.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0938,  7.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9688,  9.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3516,  9.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5547,  8.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2031,  9.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5312,  9.1484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7812,  8.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.7812, 8.8750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8984, 10.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2969, 10.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0156,  8.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9297, 8.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8281,  9.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7656,  7.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3047,  9.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6484,  9.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.7031, 9.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 9.4609, 10.0000]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8672,  9.0469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9219,  9.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3047,  9.6250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.4062,  9.4688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.5391,  9.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2031, 10.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8516,  8.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.9141,  9.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4531,  8.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5391,  8.8984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2109,  9.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0859,  8.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2969,  8.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3203,  8.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5859,  7.6602]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5234,  6.9023]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9922, 11.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5391, 10.1094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5703, 11.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6250, 11.5938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2500, 10.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.9688, 11.1094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4375, 10.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3281,  9.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2578, 10.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.6797, 10.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3047,  8.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3047,  8.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3594, 10.5547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0234,  9.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2656,  8.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.5312,  9.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.3906, 10.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.6641,  9.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.9375, 10.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3359, 12.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3281,  6.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9141,  6.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4609,  8.1094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0000,  7.4414]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.3281, 14.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[18.0625, 14.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[18.7500, 14.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5000,  9.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3125,  6.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.4609, 7.4688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.2656, 14.8438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.1875, 14.8438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0781,  7.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3672,  8.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2500,  8.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.5078, 7.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8281,  6.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1094,  8.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8828,  8.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.4766, 7.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1484,  9.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2109,  8.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.4453, 9.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9297, 8.2578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7500,  9.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8594,  9.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5312,  8.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3672,  9.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3984,  8.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5156, 10.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9609, 10.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.7422, 11.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1719,  9.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2188, 10.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7812,  9.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5234, 10.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1016, 12.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6719,  9.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1797,  9.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7500,  9.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2500,  5.5273]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.5000, 6.3086]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.4922, 7.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.1875, 7.3984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.4688, 8.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2188,  7.7695]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.5078, 7.9570]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2031,  6.5977]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.7500, 7.6836]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5391,  7.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8516,  6.7773]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3672,  8.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9141,  8.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1016, 10.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5469, 10.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2734,  9.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0391,  7.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8984,  8.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6875,  7.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4688,  6.6602]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4297,  7.9414]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1953,  8.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3594,  9.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4453,  8.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4297,  9.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.5156, 10.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4609,  9.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7422,  9.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3594, 10.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.1875,  9.9453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.5938,  9.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3672,  7.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.3906, 6.1523]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.1562,  9.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6250,  7.3359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2500,  4.6758]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0312,  6.8555]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2891,  5.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7188,  6.0195]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3984,  9.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2266,  7.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5078,  6.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6094,  7.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8359,  6.6680]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.9922, 8.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.2422, 7.8086]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.2344, 9.8984]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.5000, 9.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.9062, 8.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.0547, 7.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.5469, 9.5312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.2578, 9.4219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.2227, 9.7969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.6484, 8.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.1641, 7.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 8.8359, 10.3750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2578,  9.3984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3281, 10.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0000,  9.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3672,  9.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9609, 10.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2344,  9.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2344, 10.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.1055, 6.6758]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.5508, 5.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[5.6836, 4.8164]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[4.7070, 5.5508]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.1992, 5.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.8047, 4.3711]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.5703, 6.8633]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.8125, 5.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[6.3320, 5.9805]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[6.1836, 5.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6016,  6.2930]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9062,  6.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7031,  6.0977]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.2266,  8.7188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.8281,  7.1523]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.9062,  7.4648]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.1016,  7.6836]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.3281, 10.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.8594,  8.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.1562,  9.5547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.8281, 11.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.6719,  9.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1328, 11.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1484, 10.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.7656, 10.4688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4922,  9.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8438,  9.8359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5156,  9.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6719,  9.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5469,  9.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5547, 10.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2109, 10.6484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3281,  7.7539]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0938,  8.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1094,  9.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0938,  7.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6172,  9.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1094,  8.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3672,  9.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7578,  8.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6875, 10.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4766,  8.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8125,  8.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5000,  9.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0312, 13.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0234,  7.1367]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4531,  9.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7188,  8.3828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7969,  9.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9453,  7.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1875, 10.3906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.3828, 7.1094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.3906, 7.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5156,  7.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.0312, 7.6758]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6719,  7.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8984,  6.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.3047, 7.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4375,  7.9492]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0391,  6.6445]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5156,  8.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[18.4062,  9.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.2969, 11.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.5156, 12.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.2031, 11.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5781,  8.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9219,  9.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2422,  8.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2734,  8.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5703,  8.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0000,  8.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2344,  8.5859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1406,  9.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7812,  6.5898]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0000,  8.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5547,  8.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7266,  6.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9844,  6.4336]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6641,  5.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3750,  5.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1172,  4.6367]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9297,  5.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7344,  6.5938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4062,  5.5195]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9062,  5.9727]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0703, 10.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3828,  7.2461]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5859,  8.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3203,  7.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9453,  7.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5625, 11.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6875,  9.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2109,  9.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.2734, 10.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3047, 10.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4531,  8.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0703,  8.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0859,  8.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5469,  9.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.9609, 10.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4766,  9.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0938,  9.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6719, 10.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0312,  8.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2266, 10.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3359,  8.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9531,  6.2695]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8906,  7.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5781,  7.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7109,  7.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7812,  7.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8516,  5.8008]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2344,  7.1523]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7656,  6.9336]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6172,  6.3477]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3594,  5.4258]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8125,  6.0508]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7578,  6.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6016,  7.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9531, 10.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5547, 10.3984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9297, 11.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8203, 12.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9688, 11.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4844,  9.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2578, 10.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5938,  8.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3594,  9.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7812,  9.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3203,  8.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0703, 11.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8828, 12.5938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0625, 11.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5547,  8.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9375,  8.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8125,  8.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3594,  7.6289]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4688,  8.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7734,  8.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1484,  9.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0234,  7.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8125,  8.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3203,  7.3594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8438,  6.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6719,  8.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6797,  8.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5078, 10.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8047,  8.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0391, 10.3984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2812, 10.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8750,  8.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5156, 10.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4062,  8.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1250,  9.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0703, 10.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2656,  9.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3516,  9.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7656, 10.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0781,  9.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4766,  9.5859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9297,  9.6250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3047, 10.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5000, 10.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8203,  8.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.7422, 10.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2266, 10.7188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.7031, 11.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.1016, 7.6758]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5469,  7.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1953,  7.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5312,  8.3594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6016,  8.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6484,  8.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8359,  9.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7891,  8.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0234,  8.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0781,  8.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7734,  7.2930]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3828,  8.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.4531, 8.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.5234, 8.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0000,  7.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3828,  8.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2656,  7.6055]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2422,  8.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3594,  8.3359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6016,  8.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5781,  6.9570]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6719,  6.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0781,  7.0977]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1484,  8.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.7969, 6.7031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.6562, 7.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.5938, 6.9180]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.3828, 7.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.6641, 7.4570]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.7578, 7.5586]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.2656, 8.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.7109, 7.0664]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.8828, 6.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.1875, 7.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.6406, 6.2773]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.5938, 7.2578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9609,  7.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2188,  8.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5156,  8.0469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4688,  8.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.4219,  9.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.4375,  8.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8828,  7.2070]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3750,  6.9414]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1797,  8.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1406, 11.3984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4922,  9.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4453,  7.6055]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4531,  8.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5391,  7.1680]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1562,  8.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.1797,  7.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.7734,  8.2578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6172,  8.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7578,  7.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.8125,  9.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5234,  8.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6875,  8.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3281,  6.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5078,  6.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6484,  7.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.1094,  8.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2656,  8.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.8906,  8.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5078,  9.8359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.0156,  9.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3047,  8.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3750,  7.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3672,  8.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6172,  8.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3516,  8.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6641, 10.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.8750, 9.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6406,  9.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2969,  8.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.8125,  9.8359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.5000,  9.8438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.2969, 11.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.2344, 10.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[18.1250,  8.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.1250, 10.5938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.3281, 10.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.7031, 11.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.8984,  7.5664]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.1641,  8.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1641,  7.9727]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.8984,  8.3828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.0312,  9.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8828,  8.9219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1094, 10.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.9688,  8.3359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7344,  6.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 9.2812, 10.5859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0156,  8.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.8594, 8.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8672, 10.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0078, 11.2656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2656, 10.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6250, 10.6484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1875,  9.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9453,  9.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8672,  7.5742]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1406,  7.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8594,  6.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5938,  7.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1641,  7.8711]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5625,  8.6250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4766,  8.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7344,  6.9258]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5312,  7.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3203,  6.0352]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2266,  7.5586]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6562,  5.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8438,  5.1367]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7109,  7.5664]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8047,  6.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0781,  6.3633]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2422,  6.0273]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5000,  7.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6875,  5.7148]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5938,  8.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7344,  8.3906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5000,  7.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4688,  8.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8594,  8.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7734,  6.9648]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5547,  7.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0078,  9.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1328,  7.8984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0469,  7.0508]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0234,  8.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5156,  8.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7812,  7.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2500,  7.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6172,  7.5352]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.0000, 12.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6641,  9.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.9609, 12.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.6719, 11.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1406, 11.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.9453, 10.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3750, 12.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.6094, 12.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.2344, 11.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.2969, 13.1484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.5000, 10.8438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.5938, 11.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.5234, 10.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.6484,  9.3359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.2031,  8.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4062,  6.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8438,  7.0820]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1797,  5.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8750,  6.5508]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4453,  8.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3359,  5.8633]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7969,  4.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4375,  6.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8516,  5.0469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2734,  6.5586]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7109,  7.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0234,  7.7695]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1016,  8.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.1484, 7.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.2656, 7.7852]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0781,  6.3242]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0703,  6.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.9453, 5.8398]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.6797, 5.6758]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4219,  7.5547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.8672, 6.9258]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4453,  4.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2656,  8.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.1016, 7.4648]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9375, 8.0469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.7812, 7.3086]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.6172, 8.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.8750, 7.6680]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0781,  7.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.2266, 9.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9219,  8.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9922, 7.7695]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1016,  7.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8438,  7.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1875,  7.3945]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7188,  7.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8828,  7.1680]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5625, 10.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0391,  7.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2188,  6.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2656,  7.1914]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8203, 11.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4297,  9.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.0391, 6.2148]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6484,  8.5547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.8984, 8.1953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.8477, 6.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.5156, 5.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.9453, 7.7461]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.8047, 6.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[5.7344, 5.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9062,  8.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8984, 11.9688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7500,  8.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7578,  7.8164]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6016,  7.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5469,  8.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9219,  8.0625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9531,  8.3828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9141,  9.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4375,  7.6602]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0547,  7.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3438,  8.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4766,  8.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8438,  7.1992]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8828,  9.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4531,  9.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9219,  5.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2969, 10.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5859,  8.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2344,  6.9648]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6094,  8.0625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8359, 10.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1250, 10.7188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8672,  9.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9531,  8.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7422,  6.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5625,  7.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1484,  7.0898]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9688,  7.2305]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9375,  9.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9688,  8.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5234,  9.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3281,  8.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2109, 10.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2891,  5.6602]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3203,  5.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1172,  8.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0469,  3.0645]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.7109, 3.6309]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3828,  5.3555]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8359,  7.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7109,  6.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.6484, 3.5957]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2969,  5.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7812,  6.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5156,  5.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2266,  5.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[5.1875, 5.2461]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[6.5391, 5.5938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[5.6172, 6.9844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[6.2188, 5.4883]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[5.2461, 6.6875]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[4.4258, 6.0391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[5.7734, 7.0195]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[4.4648, 6.9961]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[5.9141, 7.8477]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[5.6797, 5.9062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[5.0820, 7.4492]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1016,  8.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0938,  9.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0312,  7.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2500,  6.9492]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5938,  8.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2812,  8.8359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3594,  8.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1484,  9.1094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0156,  7.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.0078,  9.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0469,  7.5508]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7812,  7.3008]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.7031,  8.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6719,  8.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2969,  9.1953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8906,  9.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0859,  5.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2969,  5.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.8828, 7.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7500,  4.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7812,  7.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.5000, 4.9570]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9141,  6.1211]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4297,  9.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.1172,  8.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.5156,  6.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.4219,  8.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.4766,  7.5742]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.1250,  9.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1719, 10.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6875,  8.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.1406, 12.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3828,  8.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.8828, 12.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.9219, 10.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.9375,  7.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.2344, 11.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.9297, 11.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.8516, 10.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.1250, 10.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.1719, 11.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.0469, 11.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1797, 10.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2812, 10.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1719, 12.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8438,  8.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6250, 10.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9453, 10.6484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6094,  8.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0078,  7.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4922,  6.9258]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9062,  7.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2344,  8.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8281,  9.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.5703, 8.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0000,  7.1992]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.3906, 5.3633]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3516,  8.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2344,  8.2578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3125,  7.9570]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2031,  8.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9609,  9.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8984,  8.7188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1797,  8.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8906,  9.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5859,  9.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7500,  8.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2344,  8.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0312,  8.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2422,  9.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5781,  8.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5234,  7.8438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8516,  9.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4141,  8.3828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3984,  8.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8359,  9.3828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0859,  8.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7422,  9.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1406,  8.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9844, 10.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8984, 10.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8281,  9.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7266,  9.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4219,  9.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4375,  7.1523]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5391,  9.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6406,  8.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6172,  8.8750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6719,  7.7930]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.5469, 8.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9062,  6.3086]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 9.4766, 10.5938]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4297,  9.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0547,  9.3906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8047,  8.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5078,  8.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0391,  7.1055]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7578,  7.9570]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.7578, 9.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8047,  9.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9922,  7.7227]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0000,  7.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9453, 7.5820]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2031,  8.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7891,  7.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9453, 9.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7344,  8.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8281,  8.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9062,  7.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3750,  6.4492]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3438,  5.9805]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8984,  4.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0625,  7.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6875,  7.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1484,  6.0117]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0547,  7.2383]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5547,  7.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3906,  9.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2891,  9.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0469, 10.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1406,  9.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9453,  9.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0312,  9.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9531,  8.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9531, 10.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3125, 10.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3594,  8.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6953, 10.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6953,  9.1953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8594,  9.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8750,  9.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.9609,  9.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5547, 10.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5703,  8.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2500, 10.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4375, 10.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2812, 10.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3516,  8.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6953, 10.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1875, 10.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4766, 11.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3281, 10.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0000, 11.0469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.8906, 11.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6797, 11.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9531,  9.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.1562, 10.0469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7031,  9.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2266,  8.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8047, 12.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9141,  9.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8203,  9.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8516, 10.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5781, 10.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9297, 10.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7188,  9.3984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1797, 10.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6328,  9.8750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0859, 11.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2344,  9.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6719,  8.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2109,  9.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3438,  8.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2812,  8.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7109,  7.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5391,  8.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1953,  8.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4609, 10.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7266, 11.8359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9297,  8.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4375,  9.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7031,  9.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2109,  9.5859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9375,  9.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6953,  9.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5625, 11.1094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.8750, 8.7188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.3281, 7.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.2656, 8.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7031,  9.0625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6719,  9.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7031,  9.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3516,  8.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3828,  9.6562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4453,  7.3789]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.7578, 10.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5234,  8.8984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1406,  8.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8750,  7.4883]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5781,  8.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4062,  6.8555]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0156,  9.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3516,  8.5547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9141,  9.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6953,  9.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2578,  8.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.0625, 14.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.1094, 11.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.0156, 10.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3594, 11.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.2266, 12.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.1250, 11.8750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.4688, 11.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.0312, 12.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3438,  8.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2969,  9.1953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8750,  8.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6016,  6.2930]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2109,  8.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7031,  8.6562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6953,  7.0898]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0000,  7.3906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9922,  7.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7422,  6.6758]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7109,  5.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9219,  8.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6562,  8.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9922,  7.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7344,  6.7930]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6875,  4.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2266,  5.8359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0234,  3.3711]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2188,  2.8535]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8750,  3.3145]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.0625, 11.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.2031, 10.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.8203,  9.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.4844,  9.9219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3281, 10.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.1719, 12.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.8516, 11.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.7500, 10.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.5391,  9.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.6406, 10.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.7031,  8.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3750, 10.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0547,  7.5508]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5312,  8.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7812,  9.1484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2188,  8.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5703,  8.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2891,  7.2383]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3438,  7.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5781,  9.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1562,  9.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6094,  7.0977]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9297,  9.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6719,  8.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6875,  7.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9844,  6.9453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6562,  7.9727]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1094,  7.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0469,  6.9727]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8359,  8.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2500,  8.6484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0703,  7.7461]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9453,  7.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2891,  8.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6875,  8.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.3750,  9.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1328,  7.1211]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4922,  7.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4609,  9.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9844,  9.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6016,  9.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7422,  9.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4141,  9.3359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0234, 10.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3594,  8.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.1641,  9.3359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.8203, 10.6562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.1797,  9.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5000, 10.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8594, 10.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8750,  6.7148]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5859,  8.2578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1875,  8.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6562,  8.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9844,  7.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2500,  6.0898]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0078,  8.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0547,  9.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2656,  8.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7656,  8.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2422,  8.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7656,  9.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1328,  7.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3672, 10.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8125,  9.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9375,  7.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4922,  7.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1172,  7.2773]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4531,  7.1758]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2109,  8.3359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1719,  8.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5859,  7.2578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2578,  9.3984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5547,  9.3984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9453,  7.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.4453, 8.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6484,  7.5938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4219,  6.4961]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0312,  7.9805]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7812,  9.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0312,  9.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4844,  8.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4531,  9.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8672,  8.4688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9297,  8.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4844,  7.6133]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6016,  9.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4062,  8.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5000,  8.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8281,  8.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8125,  8.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3047,  8.7031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3594,  7.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7969,  8.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8203,  8.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4062,  8.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7422,  8.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.2656, 10.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.5312,  9.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.5469, 10.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1016,  8.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.1016, 10.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.2891, 10.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6484,  9.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.6406, 10.7031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8984, 10.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3125,  8.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.5312, 10.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.7188, 3.5664]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1094,  5.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9453,  6.3164]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7266,  7.5430]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8984, 12.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6797,  9.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5000, 10.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5547,  9.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0000,  8.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8672,  6.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5156,  6.5430]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6797,  6.9453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8828,  7.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3984,  8.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2656,  6.3984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0156,  7.9961]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3516,  8.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2188,  7.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8047,  7.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3984,  6.7695]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9844,  6.3633]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7031,  8.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.2344, 5.5508]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3750,  7.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8594,  8.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2031,  8.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8594,  8.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9453,  8.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4141,  8.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6484,  7.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0938,  8.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8359,  8.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1172,  8.7188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1719,  7.3008]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3828,  6.1484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5469,  7.7031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1641,  8.8750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1406,  7.6289]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5938,  8.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1562,  8.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1328,  9.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6953,  8.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5391,  6.5820]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5469,  9.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3828,  9.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2031, 11.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6406,  8.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0547,  8.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1094,  9.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1250,  8.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4531,  8.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6797,  9.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6875,  9.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8516,  7.8984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6641, 10.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1094,  9.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4609, 11.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0469, 10.8984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3516,  9.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2656, 10.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1562,  9.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.4922, 11.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8906,  9.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4531,  9.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9766,  9.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3750, 11.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0156, 11.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1172, 10.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6250, 10.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5859,  9.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4609,  9.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9297, 10.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2109,  9.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1250, 10.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2266,  8.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6719,  9.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9141,  8.6484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0000,  9.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6328,  9.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9766,  9.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5703,  7.9102]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2266,  9.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9219,  8.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1797,  8.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9062,  8.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2344,  7.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7812,  8.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2188,  7.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1250,  8.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4844, 10.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1016,  8.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.0312, 6.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.1016, 8.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.3203, 8.8125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1562,  7.2617]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7344, 10.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0000,  9.3594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.8828,  8.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.8516, 10.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6250,  5.8320]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8047,  8.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0391,  8.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9375,  6.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4844,  5.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1016,  7.0977]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1797,  7.3789]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9375,  7.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5000,  8.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.6172, 5.9258]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2812,  6.6758]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7578,  6.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4062,  6.4727]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2031,  6.8633]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.8516, 4.8867]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3359,  7.3477]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.1172, 7.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8906,  6.8320]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9922, 6.3008]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9062,  6.3320]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.2812, 5.4805]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6875, 10.8984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6641, 11.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7578, 12.1484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7969, 10.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6250, 13.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3906, 10.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3125, 13.8281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6953, 12.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.8047,  8.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7188, 12.7734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7656,  7.9180]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2500,  9.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2969, 10.1484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6484,  7.6250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6484,  9.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8828,  9.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3906,  7.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3516,  8.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5078,  7.3164]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4922,  7.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1094,  7.6992]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0078,  7.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6172,  8.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3047,  9.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7969,  9.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6641, 10.3594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.5000, 11.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.2344, 11.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.7812, 11.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.0781, 11.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.5859,  8.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.8125, 11.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.2031, 11.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7656,  9.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7656,  8.7031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1797,  9.6562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9219,  8.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9062,  8.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2266,  8.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.0859,  9.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4922,  8.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3750, 10.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5312,  7.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0391,  8.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1797,  4.9102]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0469,  5.1680]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0781,  4.6133]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.3125, 6.3984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3438,  7.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7266,  6.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1328,  5.5664]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4141, 11.0547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.8672, 12.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2734, 10.8906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7891, 10.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.7891, 9.8281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.1562, 8.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.8438, 9.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 9.8203, 10.4219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0469,  9.7188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2188, 11.0078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7500, 11.7891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4375, 10.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8125, 10.8828]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8672, 10.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 9.5938, 10.5156]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3672, 12.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8672, 11.1406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8203, 11.4297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.8594, 13.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5781, 12.7031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7266,  7.4180]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5859,  6.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1250,  7.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7109,  5.5898]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1875, 12.2031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2969,  9.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8281, 10.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7422,  9.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4141, 11.5469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7734, 10.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4375,  8.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1719, 11.4688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2734, 11.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2422, 10.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5000, 10.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4297, 10.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7031, 12.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1641, 12.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2656, 11.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0859,  8.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3125, 10.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3359,  8.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8516,  9.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8516, 10.9219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6797, 10.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1484, 10.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3125, 11.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1406, 12.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0469, 10.0469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2031,  8.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9688,  6.4648]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9219,  7.8945]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1094,  9.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5547,  8.3906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3828,  7.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3125,  8.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6562,  7.8633]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0156,  7.6836]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1406,  5.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2891,  9.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1875,  7.3359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5078,  6.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6016,  7.2852]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8984,  7.2461]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.5547,  8.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.2188,  8.3359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.6719,  9.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.4531,  8.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.4531, 10.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2812, 10.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.4453,  7.8945]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3750, 13.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.2031, 13.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9688,  7.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7031,  8.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8828,  6.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9375,  7.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9766,  7.0898]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7422,  6.5117]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.7188, 4.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3281, 11.6875]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2969,  5.3398]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5625,  6.1094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2969,  5.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1094,  7.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.0547, 10.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.2422, 10.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.9531,  9.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9688, 10.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.0625, 11.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.7031, 11.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.0781, 11.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.1719, 11.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.7109,  9.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.7031, 10.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.8203, 10.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9062,  8.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5391,  7.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7812,  5.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7812,  6.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1172,  5.7695]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8984,  6.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7266,  5.7617]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5312,  6.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5625,  8.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3984,  8.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0000, 10.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3594,  8.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6250,  8.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3438,  8.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1875,  9.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7266,  9.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4766,  9.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6016,  8.5938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5703,  8.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5547,  8.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8828,  9.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3125,  8.9453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2500,  9.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1250, 10.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6016,  8.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3906,  7.3711]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7812,  8.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9453,  9.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5859,  9.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5859,  9.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3906,  9.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4375,  4.9414]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9922, 5.2773]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.5625, 6.2383]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.2109, 4.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6875,  9.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4375,  7.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4297,  9.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.6016, 7.3242]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3672,  7.8438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.1562, 7.5938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3672,  9.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2422,  9.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6797, 10.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1250,  9.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2422,  9.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9844,  7.7617]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4766,  6.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2031,  6.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8516, 10.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6328,  7.6133]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5469,  9.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7969,  9.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9375,  8.9219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3984,  9.0625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1406,  9.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7344,  8.1953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.7891,  9.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.5078, 12.0469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0234,  8.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9531,  8.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6797, 10.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1406, 10.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0156, 11.0078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1016, 10.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3828,  9.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7578, 10.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5078, 11.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2734, 10.2812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7500,  8.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0547, 10.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5547, 11.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9219,  9.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2344, 11.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2969, 11.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 9.7422, 10.4062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2500, 11.2578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8359, 11.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5234,  9.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3125,  9.6562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8438,  7.0117]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0781,  8.6562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.6797, 7.9961]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8672,  7.4102]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.8281, 7.5117]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.7422, 6.7773]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9922, 7.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5234,  7.8164]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5312,  8.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1406,  7.0195]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1406,  6.9336]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4297,  6.2773]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5156,  7.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6328,  6.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6250,  8.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.9219,  8.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0078, 11.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3281, 11.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2422, 10.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1953,  9.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0312, 11.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8906,  9.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3125,  8.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2734,  4.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2891,  7.4180]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2266,  6.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6641,  5.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5547,  8.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1094,  6.2383]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2812,  6.9023]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4375,  6.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6484,  8.0625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7031,  6.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2031,  7.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3594,  6.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.1172, 6.5547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.5547, 6.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1172,  6.4258]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9219, 7.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9922, 5.4258]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4062,  7.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6562,  7.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9922,  7.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8203,  7.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7344,  8.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8438,  7.9102]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2891,  8.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.3594, 12.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2109,  8.5859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.6250, 8.4922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.7500, 8.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.9180, 7.8789]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.2578, 7.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.1641, 9.5781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.7773, 5.3555]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2891,  8.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.6562, 5.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0938,  7.6133]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.2812, 11.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.4062, 11.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7188,  7.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.3594, 11.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3594,  8.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9688,  8.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8828,  7.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6641,  9.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1562,  9.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7656,  9.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7656,  9.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4219,  7.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6328,  8.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8906,  8.2578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8203,  7.3789]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5938,  9.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5391,  6.4023]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5234,  8.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0781,  8.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5312,  7.9805]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8984,  8.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9922,  7.4414]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8125, 12.0703]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 9.9844, 10.5234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4453,  8.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5312,  8.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0312, 10.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6953,  9.3906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3672, 10.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5391, 11.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0000, 12.1875]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0938, 10.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3203,  8.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2188,  7.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2500,  6.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.7188, 6.8633]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1484,  7.0625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3047,  6.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0469,  7.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0391,  8.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9219,  8.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7031,  8.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.8906, 7.1953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2891,  5.1289]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9375,  6.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7891,  6.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9844,  6.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8203,  7.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3516,  5.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7188,  7.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3828,  6.4688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6016,  9.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7188,  5.6250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4688,  4.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8672,  6.9727]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2578,  7.0273]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5625,  8.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0000,  8.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1797,  9.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3672,  7.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0234,  8.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5703,  7.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6484,  6.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1094,  6.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3438,  6.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7734,  6.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8125,  8.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6328, 10.3828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4688,  8.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1797,  7.5664]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7188,  6.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1406,  8.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5625,  6.2695]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5312,  8.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9453,  7.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8125,  7.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3047,  7.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5469,  8.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3672,  7.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3750,  7.5664]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7109,  6.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5156,  8.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0156,  6.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6953,  6.5508]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8281,  8.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5469,  8.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.0312,  9.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5781,  9.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5625,  9.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2188,  9.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3828,  9.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5469,  9.5547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3047,  9.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0625, 10.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0859,  9.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1016, 11.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.2344, 11.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7422,  9.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3594,  6.9648]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0078,  6.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.5938, 10.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8750,  6.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6641,  8.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3750,  8.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6875,  7.8086]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8438,  7.9180]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9062,  8.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6328,  5.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8125,  5.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6797,  6.6680]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0312,  6.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2500,  7.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5469,  6.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.9297, 8.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3125,  8.5938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.3672, 7.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1328,  7.9727]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0625,  8.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4219,  8.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7891,  8.2578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3828,  9.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0781,  7.7539]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6172,  7.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.7266, 11.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.5703, 13.5547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.6406, 13.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.9219, 12.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.0391, 12.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3984, 13.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5234, 11.8984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.1094, 12.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.1875, 12.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.9297, 13.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5156,  9.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8359,  8.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2812,  8.9453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9844,  8.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2109,  8.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6484,  8.0625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9609,  8.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.0547, 4.6055]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.5625, 2.3770]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.6484, 3.7441]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.2266, 4.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7188,  6.3398]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.2109, 3.9395]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.2109, 4.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.7969, 11.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.3906, 10.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[18.2812,  8.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.2031,  8.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.9844,  9.4688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.5625,  7.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.9062, 10.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.0469, 10.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.1719,  9.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8984, 10.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8984, 10.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7734,  9.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0000, 10.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1797, 10.3594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3359, 10.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2344, 11.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2578, 10.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5234,  9.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9062,  9.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0312, 10.9453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5625,  9.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8359, 11.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3125,  8.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1094,  9.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0938, 10.6484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6953, 10.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2812, 10.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4375, 10.9219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5000, 10.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7344, 10.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4062,  6.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5234,  6.5898]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2500,  5.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.2969, 7.6211]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7266,  5.4727]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9219,  7.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2734,  6.5273]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6328,  6.4883]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4688, 10.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2500, 10.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8281, 10.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7344,  9.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5156,  9.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2500, 10.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3984, 10.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0859, 10.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0547,  8.6484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9453,  9.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1719,  8.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2109,  8.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2500,  8.9453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5938,  9.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8984,  6.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6250,  7.8320]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2422,  7.3867]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1641,  7.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6719,  6.8711]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2109,  7.1055]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5469,  8.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.6719,  9.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4609,  9.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1719,  9.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.7266, 10.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.0000,  9.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.4297, 10.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2578, 10.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8906,  8.6562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5234,  9.8359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.2422,  9.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6094, 10.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9844,  9.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5938,  6.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5625,  6.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6328,  6.5664]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3984,  7.6836]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9219,  5.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5703,  6.8711]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1484,  7.7383]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3438,  7.7695]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3281,  7.5352]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4453,  6.4648]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3438,  7.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7812,  7.4492]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7266,  8.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2266,  7.7617]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.2891,  8.3359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7812,  8.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0859,  8.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6016, 10.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2188,  7.9258]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5000,  8.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3672,  7.5977]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.1953,  9.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5547, 10.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7031,  9.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5469,  6.0352]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0859,  6.9414]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9375,  8.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4375,  8.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4062,  7.9336]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3906,  8.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7812,  8.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3594,  6.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1484,  9.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0781,  8.6562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4453,  9.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5391,  8.1484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0469,  8.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6094,  6.6914]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8125,  7.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.2266, 9.4531]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5078, 10.8516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 9.3516, 10.2109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 8.9141, 10.7578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.8828, 9.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.9375, 9.2656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0938,  9.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 9.9531, 10.2500]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1250,  8.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 9.1797, 10.1484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.7031, 8.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3750,  9.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.3203, 9.5391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1328,  9.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2266,  9.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.8984, 9.6016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2969,  8.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4219,  9.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.8438, 7.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.8828, 6.3867]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4453,  8.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3125,  7.2461]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.6797, 6.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.4609, 6.1992]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.1562, 6.7148]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.7500, 6.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.2969, 8.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1641,  8.1953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9609,  7.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3125,  6.8438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5938,  8.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5391,  6.7617]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9922,  9.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2891,  8.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9453,  7.9727]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8438,  8.8750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9531,  9.5938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9297,  6.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9922,  8.5547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1875,  9.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2422,  5.9258]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6328,  7.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5547,  9.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.7812,  9.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3828,  7.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7734,  6.6055]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4062,  8.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4688,  9.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8984,  8.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5547,  6.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1719,  9.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6719, 11.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3516,  8.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8359, 10.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3594, 10.0469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2344,  8.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9141,  9.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3984, 10.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1094, 10.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1641,  9.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.5859,  9.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3359, 10.7031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.2344, 10.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.7031, 10.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.8906, 13.0625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6953, 12.3828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0391,  7.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2344,  8.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5625,  7.4688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0391,  8.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4297,  8.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2344,  7.6133]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1875,  7.6133]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8203,  6.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5938,  7.2617]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2031,  7.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8281,  7.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9297,  8.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4062,  7.5508]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7500,  8.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.8984, 7.3945]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.5234, 8.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4766,  8.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.7031, 6.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.3828, 7.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.8359, 8.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.9297, 8.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3516,  7.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.9609, 7.6836]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.5859, 8.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.2578, 6.2148]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.6719, 7.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.5859, 6.7695]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.2656, 8.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5234,  2.9648]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1484,  5.0039]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6484,  6.7852]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4141,  8.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7812,  7.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9375,  7.1523]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9453,  8.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.7969, 10.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5000,  8.3828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4141,  9.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8672,  9.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6484,  8.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6406,  9.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5625,  9.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9922,  8.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1875,  9.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7266,  7.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7656,  7.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7266,  9.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3047,  7.1836]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9844,  6.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0234,  8.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.1484,  7.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.6094,  7.7305]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6641,  8.1953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.4766,  8.6562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.8359,  9.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6250,  7.4414]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.9219,  8.6484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5781,  9.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9141,  8.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2031, 10.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.0703, 10.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9375,  9.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9062,  9.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7500,  8.8750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8516,  9.0625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3672, 10.8281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.8047, 9.5625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1406, 11.6094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1562, 10.2578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5000, 11.1016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 9.8438, 10.0625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4375,  9.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 9.0469, 10.0000]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.0938, 9.7344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8516,  9.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9297, 11.0078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3047, 11.7188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.3359, 9.3516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.5469, 9.2734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0312, 10.6406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1875, 10.8438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2969, 10.8203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3203, 10.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3125, 11.2578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9375, 10.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5391,  9.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3047,  5.9648]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3594,  6.5859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1016,  6.8555]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9609, 7.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3203,  8.4688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7891, 10.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9766, 10.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.2578, 8.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.1719, 7.7383]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9531, 9.8438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.3516, 7.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.2344, 7.9453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2344,  6.8477]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.8203, 7.3867]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9766,  7.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.5625, 8.9219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6797, 12.5625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4766, 10.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4688,  9.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0469, 11.9219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2422, 11.6719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7656, 11.4375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7188,  9.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8359, 10.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9219, 11.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7734,  9.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8750,  9.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.7500, 12.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1328, 10.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1406,  9.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7344,  9.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4844,  9.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5781,  9.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5469, 10.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5703, 10.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1406,  8.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.1484,  9.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8281,  9.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8125,  8.8359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6484, 12.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3672, 11.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4766, 11.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0312,  8.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9375,  9.8750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2578,  8.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5000,  8.5547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8125,  8.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8359,  8.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7344,  8.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2422,  5.7461]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5156,  6.1523]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.4844, 11.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2031,  9.2578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.7969, 10.3828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[18.8594, 10.3594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.7344, 10.3594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.7500,  8.1484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4219,  8.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1016,  5.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7891,  9.9453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8047, 10.3906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.6250,  8.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.3438, 11.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.8594, 10.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.4844,  9.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.1719,  8.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.1562,  8.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.5000,  6.4727]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3594,  8.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.3750,  8.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.8906,  8.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.7500, 10.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.7500, 10.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.6250, 11.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.9844, 11.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.2344, 10.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.5000, 10.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.5000, 10.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[18.2656, 10.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.8750, 11.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.8125, 10.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.4922, 10.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.7109, 10.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4297, 11.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3438, 12.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4297, 10.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3750,  9.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2422,  9.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2656,  8.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.9766,  9.1484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.4297,  9.1953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.7500,  9.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5312,  9.6562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.1094,  8.3594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7266, 14.4062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3281, 10.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0078, 11.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.6133, 9.5391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.9531, 8.9766]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3750,  7.8555]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5000,  7.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1328,  9.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2578,  8.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2031,  8.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3359,  7.6367]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3984,  7.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.6562, 6.9102]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.6328, 5.4414]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.8750, 5.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9922,  6.9102]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5156,  5.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7422,  6.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9062,  8.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7969,  9.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2188, 11.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.2188, 10.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1641, 10.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6719, 12.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3125,  9.7031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.6875, 11.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.4219, 10.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6953, 10.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6484, 11.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7734, 10.9453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5156, 10.7188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3672, 11.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5938, 10.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6875,  9.3906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3047,  9.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.8672, 9.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8672, 11.0625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0234,  9.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6719,  9.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4453,  8.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7578,  8.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.8672, 9.1484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0859,  9.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2188,  9.0625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7109,  5.3086]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.0859, 8.2891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.1641, 5.6836]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.3047, 5.9961]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.5938, 4.5352]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.5078, 5.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.0156, 5.0820]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.1172, 8.8203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.8789, 6.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.9609, 5.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.8711, 6.3555]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5078,  7.5977]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7812, 10.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.7109, 9.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3438,  9.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0703,  9.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9062, 9.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.0938, 9.7188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8438,  9.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.4453, 8.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9141,  9.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3984,  9.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4688,  9.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7578,  8.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1797,  6.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9766,  9.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1641,  7.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.0781, 6.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.2969, 6.9805]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2969,  4.4648]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4062,  5.3164]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3594,  4.5508]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.0000, 5.7773]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8047,  4.7617]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7812,  5.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5078,  4.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.7344, 6.6211]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1172,  4.5195]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7578,  6.9805]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2109,  5.9453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1953,  7.3633]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8203,  7.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1094,  8.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7578,  7.3398]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2344,  7.0898]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6172,  9.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9609,  8.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0234,  8.1484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6328,  8.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7031,  8.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0391,  9.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4922,  8.6562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5078,  8.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0703,  8.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3828,  9.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6172,  7.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5234,  9.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3047, 10.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.6797,  9.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.8359,  9.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.3281,  9.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6016,  9.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.2500,  9.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5469,  8.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.2500,  9.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 8.7266, 10.1094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.8828, 8.9062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.5469, 8.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.7969, 8.8516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.8359, 8.5547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.1172, 7.4766]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.2812, 9.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1094,  9.5938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2422, 10.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1797,  8.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3828,  8.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3281, 10.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4766, 11.1953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5625,  9.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6719,  9.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5469,  9.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9609, 10.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1250,  9.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9766, 11.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1328, 11.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5625, 11.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4844, 10.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.9062,  8.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0312,  9.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6094,  9.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2891, 11.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7422, 11.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7891, 10.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0625,  9.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8516, 10.0469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0312,  9.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1875, 10.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1875,  7.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9141,  7.6992]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9844,  7.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4453,  7.7031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2109,  9.3359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1094,  9.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.4141, 8.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1484,  8.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6953,  8.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8047,  9.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.4453, 8.7969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.5000, 7.5664]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5312,  8.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0781,  9.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 9.7266, 12.6719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 9.9766, 10.8594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1016, 13.7109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4531, 12.1719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3438, 11.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6172, 11.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.7539, 9.7266]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 8.2656, 10.7969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7031, 10.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 9.1016, 11.3750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2891, 12.0859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 9.2891, 10.5391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3438,  8.3828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.1328, 9.1094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.8438, 8.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.9844, 8.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1719,  9.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.8750, 8.0469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3438,  9.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0391, 10.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.4375, 11.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2656,  8.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3281,  9.3594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0625,  8.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8672,  7.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8594,  8.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2969,  6.0820]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8203,  7.7227]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4531,  5.6602]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8984,  5.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3438,  7.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6797,  7.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0781,  8.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.7500, 11.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.9062, 10.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.6953,  9.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9688,  7.6992]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7031,  7.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3672,  6.1289]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2344,  5.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5625,  8.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8125, 11.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4375, 10.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9219,  9.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3984, 10.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2734, 10.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1172,  9.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6250, 11.3828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9453, 12.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9688, 12.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5703, 11.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6719, 11.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4688, 11.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9922, 12.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1328, 12.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9141,  9.9219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1016, 12.8359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4766, 11.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0234, 11.4688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6562,  8.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4453,  7.6289]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0469,  7.7305]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8047,  8.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9375,  7.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1094,  8.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6875,  8.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1172,  9.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4375,  7.8008]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6562,  7.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3828,  5.8984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.8203, 7.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.7031, 6.9883]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.5078, 9.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3203,  6.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5625,  7.0117]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0938,  7.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7969, 11.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5938, 11.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.8906, 13.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0859, 13.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4688, 12.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3047, 11.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1250, 10.7188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7500, 11.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4141,  6.8711]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0312,  6.2148]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0469,  8.3594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5234,  8.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2891,  7.6680]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6328,  8.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5703,  9.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9219,  8.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8906, 10.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7422,  8.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0703,  9.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2734,  9.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2734,  9.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2734,  9.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8281,  8.9453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6328,  8.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3750,  9.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7734,  9.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1875, 11.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2969, 10.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7969, 11.3984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2969, 12.5859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.1953,  9.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9297, 11.3906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9844, 12.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4844, 10.6562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8438, 10.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1953, 10.3906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3438, 11.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3672, 11.3359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9844,  9.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0312, 11.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4766, 10.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 9.8438, 10.1328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1484, 10.6094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9453,  6.7461]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8125,  8.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1406,  6.2773]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4297,  8.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1328,  8.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2109,  5.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5859,  5.8867]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2891,  9.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9688, 9.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.3750, 8.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5234,  9.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3125,  9.5859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0859,  7.8164]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3828,  7.8750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.3516, 7.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3047,  9.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5234,  9.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1953,  8.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0781,  9.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2031,  8.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1406,  8.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.7891,  9.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.9219, 10.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0312, 11.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.2969,  9.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.2188,  9.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.2109, 10.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6797, 10.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6328, 10.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0391, 10.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5391,  9.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3828, 10.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.3750, 10.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.2891,  2.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.9297,  4.0977]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.0000,  4.4492]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.7344,  3.6680]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.6016, 10.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.5312, 11.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5156,  9.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.6016, 12.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7656, 14.5547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5234, 10.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9219, 12.7188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1484, 11.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7031, 13.6406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3984, 12.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3203, 13.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3125, 13.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7734, 11.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3750, 12.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1719, 11.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7344, 13.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1797, 13.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3750, 12.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.9805, 2.7832]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.1094, 3.6250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.4688, 3.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.9141, 3.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.7969, 4.8477]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.7422, 4.3906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2734,  5.1914]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.7500, 4.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.7500, 5.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.3359, 5.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.3516, 4.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.5234, 4.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.0547, 5.7930]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.8438, 3.7520]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.3281, 4.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.8203, 6.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2109,  5.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3438,  5.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9062, 4.0508]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3203,  2.9512]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.5625, 4.4961]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.2891, 4.8438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2969,  2.3535]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.9062, 3.8457]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.7031, 2.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.7266, 2.3008]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.1328, 4.2930]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8984,  8.0625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8750,  8.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8203,  8.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5391,  8.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5781,  8.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8047,  7.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4766,  7.0352]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7969,  4.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4766,  5.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5000,  5.7070]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9062,  6.2148]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1406,  5.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8984,  5.2930]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2500,  5.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1562,  6.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5078,  6.5430]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0234,  6.1758]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5078,  6.7305]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5703,  6.0039]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9297,  4.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5703,  6.7070]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3984,  7.2227]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9375,  7.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1172,  5.7070]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.8516, 6.4648]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7500,  6.9258]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2969,  6.9453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4375,  8.8984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2500,  8.2578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0938, 10.1953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1172,  7.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.9688,  7.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6250,  9.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1953,  9.8984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0234,  9.5938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6875,  9.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4844,  8.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6562,  8.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7422,  9.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6250,  8.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8906,  9.5547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4141,  8.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7500, 11.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7891, 11.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9141, 10.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.6016,  8.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7109, 11.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1641,  9.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2344, 11.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5156, 10.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6875, 11.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5625,  9.8359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4141,  8.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6328,  9.8359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8438,  8.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2656,  8.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2969,  8.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3203, 10.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4531,  7.2773]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0312,  7.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5312,  8.1094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3359,  4.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3672,  5.4102]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1484,  7.6602]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1328,  7.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3906,  8.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.2109,  7.7227]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1875,  5.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8672,  7.6445]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9844,  7.1523]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.0469,  5.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3359,  3.9785]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4375,  6.3008]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4062,  7.9570]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.3281, 5.3242]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.8438, 7.1758]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.4453, 5.8477]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9688, 6.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.7266, 5.8867]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.2031, 5.4023]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.0469, 6.8477]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2266,  6.7695]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.4766, 8.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.0859, 8.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.5078, 8.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.0391, 6.9414]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.5703, 7.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.7266, 6.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3125,  8.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4375,  7.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7031,  7.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8047,  6.7227]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7188,  9.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5625,  7.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6406,  8.7188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1797,  5.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5938,  8.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4375,  9.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2578,  8.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2656,  7.0820]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5469,  7.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9766,  7.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7734,  8.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0625,  6.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9375,  6.2305]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5547,  7.2578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.4688, 14.4688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.1562, 13.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.8906, 14.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.7969, 15.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.6406, 13.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3047,  7.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.8438, 10.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0391,  8.9219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1484,  9.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3594,  8.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 9.2578, 10.5625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.2344, 11.4688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[15.8906, 11.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.3438, 13.0469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5234, 10.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.4766,  9.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.6094, 12.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9453, 12.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0234,  8.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.6094, 8.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1484,  8.1094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7891,  8.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5391,  9.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.4531, 7.7461]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9766, 8.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7578, 10.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.1719,  8.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.5312, 8.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1797,  8.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[ 9.3906, 10.4453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.2969, 9.6094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[7.3594, 8.1250]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.1250, 10.7188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.2500, 11.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[17.0312, 14.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[16.8125, 14.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0078,  9.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9141, 9.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.6484, 7.0742]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4375,  8.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5391,  9.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.6953, 8.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9844,  9.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3984,  9.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8516,  8.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5859,  8.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5078, 10.4688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2812,  9.1953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0938,  8.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2188,  7.7070]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6484,  5.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7969,  6.6562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0234,  5.2227]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4297,  7.5430]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6328,  6.6758]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1641,  5.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5625,  5.1914]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1562,  7.9258]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.5547,  7.4336]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3828, 11.8750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6953, 11.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.3438,  9.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4688,  8.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0625,  8.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2500,  9.9453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.4453,  9.5859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1094,  8.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7734, 11.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1719,  9.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.2031, 10.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7891,  9.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.2656,  8.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8047, 10.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0938, 10.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5391, 10.3984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9688,  9.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.5312, 10.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.3125, 10.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7812,  9.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.8203,  9.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.9922,  9.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0469, 10.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.1328,  9.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.9688,  8.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6016,  6.4570]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4688,  8.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6562, 10.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.6172,  8.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1484,  7.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1328,  7.9336]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7344, 10.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0156,  9.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.0859,  7.3984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[14.0859,  9.7188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5703,  7.5430]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8750,  8.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1094,  9.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.5234,  8.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.1641,  8.3594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.3438,  8.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.9688,  9.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.2656, 8.5000]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.1875, 7.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.9688, 9.4844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.4219,  9.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.9688, 8.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[8.8438, 8.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.0078, 6.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.5391, 8.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2812, 10.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2031, 11.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2109,  9.8750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.4453, 11.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4453,  8.8750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7266,  8.9219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.0391, 10.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.7188, 11.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1953, 10.8438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.1328, 10.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.4609, 11.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.3594,  9.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.1641,  9.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0469, 11.5781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.0938, 10.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.9219,  9.3359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.8516,  8.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.2969, 10.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.3750,  7.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[9.3594, 7.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.7578,  8.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.0469,  8.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.5781,  7.5820]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[11.6484,  8.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[12.8359,  8.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.7422,  7.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[10.5625,  6.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "0_real\n",
      "--------------\n",
      "tensor([[13.7109, 15.1953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2188, 14.8672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3281, 15.8828]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4609, 13.3672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5391, 14.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0859, 14.8516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3594, 11.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.2500, 13.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.0547, 13.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6484, 13.8672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9609, 14.1016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3047, 12.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9062, 15.1719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.8203, 12.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.9062, 12.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.4844, 12.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9688, 13.7734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4141, 13.9375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.6914, 13.8359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2422,  8.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2266,  8.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7812,  8.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7109,  6.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.3906, 9.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2031,  8.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0703,  8.1484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7344,  6.7930]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5938, 12.6719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9375, 13.6641]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3203, 12.0234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8203, 11.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1875, 12.0312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5312, 12.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7500, 15.0625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1172, 15.3438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0859, 15.4141]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5859, 13.5000]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0547, 14.7812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6875, 13.5078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0703, 11.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5859, 12.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3828, 14.7422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.7812, 14.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4609, 13.8281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9297, 13.9375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5312, 16.0781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0078, 13.6641]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6875, 11.0781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9453,  8.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.7734, 8.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4844, 10.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.8594, 9.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9141,  9.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4375, 11.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1172,  9.3359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6328,  9.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8828,  8.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7578,  9.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1719,  8.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4688,  8.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5781,  7.9570]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0312,  9.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8906, 10.3906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2656, 10.8906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5391,  8.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.9141, 9.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3516,  9.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5000, 10.9688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3594, 10.0625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8281,  9.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3125, 11.0625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3047, 10.6641]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8828,  9.3594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2500,  9.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.6641, 10.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.8242, 11.3125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.1641, 14.2031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.0312, 12.3047]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[5.6172, 9.4375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.3594, 13.0078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.3516, 12.1797]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.6914, 11.0312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.8320, 12.2969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4062, 11.4531]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.3047, 12.4609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7266, 12.3984]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2266, 10.3906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.2422, 9.0625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.3945, 7.8047]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.9492, 8.0000]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.3555, 8.5781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.7891, 7.7695]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.8320, 8.2422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.6445, 7.7812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.7109, 9.7578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1875,  9.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9062,  8.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3750,  8.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8047,  9.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8281, 11.9688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0000,  9.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0625,  9.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5859, 10.3984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6328,  8.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0547, 11.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6797, 10.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.7500, 10.0234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.9141, 9.0469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.7891, 9.1328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.9297, 9.5938]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.8047, 9.9375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4922, 10.0312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.6641, 9.6094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8516, 11.7031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.7109, 12.5859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.0469, 11.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1406, 12.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1953, 12.6562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5234, 14.1250]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0938, 14.7344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.7266, 14.7344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.2266, 12.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.7188, 12.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.8906, 14.0625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9922, 12.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1484, 12.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4844, 12.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4609, 16.8594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4141, 16.8906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5859, 17.2031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9922, 17.8281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5156, 17.4375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9062, 16.9219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7109, 17.4844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3750, 17.3438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8594, 17.4531]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9844, 18.0312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6484, 17.6875]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7969, 18.2031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7422, 16.8438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.6875, 17.7812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.4375, 17.5312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.3125, 13.5234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7031, 15.2188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.0000, 13.4766]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7344, 13.5312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4062, 14.3203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2422, 13.0000]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4141, 13.6094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5000, 12.0312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.7852, 12.1016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0859, 12.7969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6094, 11.7734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.6016, 14.7969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8203, 14.8672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.5000,  8.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.5000,  8.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.3750, 10.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.4375,  7.0430]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.4219, 13.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1250, 12.4297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3125, 10.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[20.4844, 14.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.2266,  8.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.1250,  9.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.0781, 11.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9766,  8.8750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9453, 11.1172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.7109, 8.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1875,  7.9102]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1875, 10.0625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0781, 11.3594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3984,  9.8359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3047,  7.9102]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0938,  9.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0078, 10.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8281, 10.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8281, 11.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6562, 10.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8047, 10.9375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3125, 12.2578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8906, 15.0391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0703, 15.3359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1250, 12.9297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.1484, 11.6641]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.1250, 11.9453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.0000, 13.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2109, 13.5156]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.1328, 11.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.0312, 12.9219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.6172, 12.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1641, 11.7578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8125, 12.7578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5625, 12.4531]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.6172, 12.9062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2109, 13.4688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9297, 11.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5781, 12.5156]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4531, 11.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9297, 11.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9141, 11.8906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0000, 12.0859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8672, 11.3672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5391, 11.6797]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3516, 12.3359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7109, 13.1328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.6797, 13.0312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8516, 12.9219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3281, 13.2734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5938, 12.9141]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.4062, 12.8984]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5859, 14.3672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2500, 10.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4609, 11.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.8203, 13.1484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5781, 11.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1953, 12.3672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4219, 12.6484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1484, 11.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5078, 13.1406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6641, 12.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4609, 11.7500]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5547, 13.7578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.4453, 14.0391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.6719, 13.6406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5781, 13.5703]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4375, 13.4844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5625, 12.0859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9844, 12.9844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7969, 11.7188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5000, 11.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.6719, 13.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0078, 11.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2422, 10.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3438, 11.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5547, 12.8906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9688,  9.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.9531, 11.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.6562, 12.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2188, 11.7656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0703, 11.6250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0469, 11.3672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6953, 12.2109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4766, 12.8750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5859, 13.4688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1328,  9.3594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6172, 11.3516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5547, 12.4844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7891, 14.6562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2109,  9.6484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5938,  9.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9766,  8.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2109, 10.4688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1250, 13.3672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.4688, 11.9531]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7109, 12.6172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7812, 11.4062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.6016, 10.5156]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6328, 14.5469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3594, 13.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9531, 13.3203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6406, 11.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3203, 14.5312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8047, 15.1875]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5703, 13.3359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9453, 13.2656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7734, 11.8516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4062,  9.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5078, 10.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7891, 10.5859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5469, 10.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5547, 11.7422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3984, 12.8203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2031, 10.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.5781, 8.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6953,  9.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0781, 10.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1953, 11.5781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.2734, 8.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.5156, 8.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2812,  9.3359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5000, 11.6562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8516, 10.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1328,  8.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.9375, 9.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3828,  9.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4688,  8.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0234,  9.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.7422, 9.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1484,  9.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.1016, 11.0781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8828, 11.8125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.7266, 8.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2891,  9.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1484,  9.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9766,  9.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5234,  9.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.8984, 7.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8672,  9.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9922, 10.3984]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4609,  9.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1953, 10.8047]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.3594, 12.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.3516, 10.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.8984, 12.1953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.3750, 11.6484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.4531, 12.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.8672, 10.9219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.4062, 12.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.6875, 12.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.9141, 15.3672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1406, 14.9375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2031, 15.8125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5859, 14.4453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0938, 14.8750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8594, 16.4062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.0781, 13.0234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2578, 14.0469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.6250, 15.8047]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.6484, 11.4219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6797, 14.2266]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3281, 15.1719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1719, 16.3438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5000, 10.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5703, 10.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.0859, 10.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8906, 11.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8906, 10.7031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3906,  9.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0781, 10.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1406, 12.7031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7656, 11.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5547, 11.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2969, 10.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6250, 10.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.9844, 11.9219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8984,  9.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.9297, 10.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.2266,  9.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.6250,  9.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6172, 10.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.1094, 12.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.1953, 11.6250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5781, 10.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0938,  9.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7500, 10.8594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9844, 12.0156]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.4844, 13.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.7656, 13.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.7031, 13.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.7031, 12.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.7031, 14.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2344, 11.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6562, 12.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.7031, 13.4688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.1250, 13.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.2656, 8.6016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.9453, 6.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.0703, 7.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.3008, 7.7422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.2422, 8.1875]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.0078, 9.8828]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.2812, 8.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[5.0938, 7.8047]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.2969, 8.8750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.5547, 6.9023]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.9180, 8.5781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.7422, 8.8438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2266,  9.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.0156, 7.7461]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.4258, 10.0547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0469, 14.1719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7109, 13.4062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6641, 14.7734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7344, 13.2656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7344, 14.4219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7188, 13.3203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7188, 16.2656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8438, 11.2656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1484, 12.1953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6094, 13.3750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4062, 13.6797]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0000, 10.3906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.9609, 10.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.9531,  9.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.9297,  9.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.3047,  9.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.1172,  9.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5547,  9.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.5078, 10.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.6875, 11.8750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.8477, 8.7891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.1172, 12.4297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.0703, 13.7500]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.6641, 13.6094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8359, 11.4141]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4844, 10.7812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.0586, 7.9258]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.1641, 7.8555]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.4375, 8.7266]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.5352, 12.2656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2812, 13.2188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9922, 13.3125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.6719, 12.4844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2344, 11.0078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5469, 11.1484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1016, 12.0859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9688, 10.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.4375, 12.0156]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7031, 12.2656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9766, 12.7734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3594, 12.2500]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1406, 11.9688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0938, 11.7734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5391, 12.2578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7969, 11.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8125, 13.3594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3516, 11.5625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.0703, 10.1016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8125, 12.6250]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4297, 12.0703]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8281, 11.9844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7188, 13.6719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1016, 12.1016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9531, 14.6484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0859, 13.9219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.3281, 7.7148]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4844,  7.8633]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.3672, 7.3906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.4766, 7.3945]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1562,  9.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.7500, 7.2305]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9688,  9.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2422,  8.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3047,  9.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3359,  9.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5469, 10.5781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8281,  9.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5234,  8.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0469,  7.3789]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6641,  9.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1641,  9.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3594,  9.7031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1641,  9.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.1992, 11.2812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.8555, 13.5312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.0586, 10.4922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.9414, 11.9609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.6641, 12.2266]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.2344, 15.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8828, 13.3359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.6250, 13.1719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4766, 15.6719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.1562, 15.4688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5938,  9.7031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.9922, 9.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.8516, 9.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0703, 10.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4688,  8.3359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9922,  9.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1406,  9.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9922, 11.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5234, 10.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.0703, 10.2188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0781,  9.8984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6250,  8.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5938,  9.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5234,  8.5859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5156,  9.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4531,  9.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.0000, 10.0625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.5938,  9.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.4844, 10.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.1719, 10.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.9531,  9.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.8125, 12.1406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.5898, 10.8047]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.9023, 10.6875]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.1719, 12.0391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6484, 12.5938]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.7539, 8.2266]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.9453, 9.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9688, 12.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.1719, 14.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.0469, 14.9375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.8203, 15.3672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.8281, 12.2578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.8125, 11.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.9453, 13.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[5.8359, 9.1953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.0625, 11.2422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7500, 12.4453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.3398, 9.9844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.1992, 11.2812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.5117, 9.7891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.0703, 11.4609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.3125, 11.5391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.8945, 10.1094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.6562, 12.3594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2344,  9.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6562,  8.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2031,  8.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3125,  7.2070]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0703,  9.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5547,  9.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7656,  5.9219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.7344,  9.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.5078, 10.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.8125, 11.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.4766, 10.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8594,  7.9883]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.5938,  6.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.6875,  8.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8125,  7.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.7031, 12.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[18.6250, 10.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.3438,  9.3984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.8750, 11.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0078,  8.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5391,  8.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7109,  8.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0859,  7.4180]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9688,  9.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2969,  8.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5312,  8.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7500,  8.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3516,  7.8398]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2422,  8.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7656, 11.1250]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.9766, 10.2734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.4336, 8.0781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.5547, 8.8906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8047, 10.5312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2500, 10.3828]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.8867, 9.2266]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.1953, 10.1016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9688, 11.6172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1484, 12.1953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0625, 12.0547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.0391, 8.6875]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.1328, 8.7422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0000, 13.3906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0312, 16.3594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0391, 16.3906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5938, 15.8359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4531, 16.8438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7266, 16.7656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5859, 15.5391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1250, 15.6719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.3672, 10.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.6641,  8.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8750,  8.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5234, 10.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3125,  9.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.4453,  9.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.0938, 10.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3438, 10.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.9141, 13.3984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3516,  8.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9688,  8.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0469,  9.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.6484, 9.9609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.7500, 8.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0312, 10.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.3242, 6.4492]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.5234, 6.2539]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.1094, 9.3125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.8750, 8.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5625,  9.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.6250, 8.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.0391, 7.6055]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.8125, 8.1094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6641,  8.1094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0547, 12.3672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7266, 10.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3359, 11.4688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3672, 11.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7188, 11.9609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0391, 11.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9766, 12.0547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8828, 13.0078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2734, 13.2656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8672, 14.1484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.4844, 12.5938]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.5938, 11.6562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.5391, 11.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.8438, 11.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0781,  9.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.8438, 10.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4766, 11.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4766, 10.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1797, 10.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6172, 10.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4297, 10.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.6484, 11.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.0781, 9.9375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8516, 10.2500]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.1172, 12.6953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.0703, 13.6875]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.0391, 13.3750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.4180, 14.5078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4766, 13.2812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.5977, 12.5703]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.9414, 13.4141]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5938, 14.1719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.1953, 14.9141]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9844, 14.9297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5078, 11.8359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.0469, 11.8594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.5156, 9.9453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.3672, 12.6094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3125, 10.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8516, 11.5547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6484, 11.9844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2734, 11.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1016, 13.4453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4609, 11.8984]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6172, 11.1641]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5000, 11.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0703, 10.6562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6562, 12.6484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5234, 11.0859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8438, 12.3984]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0391, 11.1797]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4922, 11.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7656, 10.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5938, 12.3672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2578,  9.8438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5078, 12.1875]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8906, 11.7109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.4297, 12.6406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5703, 12.3359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.7227, 11.6641]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7031, 10.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7812, 11.8438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5859, 13.1875]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0156, 11.8672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8359, 12.4688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.1641, 12.1172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7422, 10.7969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0938, 10.0469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0156, 10.9219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.3047, 12.3906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.6953, 12.4062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8281, 13.7891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1328, 14.2891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2500, 15.3516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.6328, 12.0312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.4922, 11.7266]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.6875, 13.6641]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.0469, 14.0469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.0156, 13.6484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5781, 14.5938]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.3906, 11.9219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.7422, 12.9844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.6445, 11.5547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.9375, 13.6016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7188, 13.2734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.9297, 10.2734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1484, 10.6484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4609, 10.2031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2812,  9.8438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2344, 11.0469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3438, 13.4141]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4219, 11.1094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4297, 10.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0859, 11.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0078, 11.4766]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.0547, 11.4844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8125, 10.9922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9688, 11.0078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2031, 10.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.6406, 10.8594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.7695, 11.1250]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[18.7344, 10.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.7969,  7.8320]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8203,  8.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.9922, 12.4141]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0547, 10.9062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7891, 15.8594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0391, 14.5312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.8359, 15.4531]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.4844, 15.5000]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6719, 13.1094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1094, 12.9062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1875, 13.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5859, 12.5547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1172, 13.3906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3984, 13.7344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9688, 13.4453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6953, 13.4219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8594,  9.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1484,  8.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.0000, 10.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7188,  8.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1875,  8.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.9375, 10.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2969,  9.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2891, 12.5781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.6875, 11.5547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3828, 15.3828]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4688, 11.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7422, 13.8672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5391,  9.1484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7109, 10.5078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7969, 10.2734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2812, 10.6484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0391, 10.4609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5391, 10.0938]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4453, 11.4922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3984, 12.8672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3281, 11.8359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3594, 11.7578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1875, 11.5391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.9688, 13.0781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2891, 13.0156]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2188, 11.2031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7109, 10.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9531, 11.3125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1953, 10.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1797, 10.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6406, 11.9922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2188, 11.5078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5703, 10.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4062,  7.0273]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2344,  6.6992]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4766, 10.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7266,  9.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6562,  9.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5781,  9.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3828, 12.9688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2109, 12.8516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.3594, 12.4688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1406, 11.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1641, 13.4922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6797, 10.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7500, 12.9688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6797, 13.6406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9688, 12.4922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5391,  9.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7969, 11.3203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4453, 12.7422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.7031, 13.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6328, 12.7109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9375, 11.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.6094, 13.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8438, 11.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7734, 11.8047]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7266, 11.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.3594, 10.6641]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.8125, 9.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4375,  8.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0078, 13.8125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8203, 13.7109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.6719, 13.5547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1797, 15.3438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5547,  6.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7109,  7.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3203,  7.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4688,  8.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.6641, 7.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6719,  7.8789]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.7344, 7.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8203,  6.5547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8594,  6.6211]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2031, 10.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7734,  7.6250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2344,  8.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4375,  9.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9141,  8.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0234, 11.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.7578, 6.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3828,  8.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.6016, 6.1289]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.0000, 5.3633]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.4922, 5.6758]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.9453, 9.5234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8828,  7.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.9922, 8.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.1328, 5.3320]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7500, 12.5312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0078, 10.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9375, 13.5781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5547, 14.0391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4609, 15.5781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7812, 13.3906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4844, 13.5859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4062, 13.1953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8047, 10.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2266, 11.6094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5391, 10.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3906, 11.4609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1484, 11.8984]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7031,  9.5859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2734, 11.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2109,  9.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.0039, 13.4141]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2734, 15.6719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0391, 15.6094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0234, 15.4688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9141, 16.0312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5625, 14.1406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4766, 14.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2812, 15.2969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.3516, 14.7656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6172, 15.7969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9297, 15.1953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7266, 15.7578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4297, 14.1094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5859, 14.0078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2188, 12.8828]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4766, 11.0547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9688, 11.3281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5859, 12.4453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.8047, 9.8281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3594, 12.8047]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3906, 14.2344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2969,  9.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1250,  8.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1875,  9.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5312, 10.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3906,  9.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.1172,  8.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8594,  6.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8203,  7.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0859,  9.2578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1094,  8.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4766,  7.8867]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3750,  7.7461]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2031,  9.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3906,  9.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7734,  8.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5234,  8.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4531,  6.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1641,  6.4336]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5234,  8.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1797, 11.4844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.6641, 13.4297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.3750, 14.4766]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2500, 14.1328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8516, 13.5312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6328, 14.5000]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4141, 14.1719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.4375, 13.1250]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6797, 14.1719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.6406, 14.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8438, 13.0469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7109, 14.1172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.0078, 11.6719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.2539, 9.9375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.6328, 9.5312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.6758, 8.6641]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.9844, 7.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[5.9492, 8.0859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4609, 10.1484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.1328, 11.4219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[5.1641, 9.2969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[5.6914, 9.1875]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.4570, 8.1172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.6680, 9.6641]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.5547, 7.7227]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.0547, 8.7500]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.6367, 6.2617]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.3672, 8.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.0469, 8.2812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.6484, 7.6055]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.9609, 8.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[5.6367, 8.6172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.9258, 12.1562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2031, 10.4766]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.0938, 12.6562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.1328, 13.5547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.6328, 13.6016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2969, 14.2656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4141, 13.8594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.2109, 14.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5859, 11.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.0469, 13.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.6562,  8.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.6719, 12.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.7969,  8.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1484, 11.7578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7109, 12.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.5938, 11.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.0469, 10.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.8281, 12.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[18.3438,  9.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.3281, 10.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.7734,  9.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.7812,  9.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1328, 11.1406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.2969, 16.5938]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.4297, 13.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.6953, 13.3828]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.5820, 12.3750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5000, 12.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7500, 12.5391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.9844, 12.3438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8672, 12.5781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.1172, 11.0391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.9141, 12.3047]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.4023, 12.3672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.6055, 13.7344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4766, 13.0625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.6016, 13.0000]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.3750, 12.0391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.9102, 11.7812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7109, 12.5469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.3984, 12.7578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.8086, 12.4297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.1758, 11.4375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8906,  9.9219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2422, 10.3984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6953,  8.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4297,  9.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4297,  9.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7031,  9.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5859,  9.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1875, 10.1484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.7969, 9.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.5391, 9.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9531, 10.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3047,  8.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0781, 10.2344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8672, 10.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7422, 10.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6875, 11.3516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1094, 11.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8594, 10.3828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8906, 12.0938]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8438, 10.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3281,  9.5547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2812, 11.6172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8359, 13.3594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7578, 13.8594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5781, 10.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0312, 11.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5234, 13.0859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5391, 12.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8281, 11.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5547, 11.1094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3359, 13.3594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7266, 13.6094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6953, 13.0234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9453, 11.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0391, 10.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7422, 12.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8828, 13.6953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1406, 11.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.9336, 14.7578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.8398, 11.5625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.7695, 12.8906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8828, 15.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.1445, 12.2969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.7969, 11.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0312, 10.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.7969, 7.7305]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.3750, 9.2578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8125,  8.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1797,  8.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8281,  9.9453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3438,  9.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0391,  7.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6641,  9.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0859,  9.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9922,  8.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4844,  9.6250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4375,  7.0352]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.1250, 11.2188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3125,  8.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.6172, 10.3281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2188,  9.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7734,  8.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3750,  9.4688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5781,  6.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2969,  6.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7266,  8.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8984,  9.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0859, 10.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1328,  9.3594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.9766, 8.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1953, 10.5469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7109,  8.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7422, 10.3359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4609,  8.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8984, 12.6094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2500,  9.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3594,  8.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1406,  8.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0391,  8.5938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5234,  8.7031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5156,  8.5547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3828,  7.5742]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.2031, 11.5859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.0234, 10.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2656,  9.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0000, 11.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2344,  8.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.1016, 9.5156]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4297, 12.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3984, 12.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4844, 13.7109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0703, 13.5859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6719, 12.2578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0547, 13.5469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1328, 12.9766]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4297, 13.7656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0469, 13.9453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1953, 12.4688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3906, 11.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5469, 11.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0469, 12.3516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3047, 15.5547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.3594, 10.7891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.9219, 9.4844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.1719, 9.6641]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4297, 10.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7344, 10.7969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.9453, 11.3203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.4375, 12.1797]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3672,  9.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.5469, 9.5938]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5781, 10.2891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.6406, 11.1953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5547, 11.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7812, 12.8359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.5547, 9.9062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.8359, 13.6484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.9688, 14.3906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.7188, 13.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4609, 14.0859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2188, 13.3203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6484, 11.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5859, 13.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9141, 11.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7422, 11.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2891, 11.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5625, 13.4688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4688, 11.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0625, 10.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7109, 12.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.6484, 11.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1484, 11.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.2578, 13.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6641, 11.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5000, 10.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3906, 11.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.3125, 13.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.0781, 12.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.8438, 13.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.9688, 12.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[18.5469, 13.3906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.1719, 11.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.2969, 11.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.7031, 10.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.8828, 12.4688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.7578, 13.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.2812, 15.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.1328, 15.5156]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.4141, 10.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.6719, 13.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.7656, 13.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.2188, 13.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.1875, 13.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.7969, 11.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.5703, 10.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.0234, 11.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.7812, 11.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8047, 12.8047]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7656, 13.6953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4844, 10.3281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.1289, 10.2500]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.2500, 8.6953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0312, 10.2578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.5820, 9.3828]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5625, 10.1562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2969,  9.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.0352, 7.6211]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.9609, 11.0938]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.0156, 8.3672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5781, 10.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8906, 12.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9766, 11.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.9766, 10.3594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3516, 12.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.8750, 12.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.9609,  9.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.5625, 12.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.6094,  7.3867]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6953,  8.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8672,  7.8477]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8906, 11.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3359,  6.6250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9531,  7.7852]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0469,  8.3828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0938,  7.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6719, 10.6328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3047,  8.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8828, 11.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8516,  8.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5859, 10.4688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9531, 10.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7656,  8.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.6094, 8.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2578, 10.5234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.1562,  9.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5312, 12.7891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[5.3281, 9.6094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.3672, 8.7344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.3906, 11.2422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.9570, 11.6172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.1680, 10.6172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[5.4102, 9.9609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.1758, 11.3906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.6016, 10.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.7305, 8.7500]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[5.9609, 8.6562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.9688, 13.1094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7969, 11.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8594, 13.3203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0156, 12.9375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2969, 12.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5547, 14.0469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3125, 12.4453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8750, 11.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2891,  9.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8750, 12.5859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3438, 13.4062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.4297, 12.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.0156, 12.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.6328, 11.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.3750,  9.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.9766, 11.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.9844, 11.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.4375, 13.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.3906, 11.5547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.0469, 12.1406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2812, 11.6172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8438, 11.8828]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.8984, 10.3359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8203, 11.4141]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.0859, 11.7734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6406, 11.6016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3594, 12.8516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5469, 11.7734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.0938, 10.8281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.4297, 10.9297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2031, 11.1484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5703, 11.4219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.4141, 11.4844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6875, 11.8906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2734, 12.6875]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8750, 13.8438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.7344, 13.8672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.1250, 12.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.3672, 12.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.2031, 11.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2734, 13.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1250, 11.6484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5156, 12.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.0156, 12.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5078, 13.7969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4531, 12.0234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5859, 13.2266]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2188, 12.1797]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4844, 11.7422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5391, 12.5000]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6328, 13.1562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4688, 13.9688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1094, 13.2891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1328, 12.6094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1094, 12.2969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9219, 11.9219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8672, 11.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6953, 13.4844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4453, 12.4766]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.6445, 13.3516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.6641, 13.1172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.4062, 13.3594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.7539, 12.2969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.5000, 12.8594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 4.3984, 13.0391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.5234, 13.2578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.8906, 13.1328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.0586, 12.4844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.5078, 12.6094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.9453, 13.6172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.6914, 12.7109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5312, 12.5859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.0859, 11.6406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.6641, 12.4453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6016, 10.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.5430, 12.1641]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.2031, 10.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.9062, 12.0234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7109, 10.1406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0781, 10.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2734,  9.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1016, 10.5547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.9609, 10.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0469,  8.1094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4922,  8.6484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.6172, 11.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.8047, 8.7031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.8047, 8.5938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4453, 10.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9531, 10.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9688, 12.2266]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5625, 14.8672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7266, 11.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9688, 11.8359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9141, 12.1953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1484, 12.8203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9375, 12.0078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7266, 10.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0312, 10.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8047, 12.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1641, 10.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6484,  8.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7812, 10.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3594,  9.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0547, 12.2344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.1055, 12.3828]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.5977, 12.0391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.0195, 12.1172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.6211, 14.9062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4062, 16.3281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.0781, 15.9531]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.6484, 14.1328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.5664, 15.0469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.0156, 14.1406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2969,  7.3555]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3125,  8.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7969,  7.4258]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0312,  7.4102]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4844,  8.4688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8359,  7.7539]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7578,  9.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3906,  7.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3906, 12.0312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0000,  8.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1328,  7.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0312,  8.3984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2188,  8.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9609,  7.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8984,  6.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1250, 14.0156]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3438, 14.7422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1797, 13.9922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9062, 13.1250]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4922, 13.2266]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0938, 13.7109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2266, 12.0156]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5078, 13.2109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7891, 12.5312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1250, 12.8672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.7188, 14.0859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9531, 13.3594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3516, 13.6094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7422, 13.8281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0703, 12.7422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6797, 13.1328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8125, 13.9688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.1875, 17.4844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.9961, 14.3438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3984, 15.3438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.1875, 16.2188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.1875, 15.6719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2812, 16.0781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5625, 16.5000]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3047, 15.9297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.0938, 14.5391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8359, 15.0625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5312, 16.2188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8359, 13.5312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5938, 14.4688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5000, 13.6953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8281, 13.5703]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5312, 13.7969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0469, 14.2031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7344, 13.2188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0859, 14.1094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6484, 14.2266]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1875, 14.4453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8594, 12.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.3750, 12.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5625, 13.1953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7734, 12.8438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6953, 12.7344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1406, 12.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9531, 10.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5234,  8.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9844,  7.6250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8047, 11.7344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4453, 15.6406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[18.0625,  8.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2734, 12.5703]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9922, 15.1875]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0156, 13.9688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8516, 11.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4766, 12.8906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3438, 12.1016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2188, 11.8984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2266, 10.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.1328, 13.8125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5938, 14.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.2969, 15.9922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8047, 12.7422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.6484, 11.6406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.4375, 11.8984]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.8828, 14.1016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4297, 13.9062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4375, 12.7500]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7734, 12.8438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5156, 11.8750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3750, 13.1719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3984, 11.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1094, 13.0156]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8594, 12.4453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[3.6172, 9.3203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[5.6719, 9.8984]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[4.8164, 8.4297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.3008, 10.0234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[5.8320, 9.8203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[4.9961, 9.6172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.4336, 11.2656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.4336, 9.6562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.6719, 10.2109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.6445, 9.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[5.4258, 9.2109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[5.5820, 9.9297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7578,  9.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3516, 10.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6250, 11.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2422, 11.3594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2344,  9.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1484, 10.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6016, 10.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5625,  9.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8281, 13.2188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.4180, 11.2109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2969, 12.6562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4297, 15.3438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.2891, 13.5547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3750, 14.6484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5859, 11.1094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5391, 13.9297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3516, 14.6484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2812, 15.2031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3125, 14.8750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7812, 15.2891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.0391, 15.0781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6719, 15.8750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4844, 14.3281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6562, 15.2344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6250, 14.8516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.6641, 14.9219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6094, 15.6797]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6406, 15.3516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3906, 14.6641]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4141, 14.6562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6016, 14.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4531, 10.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9609,  8.8359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5156, 10.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3594, 10.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1641,  9.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2656, 11.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2812, 11.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6641, 13.1328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0547,  9.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8984, 13.7812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8750, 11.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.6016, 13.6250]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.3516, 14.7969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7969, 14.3594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7266, 13.2969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.9531, 14.2500]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.0859, 13.0703]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7891, 16.0625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8828, 15.7578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.8164, 14.8828]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.2969, 14.4453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.0391, 14.6406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.6641, 12.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5078, 10.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6016, 12.8750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7891, 10.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6562, 13.4219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5703, 11.6641]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7734,  9.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7266,  8.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0078, 12.8984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5781, 11.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1484, 10.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6875, 11.6484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2969, 10.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4766, 10.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.9688, 10.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4922, 10.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.1406, 8.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.7812, 12.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5859, 11.8906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8750, 10.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5469,  9.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5703, 11.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.8281, 11.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7969, 10.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9688, 11.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3594, 11.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3984,  9.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5781,  8.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5391, 10.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3594,  9.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5078, 10.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9531, 11.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9922, 10.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9531, 12.1250]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8281, 10.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9531, 12.8906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0469, 10.4844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7812, 11.0312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1797, 10.9766]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1797, 12.7109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2891, 12.8359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8438, 13.2734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4844, 13.6484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.1953, 13.3672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.1406, 13.0234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4766, 13.9609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.8359, 12.2344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2109, 13.8047]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8125, 13.4766]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7734, 11.8906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.3750, 12.0156]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4141, 12.0703]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7969, 11.9453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1875, 13.1328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2734, 11.9297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0469, 11.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9141, 13.5625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8984, 13.2891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.4219, 10.5703]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.9375, 11.7266]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.9531, 11.4453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.9531, 12.1719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.7188, 11.0547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.1250, 11.2969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2734, 12.0547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7969, 11.7891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5781, 10.5547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.9492, 10.9688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5078, 11.8594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.1094, 10.8125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.6406, 10.3750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.9297, 9.9922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.6641, 11.3125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.6406, 11.5547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.6680, 13.3672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.6797, 12.7891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.5938, 12.1094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.0742, 12.5625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5938, 13.2422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.9141, 15.1797]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1484,  9.9219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6875, 10.9375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4844, 11.6172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7812, 10.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2500, 10.2891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6797, 11.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2812, 11.6719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0391,  9.8984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7266, 11.1875]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4297, 12.5938]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0156, 10.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6875, 10.9297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0391,  9.6250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3203, 10.9453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4219, 11.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2109,  9.7031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6094,  9.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3047, 10.4453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5703,  9.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3906, 11.9297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4531, 10.0469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5547, 10.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0312,  5.4492]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.8828, 6.4727]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.4258, 8.0703]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4688,  7.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5391,  6.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8984,  7.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.7344, 7.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5078,  7.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.5469, 7.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.3906, 5.8008]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5078,  9.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.6719, 11.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.6719, 11.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.6328, 11.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.6875, 12.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.5625, 11.1484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5625, 10.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4531, 11.8359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1797, 10.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7188, 12.0625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0781, 13.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5703, 12.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0234, 13.2578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6484, 12.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7812, 12.3516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0078, 17.2500]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1016, 16.3281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9062, 16.8906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4531, 16.5000]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0312, 16.4531]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8047, 17.1719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7109, 16.9844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.6797, 15.5547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0234, 15.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9141, 16.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6641, 16.9531]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2891, 16.3750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.3984, 16.4219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0547, 16.1406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0938, 15.1406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8438, 17.2031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2656, 16.0469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8125, 17.2031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8906, 15.3984]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9141, 16.0625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3594, 16.6094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5156, 16.4844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6094, 16.3594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5625, 15.4688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.9766, 16.0469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0078, 13.1328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2578, 14.5234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2031, 12.5156]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.6328, 14.6250]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4297, 13.8125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.1328, 14.3203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0234, 15.6406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2031, 14.7578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0547, 10.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1328,  6.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.8906,  8.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1094,  9.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6094,  9.8984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5703,  6.8242]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0781,  5.5195]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6641, 13.2188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.4922, 12.5625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5938, 11.1484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7891, 13.3281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7656, 13.3125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6797, 11.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1797, 10.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8438,  9.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2500,  9.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7656, 10.4062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.0000, 11.1406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1328, 12.9141]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0312, 12.2578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2344, 11.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.0859, 13.0312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.0703, 11.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3750,  8.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2969,  8.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.5391,  9.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.3203, 10.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.5312,  9.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.3906, 11.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.2812, 11.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1172, 16.2500]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7578, 14.4453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0469, 14.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5547, 14.5859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7734, 15.1484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5625, 15.5078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1094, 16.1406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8594, 16.2500]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2656, 15.7578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.3867, 10.5156]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.5039, 10.9297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3672, 12.5078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.8906, 12.0781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.3750, 12.4844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.5000, 12.0234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 4.9609, 11.3281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 4.8867, 10.2578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.5781, 10.3984]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.1797, 10.3828]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.1250, 11.8281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.0938, 10.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.6562,  9.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.8281, 10.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.3281, 11.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.2031, 11.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.9688, 12.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.0625, 10.8438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.1875, 11.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8047, 20.3750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.6016, 19.5156]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0078, 19.1719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.1953, 21.8438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.3281, 17.9062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.5938, 23.7344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.5938, 22.4375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.4766, 13.0234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1016, 14.4453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0000, 13.0078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0547, 13.1250]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.3906, 13.4922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9922, 10.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2812, 10.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7969, 10.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6094,  9.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3828, 11.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3906,  7.8867]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3672, 11.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4297, 11.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3281, 10.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9219,  9.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7578, 10.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1094, 10.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9453, 11.8203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8281, 10.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2344,  9.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7969, 12.4219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3125, 10.6250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2031, 10.0625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.7344, 8.8984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8828, 10.9922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8828, 10.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8672, 10.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.3047, 11.4375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2031, 11.3281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.6406, 11.3281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.3555, 11.1562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5859, 11.2031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8125, 10.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.5664, 11.5391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.8164, 10.3594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.1016, 12.3594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.7930, 10.9453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.9062, 11.3125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.9609, 9.8359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.9648, 10.6484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.4688, 10.5391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4297, 11.0391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4844, 10.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7266, 15.1172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9766, 11.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9609, 13.5078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7344, 14.8203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3516, 13.4219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6172, 12.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8750, 14.7812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4844, 16.5312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0078, 16.9219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8984, 14.3750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4141, 12.7656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2031,  9.3984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8125, 12.2109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4609, 12.0234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1172, 10.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6094, 10.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3359, 11.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8906, 12.7500]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5312, 11.8203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9688, 11.7734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4297, 12.1484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4062, 10.3906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8906, 10.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2891, 12.0703]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9844, 11.3516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5234, 15.8672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2031, 14.9766]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4062, 11.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4844, 11.3906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7188, 12.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7266, 10.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.9453, 11.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9297,  8.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9141, 10.1094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1094,  9.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5938, 12.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1484, 13.9609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1406, 11.3828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4141, 15.0078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5938,  8.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5234, 13.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6719, 14.6484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.4922, 5.7227]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.8125, 6.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.9453, 6.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.9219, 4.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.0156, 6.5273]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.7031, 4.4883]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6562, 11.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5859,  9.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1094, 11.3594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8281, 10.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7031, 12.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.3125, 13.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8828, 13.1641]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.7969, 14.2578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4766, 12.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5391, 12.4844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8984, 12.5312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.1719, 10.6641]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7188, 10.1953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.3906, 11.5234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.6875, 10.7891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.8203, 10.5547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4141, 14.2734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.6641, 13.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4375, 14.7891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.5781, 9.7109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.0820, 10.0859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.1602, 9.4062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7422, 12.2578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2969, 11.0469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9922, 14.8359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5703, 14.5234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.6602, 13.8750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.2344, 14.8359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.9688, 13.7891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.3984, 12.5781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4688, 15.0234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.0469, 14.8281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2031, 15.7500]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.1641, 13.8594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.4141, 12.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.0469, 11.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.0000, 11.6250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.8828, 13.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.1797, 11.8203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6406, 13.8125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8281, 15.1016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3906, 14.3828]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8516, 14.4766]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0703, 13.6172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0391, 14.0781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2188, 14.8359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.6797, 13.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4297, 13.5703]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4766, 14.2266]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3203, 14.4375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.3789, 11.6250]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7500, 12.8359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.4336, 9.7734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[5.4883, 9.3750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.6055, 11.5234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9688, 15.0000]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6172, 14.6797]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0469, 13.4922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6875, 15.6562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3281, 15.1797]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.7031, 13.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.4062, 15.8984]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.1758, 9.7969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.7812, 10.1484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0469, 11.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8203, 12.9453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.5156,  9.9453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.3125, 10.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.9219,  7.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.7969,  8.8984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.5391,  9.5859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.8047, 10.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.1875, 6.5938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.7109, 6.1094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.8125, 9.1094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0781,  7.7695]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9062,  8.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8359,  9.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.0156, 8.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.9766, 8.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7734, 10.9453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5469,  8.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7891, 11.1953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.3008, 8.3359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.9141, 6.8359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.7266, 5.3359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.4805, 7.5703]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6484,  9.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.7656, 7.3906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0391, 13.7656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3828, 13.7812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5625, 11.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8984, 12.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.6016, 14.8594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5781, 12.7422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1641, 12.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1172, 14.6484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1719, 15.4688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3125, 14.4375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.8203, 13.3516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.3125, 15.4844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.6953, 14.7812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.7617, 12.0625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0234, 14.6406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8281, 14.2812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.1875, 8.7344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.4062, 9.2109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.8906, 10.7578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.4688, 9.7422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.0664, 11.4766]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7266, 10.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.5391, 9.4609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.6797, 9.9609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2891, 10.2188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.1406, 9.3125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.3672, 11.5625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.9219, 9.7656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.2344, 11.3438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8984, 11.0781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7734, 10.1875]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.5234, 9.8125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.0078, 9.9766]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4844, 11.0781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4766, 11.2734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0391,  9.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7734, 10.1250]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.4844, 9.0625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.7812, 7.6055]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2188, 15.6953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1406, 16.2344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6250, 14.6172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8203, 15.0938]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7969, 13.6719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7891, 13.8125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1406, 16.3594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5391, 12.2422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9531, 13.5156]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6719, 14.9922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0391, 15.0703]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1719, 16.1406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7891, 15.4922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[4.9844, 5.7891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.0781, 5.4180]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.4922, 7.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2188,  4.7852]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.6797, 8.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.9297, 2.5371]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.4141, 7.4258]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.9609, 7.3711]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.8203, 5.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.1953, 8.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.2109, 6.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[4.7070, 7.5430]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.4258, 14.7734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8516, 17.0469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7656, 15.9297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0312, 15.4688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2500, 15.0859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3125, 17.2969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6953, 16.8281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.6719, 14.3750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9453, 15.7656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2422, 14.3672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4766, 15.3438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7891, 14.9141]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.0391, 7.8359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.7422, 9.0391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.7734, 7.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.8359, 6.4492]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.8750, 7.3164]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.4102, 7.3398]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.8984, 9.1250]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.1719, 9.4141]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.2695, 8.4141]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.4961, 10.9297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.4336, 8.5234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5391, 11.2578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.4961, 12.1328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.4141, 12.5469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5078, 12.8281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.8281, 13.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.8047, 10.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.7422, 11.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.4375, 12.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.9766,  9.9219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.9297,  9.8750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.1719,  9.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.3281,  9.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.5547, 10.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5547,  9.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0469,  9.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5000,  9.6484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1328,  9.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4453,  9.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5547,  9.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2188, 10.6953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6094, 11.0938]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8594, 11.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3125, 10.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1562, 10.0391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7734, 11.5859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2188,  9.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8828, 14.0391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5547, 13.5859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0391, 13.6641]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3203, 12.9062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3906,  7.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.6172, 12.2109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5703,  8.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0078, 13.0078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5625, 12.8203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5859, 12.8203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.3281, 13.7266]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[20.4844, 11.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.8281,  4.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.0938,  3.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4453, 12.8672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.5977, 11.8516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.7148, 10.7734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0312,  8.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.8555, 10.4766]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.1953, 12.2734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8672, 10.6016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1328,  8.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5703,  9.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0938, 10.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.9375, 11.0078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.6719, 12.0000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.2344, 12.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.0312, 13.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0391, 10.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.9062, 10.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7109, 10.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1641, 10.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.2109, 10.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1250, 10.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.3516, 10.5547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2969, 11.3047]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6172, 10.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9062, 11.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.1641,  8.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8281,  8.5938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.0703, 10.5859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3047, 10.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0938, 10.2891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2969, 11.1484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0547, 10.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7422, 11.8906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8047, 13.3828]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8828, 14.4453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4375, 12.7891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0156, 13.6562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2578, 14.6172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7500, 12.7812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2656, 13.7812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1016, 13.7578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4062, 13.1484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2031, 12.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1953, 13.1406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7578, 13.8516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7031, 13.5234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0781, 14.0312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1562, 14.2734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.2773, 10.7500]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.7578, 8.9609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.1406, 8.6641]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.0625, 9.8594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.5039, 6.3164]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.9844, 10.4844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.0469, 9.9922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.6367, 10.3984]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.4531, 9.3516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.0312, 8.7969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.7578, 8.0391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.2070, 9.0234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.0781, 10.1797]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2344,  9.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0391, 11.5547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5000, 10.8359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.3828, 10.3125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7188,  9.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5781, 11.7891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0156, 11.8906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5234, 10.9609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1172, 10.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.0938, 13.1797]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4766, 10.2031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8281, 14.6562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5469, 14.6484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3359, 15.5781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9922, 10.8438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6406, 16.4375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0703, 15.0469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8359, 14.1094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5938, 13.5703]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3828, 14.1719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.6641, 14.8672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1094, 14.8594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6562, 14.5000]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3672, 13.6172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4609, 14.8281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0156, 14.2500]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3984, 14.0938]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8984, 13.8594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2656, 14.4609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2812, 12.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.5312, 14.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.0703, 14.4453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.0312, 14.6562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2109, 14.6875]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4141, 15.7734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3906, 15.0156]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.9062, 16.9219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5547, 16.5938]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8672, 15.2891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8750, 13.7969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2344, 14.7266]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8047, 14.2188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5156, 15.5703]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2812, 17.1406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.8516,  9.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.1953,  7.3008]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.7031, 12.2969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7734,  7.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1094, 10.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.4844, 11.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.7812, 11.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.2344, 12.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[18.1250,  9.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4219, 12.2578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9453, 14.0703]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5156,  9.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.0000,  6.9414]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.6328,  6.2461]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.6641,  9.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.8594, 12.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.9844,  9.6250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.2969, 11.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[18.2031, 10.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.5938,  9.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.0000, 11.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.7031, 11.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.2891,  9.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.5547,  9.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4609, 13.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4375, 10.7734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2812, 12.8203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4531, 11.8516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.6562, 10.7188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4297, 11.5391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.9570, 10.9141]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8750, 13.2188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0078, 11.8281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0312, 12.7891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7656, 12.1562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.2656, 11.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.0312, 15.2578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.5156, 14.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8047, 13.0156]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.6445, 8.3594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[5.3086, 7.0117]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.3086, 10.9609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.4688, 10.3828]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.2812, 7.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5391, 11.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0859, 10.5938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.6094, 12.9062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7812, 12.3828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7812, 10.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.8984, 9.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.7266, 9.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7422, 11.8203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4453, 11.2734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9688, 10.8438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7891, 10.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0000,  9.3828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5156,  9.6953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1172, 10.1250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0703, 12.6016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3750, 12.6953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6875, 14.2578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9297, 13.3594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2734, 12.8828]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.7578, 13.5781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1172, 12.6797]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3438, 12.2188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.2891, 12.4609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7109, 12.6562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2969, 11.9297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5938, 12.5859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.4688, 11.6484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.9531, 10.4766]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.6133, 8.7266]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3438,  8.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.5625, 9.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.2344, 10.5859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.6406, 10.1328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.1758, 9.7500]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2500, 10.3047]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.4844, 10.7188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.1094, 10.8984]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.8750, 10.0547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.4297, 9.6172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.3281, 10.3047]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.3516, 9.8594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.8516, 9.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.3672, 12.7344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.1055, 11.5625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.2617, 12.1953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.9492, 11.5859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.8047, 13.4609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.5977, 11.2266]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.2539, 10.8438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.6758, 11.4922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6172, 14.8047]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.7578, 10.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.8359, 11.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5234,  9.7422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4688, 10.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7422, 11.9375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.0703, 11.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8750, 11.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3203, 12.4062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8047, 12.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9375, 13.3672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3203, 11.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.3438,  9.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2734, 12.2422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8984, 10.4531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.1953, 12.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3438, 12.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4688,  8.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.0938, 9.5000]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.1719, 11.7109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.1250, 12.0391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5391, 12.1641]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.3750, 11.1953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.5781, 8.5469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.8008, 10.2969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.5234, 9.8125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.2461, 9.9531]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.4023, 8.1875]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8750, 10.2656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.0859, 11.1797]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5000, 11.0547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3516,  7.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1328,  8.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3594,  9.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9062,  9.9297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1719,  8.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9531,  8.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5625,  7.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6719, 10.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6953, 10.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8984, 10.3672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9844, 12.9844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.6875, 12.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0469, 11.9766]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4062, 12.6797]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2266,  9.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1953, 11.0625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.9766, 11.9844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.8633, 10.2422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.4062, 10.7812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.5625, 12.3516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.7812, 10.8281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.6328, 10.7969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3281, 15.7656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8594, 16.1875]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2266, 15.3984]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8281, 15.8438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9219, 16.7656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5703, 16.9062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1250, 18.3125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.7891, 15.3672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.7578, 11.5391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.7422, 8.1641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.6992, 10.3359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.1523, 10.8359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.3750, 11.1250]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.7656, 13.3359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.2891, 13.4688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.6289, 10.9297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3906, 14.8594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8672, 15.1172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8281, 14.6172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6641, 16.0781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8828, 14.9375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2031, 13.7109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7812, 13.2578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0078, 13.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3438, 13.2109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1406, 13.0938]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5000, 12.6484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4062, 13.1016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.6797, 11.2578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5000, 12.4297]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2656, 13.8516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7812, 13.0625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8359, 14.3906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3125, 13.2656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7344,  9.3438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4766,  8.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7891,  9.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9297,  9.1094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7656,  8.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6875,  8.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3672, 16.7656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0312, 15.7891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8828, 15.4922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9297, 14.9141]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0781, 13.6406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4766, 17.7969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3984, 13.9609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6953, 16.4688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4531, 13.6875]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2500, 14.6484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8047, 14.4141]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0234, 11.8359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2891, 12.6641]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4688, 13.1094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4219, 11.8125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.7617, 11.5469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4609, 13.7266]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.7695, 15.1094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.5977, 11.1484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.4609, 12.1094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.7070, 12.6250]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5156, 14.7422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.9258, 13.2109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.6445, 13.2969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.5547, 8.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2422,  8.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.8203, 8.3906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.7969, 7.5703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.7578, 7.9688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.3906, 13.2109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7734, 13.3047]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9375, 11.6016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4844,  6.6992]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0859,  6.4258]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0625,  5.8750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9609,  5.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.9609,  6.4453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2734,  9.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5156,  7.4102]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8359, 13.4141]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4922, 11.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1641, 11.9609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0938, 10.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5078, 13.7422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9453, 11.9922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7891, 14.2656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5859, 12.4062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1484, 10.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9375, 13.3828]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3359, 12.0234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1719, 10.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8203, 11.6406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5234, 11.5469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5859, 13.5391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3672, 12.7188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9922, 13.1328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7500, 11.1016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.0312, 9.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4453, 10.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8906, 11.4062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6953, 10.5938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7344, 12.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2500, 11.2891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8438, 10.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9688, 10.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6953, 11.8281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7344, 11.3984]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9922, 10.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.3516, 13.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4766, 13.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9297, 13.1719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5469, 12.8984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3516, 12.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9062, 12.6172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1406, 11.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2891, 11.9453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9922, 10.2656]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5391,  9.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1797,  7.3711]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0234,  8.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2109,  7.3555]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2969, 14.8438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.7109, 14.8828]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3047, 12.8125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4219, 12.2031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5625, 12.2344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9141, 12.3281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8906, 15.4609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8359, 15.4219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2188, 15.4609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6641, 15.8984]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3906, 15.4766]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.1992, 13.6250]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 3.5039, 12.1406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.6797, 13.4297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.8711, 12.4297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.6445, 14.4453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.5156, 13.3125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.0547, 14.3828]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 4.6641, 11.7344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.0273, 15.5547]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 4.9766, 12.0938]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.3047, 12.4297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.3594, 12.4375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3516, 15.9766]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.0078, 11.8438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0859, 14.4375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0391, 13.6484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4922, 13.9922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0547, 14.2578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0859, 12.8906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6406, 13.4609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1797, 15.1875]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5156, 15.3047]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8125, 13.4453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8672, 13.5312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1406, 14.5625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2188, 12.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8047, 10.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5625, 11.7188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4141, 11.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3203, 12.7422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5234, 12.9766]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6484, 11.0312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1875, 10.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7969, 10.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.8281, 12.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5312, 11.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1797, 10.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0859, 11.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0156, 10.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4297,  9.9219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1562, 10.6562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.9844, 12.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4844, 12.7188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5156, 16.9062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1250, 15.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9766, 17.1719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0859, 15.0156]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9375, 14.1094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4375,  6.9180]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8203,  7.6211]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4141,  9.1562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4062,  6.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5781,  6.4805]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0781,  8.7188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6094,  6.7852]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5000,  6.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.5156, 5.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.7812, 7.3164]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3750,  8.5078]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0469, 13.5938]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1797, 13.2109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6094, 13.3125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6250, 12.2734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0234, 13.4609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5469, 14.1719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1328, 11.9766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0547, 11.5312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1719, 12.3438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5000, 12.8047]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5469, 13.2734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8203, 13.2344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0859, 11.9453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.7031, 12.8359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.5391,  3.1289]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5938, 11.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0625, 12.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7344, 14.8906]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5000, 13.2422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3672, 12.4453]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5312, 12.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9297, 12.1094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8828, 12.2266]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9844, 11.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2266, 12.3594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4062, 13.1172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7578, 10.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9844, 10.0547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7656, 10.6484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8906, 10.6094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5234, 10.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8203, 10.2656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8203, 11.7344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4766, 13.0469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8516, 12.7188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5781, 14.3984]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8750, 13.7891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2656,  7.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.1016, 7.2422]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2812,  8.6641]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1719,  8.3203]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4141,  7.0703]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4141,  6.4102]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.7188, 8.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.8594, 7.0977]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.9297, 6.5430]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.6250, 8.8203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.4609, 6.9414]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.9531, 7.1680]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.5469, 7.1953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5312,  6.1797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.8203, 7.9219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0703,  7.7539]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3750,  7.2266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.5156, 6.1367]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2031, 10.2578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0391,  8.1875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.5547, 5.7852]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.9297, 7.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.8594, 7.7695]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.6328, 7.5234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4219,  7.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5547,  6.8125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.8516, 6.3594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4375,  9.9844]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2188,  6.9531]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.8438, 6.4180]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0156,  5.6758]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8281,  6.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.7422, 8.9219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5859, 13.8203]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.4375, 12.9141]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.1328, 13.6016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.3516, 13.4141]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6016,  9.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.6094, 9.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2891, 12.5078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.6172, 11.0703]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3047, 11.2109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9531, 12.5625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8203, 14.0156]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4062, 14.1562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.0391, 12.4219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.6719, 13.3047]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0547, 12.7969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.6719, 13.6562]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3750, 13.3438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3750, 13.9062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8984, 12.2734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6172, 13.6016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.1797, 12.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2969, 14.8516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2422, 14.2188]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2344, 14.0859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2188, 13.6562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6797, 15.2578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1719, 13.1328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.0312, 9.6406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[5.3477, 8.8750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[4.7266, 8.6250]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 5.7148, 11.7109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[6.2734, 9.2109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[5.8945, 8.7344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[4.2070, 8.3359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[5.2852, 8.1016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.7031, 10.2031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1250, 11.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0703, 10.7188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6953, 10.4609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0547, 14.7266]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1953, 10.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4297, 11.9922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8125, 10.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1562, 10.0625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9922, 11.8359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6172, 12.9531]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.9922, 12.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.3125, 12.5391]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.2266, 13.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.4453, 11.4688]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3438, 13.9297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.0703, 13.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.7891, 12.4141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1953, 11.3750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.2734, 14.1094]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.0156, 12.8438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.3906, 10.8984]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.0391, 9.2578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.0312, 9.4297]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.0703, 10.0391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.3281, 9.9844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.1836, 9.7734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[5.9219, 8.5859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5156, 11.0469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9375, 11.7812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4375, 11.9531]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.5469, 12.0312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.6094, 11.7109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.1484, 9.7969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.0234, 10.7344]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.4609, 9.5391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0859,  9.6719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.6016,  9.8750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6016,  8.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.1328,  7.5664]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4531,  9.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1953,  8.8047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1797, 12.9141]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2578, 11.6016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7656, 10.4062]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.6719, 8.9688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.1328, 11.9219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.6172, 10.5781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[8.8203, 9.4688]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8047, 11.3828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3047, 11.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3047,  9.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2500, 10.4375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4688,  9.7578]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5938, 12.0859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5703, 13.1953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1562,  8.7969]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0703,  8.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1250,  8.7500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4766,  9.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3906,  8.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4141, 10.3281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3047,  8.8750]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7656, 11.6484]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0781, 11.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3828, 10.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.1094, 14.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.6641, 13.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.1406, 13.4609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.1719, 12.6016]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.1484, 12.8047]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.1719, 12.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.6953, 13.1094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.7656, 13.1562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.7656, 13.1953]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.2305, 13.0000]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.3203, 13.1172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2109, 14.7891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.3672, 15.5469]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.0859, 14.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.6016, 13.6328]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7266, 11.5234]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.6641, 11.9766]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.6836, 13.0078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.1797,  8.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.4219, 10.8984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.6797, 10.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2969, 11.3984]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2500, 14.5859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.3516, 14.9609]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.2344, 13.4922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0156, 13.5312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9297, 15.4531]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.3047, 14.9062]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9219, 14.1406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4844, 14.1562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4609, 15.3516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9219, 13.8594]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9844, 13.7891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2969, 13.2812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.2422, 12.1172]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2656, 14.6094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5234, 14.1250]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8828, 15.2812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9219, 10.5938]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2422, 12.1562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.9375, 10.7578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2109, 12.8672]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1328, 14.7422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.9453, 12.8281]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.9922, 11.9375]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7578, 14.7891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.2500, 14.7109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.4844, 13.7109]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8750, 14.0859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.6406, 12.3438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8438, 15.0938]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9922,  9.8516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[18.4531, 14.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.9219, 14.4219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.9531, 12.6875]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[18.0625, 11.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.5000, 12.3828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.6406,  9.7031]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.6094, 14.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[17.4375, 14.1406]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.9688, 15.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6250,  7.7891]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2344, 11.2188]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.6562, 6.0820]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4766,  7.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5938, 11.5547]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3984, 12.2422]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2344,  8.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4922,  8.6250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9844,  7.3164]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[7.2930, 9.4531]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.7461, 11.8125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.1641, 13.5312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.0234, 11.7031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.7461, 11.7812]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 6.0586, 12.8438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.6953, 12.0391]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 7.0078, 11.0312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5625, 10.8281]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8281, 11.3750]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8594, 10.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5938, 11.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3594, 12.0703]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.3125, 12.9219]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.5078, 14.5078]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.9531, 14.2578]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.0938, 13.3828]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4844, 14.6406]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.2109, 13.1719]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.9062, 12.6094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.0000, 12.3594]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.3359, 13.0781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7656, 13.7734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2812, 10.8906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0859, 13.9844]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6875, 12.6094]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6172,  9.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.5000,  9.3047]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1562, 11.7969]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3438, 13.2500]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6172, 14.2031]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.8672, 10.8125]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[9.3047, 9.3516]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1562, 10.1328]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5781, 12.0234]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.1953, 11.3438]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.6406, 10.8828]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.0469, 10.9375]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4844,  9.8672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.0859, 10.8828]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.4766, 11.1172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1562, 11.6016]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3125, 11.7656]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.2500, 11.5312]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5469,  9.5938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6406, 11.2500]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1094, 10.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9766, 11.0156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9609, 10.9609]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8984, 10.6172]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3125,  9.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.8750, 10.2812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.7734, 11.1953]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3984,  7.8359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5312,  7.9961]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4453,  8.3516]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1328, 11.9219]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.9219,  9.5625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.3438, 10.5000]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3594, 12.5781]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.8906, 11.3672]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6797, 13.1562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.2109, 11.6484]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4219, 12.7734]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.3984, 13.3359]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.6562, 12.5781]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3359, 12.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0938, 13.1719]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.4297, 11.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1406, 11.0625]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.2422, 10.7109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.8594, 11.0938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.6875, 11.7344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.5547, 12.2734]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.2891, 10.5312]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.7969, 11.5859]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.4297, 10.5625]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.2031, 11.3359]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.7031, 11.9922]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 9.9844, 10.7891]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[ 8.8438, 11.1562]], device='cuda:0', dtype=torch.float16)\n",
      "fake\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.5703, 11.6797]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0781,  9.5938]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.4688,  9.2344]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[11.6406, 10.3125]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.1953, 10.8438]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[10.8203, 10.5156]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.3125,  8.9141]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[12.0625,  8.7812]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.3984, 10.4766]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1172, 11.9453]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.8047, 12.2109]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.5156, 10.5469]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[16.4219, 10.5859]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[15.1250,  9.9922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.1797, 12.3906]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5391, 11.4922]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[13.5234, 12.7266]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n",
      "tensor([[14.8281, 10.6250]], device='cuda:0', dtype=torch.float16)\n",
      "real\n",
      "1_fake\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "device = ['cuda']\n",
    "labels_map = [\"real\", \"fake\"]\n",
    "count = 0\n",
    "all_images = []\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "predicted_probs = []\n",
    "\n",
    "images = sorted(glob.glob('../Datasets/ICMRDataset/test/deepfake_eval/deepfake/0_real/*.png'))\n",
    "fake_images = sorted(glob.glob('../Datasets/ICMRDataset/test/deepfake_eval/deepfake/1_fake/*.png'))\n",
    "\n",
    "# images = sorted(glob.glob('../CoOp/data/ImageNet/images/val/n01440764/*.jpg'))\n",
    "# fake_images = sorted(glob.glob('../CoOp/data/ImageNet/images/val/n01443537/*.png'))\n",
    "\n",
    "print(len(images))\n",
    "print(len(fake_images))\n",
    "# \n",
    "# np.random.shuffle(fake_images)\n",
    "# np.random.shuffle(fake_images)\n",
    "\n",
    "all_images = images\n",
    "all_images.extend(fake_images)\n",
    "\n",
    "all_images = [path.replace('\\\\','/') for path in all_images]\n",
    "y_pred = []\n",
    "for image in all_images:\n",
    "    img = cv2.imread(image)\n",
    "    # img = cv2_jpg(img, 50)\n",
    "    # image = add_noise(image, 0.4)\n",
    "    img = Image.fromarray(img)\n",
    "    img = img.convert('RGB')\n",
    "    # img = Image.open(image)\n",
    "    # tfms = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(),\n",
    "    #                            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    #                           ])\n",
    "    img = tfms(img)\n",
    "    img = img.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = trainer.model(img.unsqueeze(0))\n",
    "        print(outputs)\n",
    "    for idx in torch.topk(outputs[0], k=1).indices.tolist():\n",
    "        prob = torch.softmax(outputs[0], 0)[idx].item()\n",
    "        if labels_map[idx] == 'real':\n",
    "            print(\"real\")\n",
    "            predicted_labels.append(0)\n",
    "            predicted_probs.append(1 - prob)\n",
    "        else:\n",
    "            print(\"fake\")\n",
    "            predicted_labels.append(1)\n",
    "            predicted_probs.append(prob)\n",
    "    \n",
    "    # print(outputs.sigmoid().flatten())\n",
    "    # if outputs.sigmoid().flatten() < 0.5:\n",
    "    #     print(\"real\")\n",
    "    #     predicted_labels.append(0)\n",
    "    #     predicted_probs.append(outputs.sigmoid().flatten().cpu().numpy())\n",
    "    # else:\n",
    "    #     print(\"fake\")\n",
    "    #     predicted_labels.append(1)\n",
    "    #     predicted_probs.append(outputs.sigmoid().flatten().cpu().numpy())\n",
    "    \n",
    "    print(image.split('/')[-2])\n",
    "    if 'real' in image.split('/')[-2]:\n",
    "        true_labels.append(0)\n",
    "    else:\n",
    "        true_labels.append(1)\n",
    "    print('--------------')\n",
    "\n",
    "y_true, y_pred = np.array(true_labels), np.array(y_pred)\n",
    "# train with compression and blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f537e716-0f1f-4315-9a1b-fd1472a25d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5396"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6782a4a-76fb-4c49-a8aa-203ec657983e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_precision: 88.6%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "average_precision = 100 * average_precision_score(true_labels, predicted_probs)\n",
    "\n",
    "print(f\"average_precision: {average_precision:.1f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88960bf5-02d8-4f0d-85f6-141c8a9a1e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.56107856510411"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf2e9b0c-fbfb-4d5c-baa6-e1c2335d8b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7607487027427724"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c544f1b5-ab15-4ab8-b9af-21e403601735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n",
      "incorrect\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(true_labels)):\n",
    "    if true_labels[i] == predicted_labels[i]:\n",
    "        pass\n",
    "    else:\n",
    "        count+=1\n",
    "        print('incorrect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20f3caaf-3573-4ac4-8782-bea3f52f1c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1291"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f5a3c1c-b226-4777-b3b8-11ea610a81fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4105"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5396 - count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e2da89b-1172-4116-bbc1-e40faecbca66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7607487027427724"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4105/5396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "851b1ecd-f177-4fa4-8b67-f0c9d6173339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5396"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2c19fa85-e9be-4feb-8a4b-94be909decfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.max(1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab3efc-784b-4d4b-adf3-96844a9a320e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
